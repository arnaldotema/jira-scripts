ID,Channel name,"Context, Root Cause, and Discussions",Severity,Caused by,Affected services,Affected teams,Root Cause Label,Post-Mortem Document,Reported at
871,inc-871-kube-memory-outage-04-06-2025,"One of the static Autoscaling Group (ASG) nodes experienced memory pressure after long uptime, causing CoreDNS to crash. Since CoreDNS runs exclusively on nodes provisioned via ASG (not Karpenter), the system could not recover automatically. The CoreDNS deployment is by default configured with 2 pods, without autoscaling, which caused DNS resolution was partially or completely unavailable.
Additionally:
Each availability zone had only one static node, with no headroom for recovery or load redistribution.
The stuck node resulted in pods being stuck in ‚ÄúTerminating‚Äù state.
Manual intervention was required to force-terminate the unhealthy node and restore cluster stability.
Root Causes
Node in ASG experienced memory pressure, leading to service degradation.
No autoscaling enabled on the ASG (min=1, max=1).
CoreDNS could not reschedule due to lack of capacity.
Default CoreDNS Helm chart allows 2, with turned off autoscaling
Resolution Steps
Manually force-terminated the stuck EC2 instance.
Cluster stabilised once new node was spawned and DNS services resumed.
Action Items
Update CoreDNS default configuration:
Enable autoscaling
Set minimum pod replicas to at least 4-6 to ensure resilience.
Enable cluster autoscaler for static ASGs:
Configure min=1, max=3 for each zone.
Let autoscaler manage node provisioning during failures or high load.
Ensure at least 3 healthy nodes can always run CoreDNS to maintain redundancy.

Another investigation window:
There is enormous increase of Kube-proxy daemonSet memory usage.
This takes all RAM from all nodes - see picture
This could be due to node consolidation tho

Update:
We worked with theory of call-transcribe drop from 40 to 30 replicas could caused this. It happened exactly before all the problems started. The subsequent outages could be potentially originating from there for consolidation of nodes and memory pressures.
We tried to simulate this, without success. We set memory limits on kube-proxy and scaled to 40 static replicas for gpu instances, then scaled down after all were healthy. We did not monitor any issues.
As @michal.popovic sent the PR for Kubernetes related issue based on logs we found, there is high possibility we may hit this after 3 months of running on this version. However, we need to be able to replicate the issue first to see kube-proxy has this issue and is exhausting memory by some trigger (probably node consolidation or coreDNS restarts)
We will consider upgrade of EKS to 1.33 to mitigate possibility of this error.

We were able to replicate:
I removed the set limits for kube-proxy we set few hours ago
Deleted 2 coreDNS pods and monitored kube-proxy
We wanted to see how it behaves when something is happening with coreDNS
kube-proxy was going over limit with memory
Saw on some pods UDP 53 error as in the GH Issue
This would cause memory pressure and would start to kill the instances and all apps running on those instances
After log investigation, coreDNS started logging DNS errors to Route53 internal resolver (10.150.0.2)
After some time, it restarted
This caused kube-proxy memory usage issue in Kubernetes
Route53 resolver timeouts -> CoreDNS restarts -> Kube-proxy high memory (https://github.com/kubernetes/kubernetes/issues/129982) -> Memory pressure to apps -> Consolidation
So far it seems that Route53 is potentially something what caused coreDNS to be terminated.
We have set strict limit to kube-proxy memory limits
It will rather restart then cause this trouble
We will plan to upgrade to 1.33 Kubernetes ASAP as emergency maintenance tomorrow (should happen without downtime, we will coordinate during the day)
We will contact AWS if there was an outage on their side (they are really good hiding it tho..)
This is really unfortunate events and we were unlucky to get this kind of specific issue. Apart from setting limit to core Kubernetes service such as kube-proxy, there is nothing much we could do to avoid this. Kubernetes should be able to survive deletions, terminations, but not if it has bug in itself.

root cause identified
the customer has approximately 16300 greetings, which is about 1/3 of all greetings in the production database
to be short, the code handling greetings processing is old and inefficient
we plan to optimize this functionality asap, therefore there is no need to contact the customer

Serhii Shevchenko
:palm_tree:  Jun 5th at 3:52 PM
root cause identified
the customer has approximately 16300 greetings, which is about 1/3 of all greetings in the production database
to be short, the code handling greetings processing is old and inefficient
we plan to optimize this functionality asap, therefore there is no need to contact the customer
:eyes_bleeding:
4
:dakujem:
1

17 replies


Filip Pro≈°ovsk√Ω
  Jun 5th at 3:56 PM
good jobs guys identifying this
3:58
@Serhii Shevchenko When was the greetings uploaded? Do we have any time when this started happening?


Serhii Shevchenko
:palm_tree:  Jun 5th at 4:04 PM
hmm, they add them every day
Screenshot 2025-06-05 at 17.03.28.png
 
Screenshot 2025-06-05 at 17.03.28.png




Filip Pro≈°ovsk√Ω
  Jun 5th at 4:05 PM
Why the hell they need so many greetings? :smile: anyway, so at some point, dashboard backend became inefficient handling this when some specific number was reached?


Serhii Shevchenko
:palm_tree:  Jun 5th at 4:06 PM
sorry, was wrong condition
Screenshot 2025-06-05 at 17.06.24.png
 
Screenshot 2025-06-05 at 17.06.24.png


4:08
any phone number, this is part of the number configuration to display a list of audio files for greetings
Also sent to the channel


Jaroslav Tomeƒçek
  Jun 5th at 7:10 PM
Who is that customer, @Serhii Shevchenko?
@Lucia K, can we somehow check what they actually do? Do they really want/need 16K greetings? Aren't they working around a missing feature? (edited) 


Serhii Shevchenko
:palm_tree:  Jun 5th at 7:11 PM
LDG  Reliance, LLC, 161817
:thankyou:
1



Jozef Vaƒæko
  Jun 6th at 7:58 AM
root cause identified
@Serhii Shevchenko you described that customer has too many greetings. But what is the root cause? Accessing them? Or code which is processing them and saving them?


Serhii Shevchenko
:palm_tree:  Jun 6th at 8:02 AM
code which is processing them


Jozef Vaƒæko
  Jun 6th at 8:20 AM
so they were adding greetings during the times of the outages which exhausted the memory resources?


Serhii Shevchenko
:palm_tree:  Jun 6th at 8:25 AM
nope, they are constantly being added, and at some point service stopped processing them


Jozef Vaƒæko
  Jun 6th at 8:26 AM
they are constantly being added
how is this possible? This sounds little bit scary :scared6:


Serhii Shevchenko
:palm_tree:  Jun 6th at 8:28 AM
why?) do we limit for example how many groups can be added to the system?


Jozef Vaƒæko
  Jun 6th at 8:31 AM
no, so the processing stopped was the issue.
Could you please add also some monitoring to this when you‚Äôll be improving that old piece of code? I believe that there is no alert or dashboard where you could see failed/stopped processing or is it?


Serhii Shevchenko
:palm_tree:  Jun 6th at 8:35 AM
it's just a memory leak, i mean the service queries 1/3 of one of the table and processes each row
memory leak alert is already in place





",P1,Dashboard Backend,"Dashboard Backend, API V3, CT Call Analyzer, Realtime API, Campaigns, Contacts, Workflow Automation, Call Transcribe, Cloudtalk Admin, Dbcache, Internal Integrations, Calls Management, Call Information Processing, Statistics Realtime, Statistics API, CT Billing, CT Audit, Active Calls API, CT Auth, Companies Management, Integrations, Realtime WS, Call Service, Voice Agent API, Cuecards API, Speech To Text",,"3rd party, Infrastructure",https://www.notion.so/INC-871-kube-memory-outage-2092bccf284f81c5a6b7c222fc16c0cf,2025-06-04T16:58:04.032Z
793,inc-793-missing-analytics-data-for-us-customers-15-04-2025,"#INC-793: Missing analytics data for US customers

*Generated by* @Michal Popoviƒç *via [incident.io](https://app.incident.io/cloudtalkio/incidents/793) on* April 16, 2025 8:08 AM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Jozef Valko
- **Incident commander:** @Michal Popoviƒç
- **Reporter:** @Michal Popoviƒç
- **Active participants:** @Ji≈ô√≠ Missbach, @Arnaldo Tema, @Filip Prosovsky, @Allison Chihak, @Jozef Valko, @Michal Popoviƒç, @Roman Hartig, @Jaroslav Tomeƒçek
- **Observers:** @Lucia Mlyn√°rov√°, @Martina Redajova, @Zoltan Viragh, @Serhii Shevchenko, @Andr√© Carvalho, @Josef Lapka, @Jakub Smadis

### üìù Custom Fields

*No custom fields have been set for this incident.*

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/793)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C08NCF399K5)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** April 15, 2025 3:21 PM (GMT+2)
- **Impact started at:** April 13, 2025 11:00 PM (GMT+2)
- **Fixed at:** April 15, 2025 2:30 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 1 day, 15 hours, 30 minutes

# üñäÔ∏è Summary

**Problem:** There was a problem with missing analytics data for customers using the US region, occurring from 13th of April 8:00pm UTC until 15th of April 12:30 UTC.

**Impact:** Call steps except calls to queues, beginning and end of call in US region cannot be restored in analytics due to missing redis call logs.

Up to 17,4k inbound calls missing in analytics.

4,3k calls should be restorable with basic data (from/to/date/duration and disposition) and without call steps at all.

Rest (~13,1k) should be restorable with basic data (same as above) and with limited call steps (only agents statuses in groups).

**Root cause:** The issue was caused by the stopped webdis binary on all US asterisk servers, which should have been started by the supervisor service, leading to missing redis call logs.

**Steps to resolve:** Issue has been detected by received tickets from US customers during discussion between dashboard and voice team. It has been agreed that dashboard team will prepare the script to restore analytics data from CDR records and from partial CIP records related to groups. Script will restore beginning, end and disposition of the call. Calls with call to group step will also contains beginning, end and disposition of particular agents.

# Leadup

We were deploying new version of asterisk image containing asterisk code fix to prevent permanent database outage every time when database went offline or replied unexpected SQL reply.

https://cloudtalk.atlassian.net/browse/VOIP-1390

New asterisk image has been deployed by terraform that executed ansible playbook asterisk-configure to setup asterisk configuration and install all necessary software. At 21:00 CEST we started deployment of the US region.

# Fault

Ansible playbook failed on several instances because of missing logical volume that should identify drive used as mount point for /var/log used store logs of the instance.
This was most probably caused by timing issue when playbook is faster than AWS mounting the device.
This cancel the playbook execution. After this I run playbooks again manually.

Deployment was not correct and there was running supervisor service that is responsible for starting of webdis binary. Binary has not been running most probably because of missing configuration during the time of start.

Missing webdis caused failures when asterisk dialplan tried to store call_logs to regional redis where CIP processing reads them from.

CIP has been not able to process all calls processed by asterisk instances in US region. This resulted in missing analytics data and 

# Detection

Missing webdis process has been reported by asterisk logs. These logs are exported to elasticsearch. However, there is no process that monitors number of error on these logs.

There is no alert reporting that webdis is not running on asterisk instance.

Failed processing of the call by call infromation processing service is reported as [a error message](https://app.datadoghq.eu/logs?query=env%3Aprod%20service%3Acall-information-processing%20status%3Aerror%20%28%22Failed%20to%20perform%20action%20ended%3A%20Error%3A%20There%20are%20no%20redis%20logs%22%20OR%20%40err.message%3A%22There%20are%20no%20redis%20logs%22%29&agg_m=count&agg_m_source=base&agg_q=status%2Cservice&agg_q_source=base%2Cbase&agg_t=count&clustering_pattern_field_path=message&cols=host%2Cservice%2C%40err.message&fromUser=true&messageDisplay=inline&refresh_mode=paused&sort_m=%2C&sort_m_source=%2C&sort_t=%2C&storage=hot&stream_sort=desc&top_n=10%2C10&top_o=top%2Ctop&viz=stream&x_missing=true%2Ctrue&from_ts=1744535921819&to_ts=1744773256936&live=false) in datadog logs and does trigger an alert but the current monitoring is not good enough.

Dashboard team received first ticket on April 14, 2025 at 9:30 PM:

https://cloudtalk.atlassian.net/browse/VOIP-1857

The Ticket was later moved to voice team. Issue has been fixed by restarting supervisor service on all US instances at 2:30 PM CEST.

# 5 why's analysis

# Root cause

| **Why 1** | **Why 2** | **Why 3** | **Why 4** | **Why 5** | **Root Cause** | **Recurrence Prevention** | Tasks |
| --- | --- | --- | --- | --- | --- | --- | --- |
| CIP service was not able to process calls for US region. | Call logs were missing logs from asterisk instance. | Webdis binary was not running on asterisk instance | First deplyment script has been not executed fully by AWX as number of parallel jobs running at the same time was too large to be processed. | According to github issue log size should be increased for this case to be able to see outcome of all jobs in case of too many jobs running at the same time. | AWX configuration prevents to see outcome of jobs in case of too many jobs running at the same time. | Check AWX log size parameters.

Test deployment of 30 asterisk instances at the same time on stage. | https://cloudtalk.atlassian.net/browse/VOIP-1856
https://cloudtalk.atlassian.net/browse/VOIP-1867 |
|  |  |  | Second deployment running at parallel has been canceled by unknown error.
waiting for projectupdate-56404 to finish | To be investigated. | Engineer missed outcome of second correction deployment. | Do not run complex deployments only by one engineer. Have at least two people watching progress to double check all executions. |  |
| Voice team was not alerted about call_logs for US asterisk instances. | There is no monitoring or alerting for the status of webdis binary. |  |  |  | Missing alerting for status of webdis. | Implement monitoring of webdis to asterisk-exporter and create related alert. | https://cloudtalk.atlassian.net/browse/VOIP-1858 |
|  | There is no monitoring of error messages on asterisk log. | This has been already discussed and we have not found any effective solution to monitor error logs by prometheus counters. Solution based on datadog was denied because of cost. |  |  | Missing aleting based on log errors. | Check again possible solutions to convert log alerts to prometheus metrics by custom exporter to have the number of errors with error type visible in prometheus. | https://cloudtalk.atlassian.net/browse/VOIP-1850 |
|  | Person responsible for deployment did not manually checked asterisk logs after deployment. |  |  |  |  | Always check the logs of deployed service if we do not have implemented alerting related to logs. |  |
| Dashboard team was not alerted about call_logs for US asterisk instances. | There are too many false alerts related to unprocessed calls. |  |  |  |  | Improve CIP alerting. | https://cloudtalk.atlassian.net/browse/DSH-5195 |
|  |  | Voice service is producing uncomplete call logs for unanswered automated calls calling internal customer groups. | Voice team was not aware of this issue. |  |  |  | https://cloudtalk.atlassian.net/browse/VOIP-1863
https://cloudtalk.atlassian.net/browse/VOIP-1865 |
|  |  | Voice service is producing uncomplete call logs for calls where sip proxy is replying by internal error response on call to agent. | Voice had task for this issue and fixed it, but we have found another error messages does not triggering any code so call logs were missing. |  |  | Continue with fixing all missing call logs related to internal error responses triggered by sip proxy.  | https://cloudtalk.atlassian.net/browse/VOIP-1851 |

# Mitigation and resolution

Service responsible for webdis binary has been restarted on all asterisk instances in US region. This fixed issue from a time of the restart.

We are not able restore missing CIP logs from asterisk service logs as this would take weeks and outcome would be still incomplete.

There is a discussion about restoring at least start and end of the call to analytics from CDR records.

# Lessons learnt

Implement monitoring and alerting for all relevant services running in our infrastructure.

https://cloudtalk.atlassian.net/browse/VOIP-1865

Many services producing error logs but does not export this information to prometheus. We need to have a system that can extract errors from logs and convert it to prometheus metrics for alerting.

https://cloudtalk.atlassian.net/browse/VOIP-1850

Do not execute complex deployments on production by single person.

The improvements for moniotoring of voice services are tracked by following epic.

https://cloudtalk.atlassian.net/browse/VOIP-1864

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-04-13** |  |
| 23:00:00 | Deployment of new asterisk image has failed on US region. This caused that asterisk instances in this region were not able to send call logs. |
| **2025-04-15** |  |
| 14:30:00 | Issue with logs has been fixed by restarting supervisor daemon on all instances. |
| 15:21:25 | **Incident reported by Michal Popoviƒç**
‚Äã
Michal Popoviƒç reported the incident
Severity: P2
Status: Investigating |
| 16:42:11 | **Status changed from Investigating ‚Üí Monitoring**
‚Äã
Arnaldo Tema shared an update
Status: ~~Investigating~~ ‚Üí Monitoring
The issue causing missing analytics data for US customers has been resolved as of 14:30 CEST. We identified that 4,348 inbound or internal calls cannot be restored due to missing redis call logs. We are now focusing on assessing the full impact and enhancing monitoring to prevent future occurrences. |
| 17:25:44 | **Incident paused**
‚Äã
Arnaldo Tema shared an update
Status: ~~Monitoring~~ ‚Üí Paused
We're continuing to monitor the situation following the resolution of the missing analytics data issue for US customers. The impact assessment confirmed that 4,348 inbound or internal calls are not restorable due to missing redis call logs. Efforts are ongoing to enhance monitoring and prevent such occurrences in the future. We'll explore options to restored calls partially if it's worth the effort, but it's unlikely at this moment. I'm pausing this incident for now. |
| 17:55:44 | **Incident resumed**
‚Äã
incident.io shared an update
Status: ~~Paused~~ ‚Üí Monitoring |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/793?tab=follow-ups)

- **Asterisk-exporter - implement check if webdis and regional webdis binaries are running**
    - **Owner:** @Michal Popoviƒç
    - **JIRA ticket:** [VOIP-1858](https://cloudtalk.atlassian.net/browse/VOIP-1858)
",P2,,,,"Infrastructure, Missing proactive alerting, Human error, Data loss, Release problems, Insufficient testing",https://www.notion.so/INC-793-Missing-analytics-data-for-US-customers-1d72bccf284f81bf8d1eeef1a02b2856,2025-04-15T13:21:25.661Z
810,inc-810-api-server-overloaded-with-high-volume-of-sms-requests-from-a-singl,"#INC-810: HighCpuLoad95 - admin and api server overloaded

*Generated by* @Josef Lapka *via [incident.io](https://app.incident.io/cloudtalkio/incidents/810) on* April 28, 2025 8:30 PM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Josef Lapka
- **Active participants:** @Jiri Srba, @Peter Stanko, @Michal Ustanik, @Arnaldo Tema, @Allison Chihak, @Josef Lapka
- **Observers:** @Tomas Sykora, @Erik Dvorcak, @Anton Shyrokoborodov, @Matej Zboron, @An≈æe ≈†u≈°tar, @Nikoleta Menyhartova, @Lucia Mlyn√°rov√°, @Zoltan Viragh, @Jaroslav Tomeƒçek, @Martina Redajova

### üìù Custom Fields

*No custom fields have been set for this incident.*

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/810)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C08PB9P4PC7)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** April 25, 2025 10:00 PM (GMT+2)
- **Accepted at:** April 25, 2025 10:16 PM (GMT+2)
- **Resolved at:** April 28, 2025 8:01 AM (GMT+2)
- **Impact started at:** April 25, 2025 10:00 PM (GMT+2)
- **Fixed at:** April 26, 2025 7:22 AM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 9 hours, 22 minutes

# üñäÔ∏è Summary

ct-prod-eu-api-1 has high CPU load, currently at 96.60% CPU load

**Summary:** More than 95% CPU load

- **alertname**: HighCpuLoad95
- **cluster**: ct-prod-eu-2a
- **description**: ct-prod-eu-api-1 has high CPU load, currently at 96.60% CPU load
- **instance**: ct-prod-eu-api-1
- **prometheus**: monitoring/k8s
- **severity**: critical
- **summary**: More than 95% CPU load
- **team**: infrastructure

**Alert instances:** 1

![image.png](attachment:8c756cd4-cb1d-416a-9fb8-b233457c9b5a:image.png)

Increased number of API requests on admin and api serverhost:

[my.cloudtalk.io](http://my.cloudtalk.io/)

https://dash.cloudflare.com/fb88130fe6364b7c08821c5bdfcd2e66/cloudtalk.io/analytics/traffic?host=my.cloudtalk.io

Based on the cloudflare HTTP trafic, the path is¬†`/api/sms/send.json`

![image.png](attachment:73272c18-c442-471e-a1ab-5d03d689d4b6:image.png)

Related to following¬†[logs](https://app.datadoghq.eu/logs?query=env%3Aprod%20service%3Aold-dashboard%20%22API%20cant%20send%20SMS%22&agg_m=count&agg_m_source=base&agg_t=count&clustering_pattern_field_path=message&cols=host%2Cservice%2C%40audit.email%2C%40audit.request.ip&fromUser=true&messageDisplay=inline&refresh_mode=sliding&storage=hot&stream_sort=desc&viz=stream&from_ts=1745569015771&to_ts=1745655415771&live=true). The error is following:

```
API cant send SMS: Client error: `POST https://api.cloudtalk.io/api/sms/send.json` resulted in a `400 Bad Request` response:
{""success"":false,""data"":{""message"":""Bad number configuration"",""url"":""\/api\/sms\/send.json"",""code"":400,""internalCode"":10 (truncated...)

```

and:

```
API cant send SMS: Client error: `POSThttps://api.cloudtalk.io/api/sms/send.json` resulted in a `400 Bad Request` response:
{""success"":false,""data"":{""message"":""SMS send failed: [HTTP 429] Unable to create record: This \u0027From\u0027 number ha (truncated...)

```

HTTP Status code:¬†`HTTP 429`¬†- Too Many Requests¬†(edited)

# Leadup

1. Company¬†`298427`¬†was sending too many SMS requests. It appeared like some campaign. They were sending the same SMS text about new feature they just launched. So we believe that there was some job that is sending SMS to all contacts.
2. [Public API](https://app.datadoghq.eu/logs?query=env%3Aprod%20service%3Aold-dashboard%20%22API%20cant%20send%20SMS%22&agg_m=count&agg_m_source=base&agg_t=count&clustering_pattern_field_path=message&cols=host%2Cservice%2C%40audit.email%2C%40audit.request.ip&fromUser=true&messageDisplay=inline&refresh_mode=sliding&storage=hot&stream_sort=desc&viz=stream&from_ts=1745569015771&to_ts=1745655415771&live=true)¬†server resend them to the APIv3
3. APIv3 got¬†[rate limited by Twillio](https://app.datadoghq.eu/logs?query=env%3Aprod%20service%3Aapi-v3&agg_m=count&agg_m_source=base&agg_t=count&cols=host%2Cservice&messageDisplay=inline&refresh_mode=sliding&storage=hot&stream_sort=desc&viz=stream&from_ts=1745570594498&to_ts=1745656994498&live=true)¬†so SMSs were not being sent immediately.¬†[DD Traces](https://app.datadoghq.eu/apm/traces?query=env%3Aprod%20service%3Aapi-v3%20resource_name%3A%22POST%20SmsController%40send%22%20%40company_id%3A298427&agg_m=count&agg_m_source=base&agg_t=count&cols=core_service%2Ccore_resource_name%2Clog_duration%2Clog_http.method%2Clog_http.status_code%2C%40context.company_api_key%2C%40context.company_id%2C%40http.client_ip&fromUser=false&graphType=span_list&historicalData=true&messageDisplay=inline&panel_tab=flamegraph&query_translation_version=v0&shouldShowLegend=true&sort=time&spanType=all&spanViewType=metadata&storage=hot&view=spans&start=1745646128204&end=1745660528204&paused=false)
4. Public API server is overloaded as a result

# Fault

The implemented logical flow did not take into account rate limitations on the Twillio side and we were not able to handle the repeated failed requests.

The overloads also caused 

- delays on billing for more than 2 hours
- API v1 to have bad responses which caused the aspi server to run on 100% utilizaiton

# Detection

CPU monitoring - **HighCpuLoad95 - admin and api server overloaded**

Additional connected incident alerts were also raised around the same time, alerting us to something going wrong

- Connected incidents
    - [**ResqueDelayedJobs225**](https://app.incident.io/cloudtalkio/alerts/01JSSTD461SMQPBRFGJBS1KWGW/details)
        
        [Prometheus Alertmanager](http://monitoring-eu-2.cloudtalk.io/alertmanager)
        
        1 day ago
        
    - [**ElasticacheMemoryUtilization95**](https://app.incident.io/cloudtalkio/alerts/01JSSR35H3K94EKJ7CGRXR1HXR/details)
        
        [Prometheus Alertmanager](http://monitoring-eu-2.cloudtalk.io/alertmanager)
        
        1 day ago
        
    - [**RunningOutOfMemory7**](https://app.incident.io/cloudtalkio/alerts/01JSRZRAF1JDE3467AYAYAC8FA/details)
        
        [Prometheus Alertmanager](http://monitoring-eu-2.cloudtalk.io/alertmanager)
        
        2 days ago
        
    - [**RunningOutOfMemory10**](https://app.incident.io/cloudtalkio/alerts/01JSRZRAFECXP94Z9N69K8SPH4/details)
        
        [Prometheus Alertmanager](http://monitoring-eu-2.cloudtalk.io/alertmanager)
        
        2 days ago
        
    - [**RunningOutOfMemory5**](https://app.incident.io/cloudtalkio/alerts/01JSRZRAF8Z2Y0MNTTQH2QP5XP/details)
        
        [Prometheus Alertmanager](http://monitoring-eu-2.cloudtalk.io/alertmanager)
        
        2 days ago
        
    - [**HighLoadAverage50**](https://app.incident.io/cloudtalkio/alerts/01JSRZZMTYEVEKJ3MMD8VR4MG4/details)
        
        [Prometheus Alertmanager](http://monitoring-eu-2.cloudtalk.io/alertmanager)
        
        2 days ago
        
    - [**RunningOutOfMemory2**](https://app.incident.io/cloudtalkio/alerts/01JSS054KCF9QEEXKAM6WHG872/details)
        
        [Prometheus Alertmanager](http://monitoring-eu-2.cloudtalk.io/alertmanager)
        
        2 days ago
        
    - [**HighCpuLoad95**](https://app.incident.io/cloudtalkio/alerts/01JSQ8J8T2ND795A4M2XDHBQWH/details)
        
        [Prometheus Alertmanager](http://monitoring-eu-2.cloudtalk.io/alertmanager)
        
        2 days ago
        

# 5 why's analysis

### **Why was the API server overloaded?**

Because it received an **unusually high volume of SMS requests** from a single customer (Company 298427), which saturated processing resources.

> Evidence: Logs showed a high frequency of requests to /api/sms/send.json originating from one company, suggesting an automated campaign.
> 

---

### **Why were so many SMS requests being sent?**

Because Company 298427 was likely running a **broadcast campaign** via an automated job that attempted to send the same SMS to all its contacts.

> Evidence: Patterns in the request logs and text indicate a mass distribution effort.
> 

---

### **Why were these requests not throttled or rejected earlier?**

Because the **Public API did not enforce rate limiting at this scale**, and **messages were blindly forwarded to APIv3**, overwhelming downstream systems.

> Supporting Note: Though a rate limiter exists on the Public API (60 req/min), it was either not enforced effectively or bypassed by job-level logic.
> 

---

### **Why didn‚Äôt the system handle Twilio rate limits and failures gracefully?**

Because **APIv3 lacked congestion control mechanisms**, such as an intelligent **SMS queue with retry/backoff strategies**, resulting in a flood of failed retries and 429 responses.

> Root Cause Note: The message queue design did not account for **Twilio‚Äôs queuing and rate limits**
> 

---

### **Why was there no proactive safeguard against this scenario?**

Because **system design did not include guardrails or dynamic scaling triggers** for mass outbound traffic from individual tenants, and there was **no automatic isolation or alerting for traffic anomalies** from a single company.

> Lesson Learned: This highlights a **lack of tenant-based circuit breakers**
> 

# Root cause

- Absence of effective tenant-level rate limiting and throttling.
- No backpressure or queuing logic in APIv3 to handle external SMS gateway limitations.
- Public API forwarded requests without validation or load anticipation.
- Twilio‚Äôs 429 (rate limit) errors were retried instead of triggering fallback behavior.
- System resource provisioning was not dynamically responsive to such a burst.

# Mitigation and resolution

## **Immediate mitigation:**

1. api server - I‚Äôve increased instance size to¬†`c5.4xlarge`¬†and utilization decrease to 80%
2. admin server - low memory, changed size to¬†`c5.9xlarge`
3. php resque redis - instance size changed to `m6g.xlarge`, php resque restarted, redis key `resque:ResqueSchedulerWorker` deleted

## **Long term resolution / next steps to consider:**

1. **Introduce per-tenant SMS throttling** at the Public API and APIv3 levels.
2. **Implement a centralized SMS dispatch queue** with retry/backoff handling and dead-letter support.
3. **Add circuit breakers and anomaly detection** for unusual traffic from individual customers.
4. **Improve rate limit feedback loop** between Twilio and CloudTalk services.
5. **Create visibility dashboards** for outbound messaging load by tenant.

# Lessons learnt

- Incident was was handled well. Messages and updates were provided in a timely manner. Infra addressed immediate capacity issues while on-call dealt with the root cause.
- Created Jira story to address the SMS rate limiter https://cloudtalk.atlassian.net/browse/APPS-1382 which will determine whether to address Messaging Service or APIv3 in some way to limit the number of messages being sent at any one time.

 

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-04-25** |  |
| 22:00:03 | **Incident reported in triage by Prometheus Alertmanager alert**
‚Äã
Prometheus Alertmanager alert reported the incident
Severity: None
Status: Triage |
| 22:16:33 | **Incident accepted**
‚Äã
Jirka Srba shared an update
Severity: ~~None~~ ‚Üí P4
Status: ~~Triage~~ ‚Üí Investigating |
| 22:22:22 | **Incident resolved and entered the post-incident flow**
‚Äã
Jirka Srba shared an update
Status: ~~Investigating~~ ‚Üí Documenting |
| **2025-04-26** |  |
| 07:22:11 | **Incident re-opened**
‚Äã
Jirka Srba shared an update
Status: ~~Documenting~~ ‚Üí Monitoring |
| 2h later |  |
| 10:17:13 | **Message from Peter Stanko**
‚Äã
Peter Stanko pinned their own message

IMO this will be related to following logs (https://app.datadoghq.eu/logs?query=env%3Aprod%20service%3Aold-dashboard%20%22API%20cant%20send%20SMS%22&agg_m=count&agg_m_source=base&agg_t=count&clustering_pattern_field_path=message&cols=host%2Cservice%2C%40audit.email%2C%40audit.request.ip&fromUser=true&messageDisplay=inline&refresh_mode=sliding&storage=hot&stream_sort=desc&viz=stream&from_ts=1745569015771&to_ts=1745655415771&live=true)

The error is following:

```
API cant send SMS: Client error: `POST https://api.cloudtalk.io/api/sms/send.json` resulted in a `400 Bad Request` response:
{""success"":false,""data"":{""message"":""Bad number configuration"",""url"":""\/api\/sms\/send.json"",""code"":400,""internalCode"":10 (truncated...)
```

and:

```
API cant send SMS: Client error: `POST https://api.cloudtalk.io/api/sms/send.json` resulted in a `400 Bad Request` response:
{""success"":false,""data"":{""message"":""SMS send failed: [HTTP 429] Unable to create record: This \u0027From\u0027 number ha (truncated...)
```

HTTP Status code: `HTTP 429` - Too Many Requests |
| 10:23:41 | **Message from Peter Stanko**
‚Äã
Peter Stanko pinned their own message

Here are traces: https://app.datadoghq.eu/apm/traces?query=env%3Aprod%20service%3Adashboard%20%40http.url%3A%22https%3A%2F%2Fmy.cloudtalk.io%2Fapi%2Fsms%2Fsend.json%22&agg_m=count&agg_m_source=base&agg_t=count&cols=core_service%2Ccore_resource_name%2Clog_duration%2Clog_http.method%2Clog_http.status_code%2C%40context.company_api_key%2C%40context.company_id&fromUser=false&graphType=span_list&historicalData=false&messageDisplay=inline&query_translation_version=v0&shouldShowLegend=true&sort=desc&spanType=all&storage=hot&view=spans&start=1745654715425&end=1745655615425&paused=false

Almost all failiures are for company: `298427` - Draiver
We can see it on this graph (https://app.datadoghq.eu/apm/traces?query=env%3Aprod%20service%3Adashboard%20%40http.url%3A%22https%3A%2F%2Fmy.cloudtalk.io%2Fapi%2Fsms%2Fsend.json%22&agg_m=count&agg_m_source=base&agg_q=%40context.company_id&agg_t=count&analyticsOptions=%5B%22bars%22%2C%22dog_classic%22%2Cnull%2Cnull%2C%22value%22%5D&cols=core_service%2Ccore_resource_name%2Clog_duration%2Clog_http.method%2Clog_http.status_code%2C%40context.company_api_key%2C%40context.company_id&fromUser=false&graphType=span_list&historicalData=true&messageDisplay=inline&query_translation_version=v0&shouldShowLegend=true&sort=desc&spanType=all&spanViewType=metadata&storage=hot&view=spans&viz=sunburst&start=1745569335408&end=1745655735408&paused=false) |
| 10:29:55 | **Message from Peter Stanko**
‚Äã
Peter Stanko pinned their own message

Anyway it looks like the company is rate limited (https://app.datadoghq.eu/apm/traces?query=env%3Aprod%20service%3Adashboard%20resource_name%3A%22api_sms%2Fsend%22&agg_m=count&agg_m_source=base&agg_q=%40context.company_id&agg_q_source=base&agg_t=count&analyticsOptions=%5B%22bars%22%2C%22dog_classic%22%2Cnull%2Cnull%2C%22value%22%5D&cols=core_service%2Ccore_resource_name%2Clog_duration%2Clog_http.method%2Clog_http.status_code%2C%40context.company_api_key%2C%40context.company_id&filteredType=web&fromUser=false&graphType=span_list&historicalData=true&messageDisplay=inline&query_translation_version=v0&shouldShowLegend=true&sort=desc&spanType=all&spanViewType=metadata&storage=hot&top_n=10&top_o=top&view=spans&viz=stream&x_missing=true&start=1745569703335&end=1745656103335&paused=false), but tbh I am not sure how this SMS sending logic works :sadpepe:  I need to go to Old-Dashboard code to investigate

EDIT:
From the code it looks like we are receiving the too many requests either from APIv3 or directly from twillio. |
| 10:46:39 | **Message from Peter Stanko**
‚Äã
Peter Stanko pinned their own message*OK so the flow is following -> API server sends requests to APIv3:
APIv3 logs are here and are failing with <https://app.datadoghq.eu/logs?query=env%3Aprod%20service%3Aapi-v3&agg_m=count&agg_m_source=base&agg_t=count&cols=host%2Cservice&messageDisplay=inline&refresh_mode=sliding&storage=hot&stream_sort=desc&viz=stream&from_ts=1745570594498&to_ts=1745656994498&live=true|logs>

`SMS send failed: [HTTP 429] Unable to create record: This 'From' number has exceeded the maximum number of queued messages`

<https://app.datadoghq.eu/watchdog/story/1fff5b2d-93a4-577c-b7bf-9243124f4f6d|DD Watchdog>

but what I am really scared of it following error message:
> `SMS send failed: cURL error 60: SSL certificate problem: certificate has expired (see <http://curl.haxx.se/libcurl/c/libcurl-errors.html>)`
Which stated TODAY! <!subteam^S088S4NQAQH> we have to check whether we are even able to send SMS on production* |
| 10:50:08 | **Image posted by Peter Stanko**
‚Äã
Peter Stanko posted an image to the channel*<!subteam^S061NC3K1QE> and <!subteam^S026QA4N10D> SMS sending might not be working for everyone on production right now.

<https://app.datadoghq.eu/logs?query=env%3Aprod%20service%3Aapi-v3%20%22SSL%20certificate%20problem%22&agg_m=count&agg_m_source=base&agg_t=count&cols=host%2Cservice&event=AwAAAZZxSStgMWvPzQAAABhBWlp4U1RhMUFBQ05IUVFQMVJvQzJBQUYAAAAkMDE5NjcxNGEtODNhZC00NzBmLTliODUtZjI0NWNjMTdhNmU0AAAAHQ&messageDisplay=inline&refresh_mode=sliding&storage=hot&stream_sort=desc&viz=stream&from_ts=1745653795620&to_ts=1745657395620&live=true|Logs>*
[link to message](https://cloudtalkio.slack.com/archives/C08PB9P4PC7/p1745657408359209) |
| 11:14:43 | **Message from Peter Stanko**
‚Äã
Peter Stanko pinned their own message

[Summary]
Here is a summary of the investigation:

1. Company `298427` sends too many SMS requests
2. Public API (https://app.datadoghq.eu/logs?query=env%3Aprod%20service%3Aold-dashboard%20%22API%20cant%20send%20SMS%22&agg_m=count&agg_m_source=base&agg_t=count&clustering_pattern_field_path=message&cols=host%2Cservice%2C%40audit.email%2C%40audit.request.ip&fromUser=true&messageDisplay=inline&refresh_mode=sliding&storage=hot&stream_sort=desc&viz=stream&from_ts=1745569015771&to_ts=1745655415771&live=true) server resend them to the APIv3
3. APIv3 got rate limited by Twillio (https://app.datadoghq.eu/logs?query=env%3Aprod%20service%3Aapi-v3&agg_m=count&agg_m_source=base&agg_t=count&cols=host%2Cservice&messageDisplay=inline&refresh_mode=sliding&storage=hot&stream_sort=desc&viz=stream&from_ts=1745570594498&to_ts=1745656994498&live=true) so SMSs are not being sent immediatelly. DD Traces (https://app.datadoghq.eu/apm/traces?query=env%3Aprod%20service%3Aapi-v3%20resource_name%3A%22POST%20SmsController%40send%22%20%40company_id%3A298427&agg_m=count&agg_m_source=base&agg_t=count&cols=core_service%2Ccore_resource_name%2Clog_duration%2Clog_http.method%2Clog_http.status_code%2C%40context.company_api_key%2C%40context.company_id%2C%40http.client_ip&fromUser=false&graphType=span_list&historicalData=true&messageDisplay=inline&panel_tab=flamegraph&query_translation_version=v0&shouldShowLegend=true&sort=time&spanType=all&spanViewType=metadata&storage=hot&view=spans&start=1745646128204&end=1745660528204&paused=false)
4. Public API server is overloaded :sadblob: 
[SSL Certificate error]
This looks like is a problem with VoxBone only (https://app.datadoghq.eu/logs?query=env%3Aprod%20service%3Aapi-v3%20%22SSL%20certificate%20problem%22&agg_m=count&agg_m_source=base&agg_t=count&clustering_pattern_field_path=message&cols=host%2Cservice%2C%40context.company_id&event=AwAAAZZxW7cAU5SmVQAAABhBWlp4VzdteUFBQkFuOG4tc1hTb2VRQWgAAAAkMDE... |
| 11:52:48 | **Incident paused**
‚Äã
Jirka Srba shared an update
Status: ~~Monitoring~~ ‚Üí Paused |
| 2d later |  |
| **2025-04-28** |  |
| 07:44:32 | **Severity upgraded from P4 ‚Üí P2**
‚Äã
Jirka Srba shared an update
Severity: ~~P4~~ ‚Üí P2 |
| 08:01:32 | **Incident resumed**
‚Äã
incident.io shared an update
Status: ~~Paused~~ ‚Üí Monitoring |
| 08:01:59 | **Incident resolved and entered the post-incident flow**
‚Äã
Jirka Srba shared an update
Status: ~~Monitoring~~ ‚Üí Documenting |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/810?tab=follow-ups)

- **APIv3: sending SMS using VoxBone is not working**
    - **Owner: @Michal Ustanik**
    - **JIRA ticket:** https://cloudtalk.atlassian.net/browse/APPS-1373
- **Decrease php resque redis back to cache.t4g.small**
    - **Owner:** @Jiri Srba
    - **JIRA ticket:** https://cloudtalk.atlassian.net/browse/INFRA-2490
- **Check voxbone certificate in api-v3**
    - **Owner:** @Michal Ustanik
    - **JIRA ticket:** https://cloudtalk.atlassian.net/browse/APPS-1372
- **Change AWS ec2 instance ct-prod-eu-api-1 size to c5.4xlarge**
    - **Owner:** @Jiri Srba
    - **JIRA ticket:** https://cloudtalk.atlassian.net/browse/INFRA-2489",P2,API V3,Dashboard,BIT Devs,"Performance, Architecture weakness",https://www.notion.so/INC-810-HighCpuLoad95-admin-and-api-server-overloaded-1e32bccf284f814d9cecf1a929154af0,2025-04-25T20:00:03.613Z
856,inc-856-phadmin-timing-out-19-05-2025,"*Generated by* @Jozef Valko *via [incident.io](https://app.incident.io/cloudtalkio/incidents/856) on* May 19, 2025 11:32 AM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Jozef Valko
- **Reporter:** @Ji≈ô√≠ Missbach
- **Incident commander:** @Arnaldo Tema
- **Active participants:** @Jozef Valko, @Lucia Mlyn√°rov√°, @Debby Wu, @Jiri Srba, @Ji≈ô√≠ Missbach, @Jaroslav Tomeƒçek, @Arnaldo Tema
- **Observers:** @Peter Stanko, @Zoltan Viragh, @Erica  Hoelper, @Filip Prosovsky, @Jakub Bober, @Nikoleta Menyhartova, @Matej Zboron, @An≈æe ≈†u≈°tar

### üìù Custom Fields

- **Affected teams:** [BIT Devs](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP17YAG9JGWWCMSVWPF)
- **Affected services:** [Dashboard](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDETDDN3FY1ZGXQ91KH6A)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/856)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C08TBAFPMNV)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** May 19, 2025 9:18 AM (GMT+2)
- **Resolved at:** May 19, 2025 11:25 AM (GMT+2)
- **Impact started at:** May 19, 2025 8:45 AM (GMT+2)
- **Identified at:** May 19, 2025 9:30 AM (GMT+2)
- **Fixed at:** May 19, 2025 9:30 AM (GMT+2)

### ‚è≥ Durations

- **Incident duration: 36** minutes

# üñäÔ∏è Summary

**Problem:** The my.cloudtalk.io/phadmin/companies page was not loading due to HTTP ERROR 500, indicating a timeout issue.

**Impact:** Support and carrier operations were unable to work, affecting multiple users experiencing the same issue between 8:45 and 9:23. Some PHP processes were unable to allocate memory meaning that the part of API calls (SMS included) were throwing 500 errors for clients.

**Root cause:** The problem was caused by memory limitations on the admin server, specifically after memory limits were changed during the last weekend. The admin server was reloaded at 8:45 to enable PHP limits and default PHP-FPM limit was enabled.

**Steps to resolve:** Reverting memory limits for PHP processes to 2G per process, and the issues were resolved after this change. A temporary increase in server instance size has been applied to mitigate the issue.

# Leadup

There are CPU and memory issues ongoing with Admin and API servers for a couple of weeks now. On Saturday Admin server once again was running out of memory. As a hotfix the EC2 instance was increased to `c5.9xlarge` to handle the load. Furthermore, PHP-FPM limits were set to be enabled on Monday morning.

# Fault

PHP-FPM memory limits were enabled leading to multiple processes failing to allocate more memory as default limit 128MB for PHP-FPM was unintentionally enabled. This caused 500 errors on multiple pages served by admin EC2 instance.

# Detection

@Ji≈ô√≠ Missbach on behalf of support raised the incident, there were no alerts for this issue.

# 5 why's analysis

1. Why PHadmin pages were not loading?
    1. Because PHP-FPM processes were unable to allocate more memory.
2. Why processes were unable to allocate more memory?
    1. Because PHP-FPM limits were enabled applying default limit of 128MB memory per process
3. Why were limits applied
    1. Because we were aiming on protecting the EC2 instance from memory swapping as current global limit of 2GB per process with possible 200 processes means that ideally we would need 400GB RAM on the instance

# Root cause

The root cause is ongoing resource issues with SMS endpoints and billing which is all handled by 2 EC2 instances and sometimes exhausting the resources completely - meaning that affected instances are not reachable at all.

# Mitigation and resolution

Incident was resolved by removing PHP-FPM limits and leaving only global 2GB limit per PHP process.

# Lessons learnt

1. We should communicate changes more clearly and have the clear roadmap of the issue resolution.
2. Legacy configuration is not versioned, we are changing config files directly on the instance level.
3. We‚Äôll communicate the proper HW requirements for Admin and API servers with owning teams and will set the resources and limits to avoid such failures in the future.
4. We were missing alert about ongoing issue from the logs - https://cloudtalk.atlassian.net/browse/BIT-3168
    1. API in old-dashboard is also not covered by synthetics check - https://cloudtalk.atlassian.net/browse/BIT-3177

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-05-19** |  |
| 09:18:41 | **Incident reported by Jiri Missbach**
‚Äã
Jiri Missbach reported the incident
Severity: P2
Status: Investigating |
| 09:56:52 | **Message from Jozef Vaƒæko**
‚Äã
Jozef Vaƒæko pinned their own message

PHP allocated memory exhausted :crycat:

```
2025/05/19 08:45:54 [error] 1198#1198: *554250 FastCGI sent in stderr: ""PHP message: PHP Fatal error:  Allowed memory size of 134217728 bytes exhausted (trie
d to allocate 67076080 bytes) in /usr/share/nginx/www/pbx/lib/Cake/Cache/Engine/FileEngine.php on line 192
PHP message: PHP Fatal error:  Allowed memory size of 134217728 bytes exhausted (tried to allocate 39842548 bytes) in /usr/share/nginx/www/pbx/lib/Cake/Cache
/Engine/FileEngine.php on line 192"" while reading response header from upstream, client: 10.1.34.147, server: my.cloudtalk.io (http://my.cloudtalk.io), request: ""GET /c/show/84963180
5 HTTP/1.1"", upstream: ""fastcgi://unix:/var/run/php/php7.2-fpm.sock:"", host: ""my.cloudtalk.io (http://my.cloudtalk.io)""
```
 |
| 11:00:21 | **Message from Jozef Vaƒæko**
‚Äã
Jozef Vaƒæko pinned their own message

This morning we‚Äôve applied memory limit for PHP processes to be at 2G per process. This although applied also default limit per PHP-FPM process which is 128MB per process. This caused issues this morning between 8:45 - 9:23. From 9:23 onwards the traffic is routed to API server which doesn‚Äôt have limits yet.
We are now reverting the limits. |
| 11:21:33 | **Message from Jozef Vaƒæko**
‚Äã
Jozef Vaƒæko pinned their own message

‚ÑπÔ∏è status page updated
https://status.cloudtalk.io/incidents/01JVKXH09GM18ZWK0G0F5W0XN5 |
| 11:25:04 | **Incident resolved and entered the post-incident flow**
‚Äã
Jozef Vaƒæko shared an update
Status: ~~Investigating~~ ‚Üí Documenting
This morning, a new memory limit for PHP processes was applied, setting a default of 128MB per PHP-FPM process. This led to widespread errors and timeouts for PHadmin and API endpoints between 8:45 and 9:23.
From 9:23 onward, traffic was rerouted to an API server without these limits, and service recovered. We are now reverting the memory limit changes.
Multiple customers were affected, and support teams are being informed. A public status page update has been posted. |",P2,Infrastructure,Dashboard,BIT Devs,"Performance, Regression, Infrastructure",https://www.notion.so/INC-856-Phadmin-timing-out-1f82bccf284f8170b4e7c945f55c77aa,2025-05-19T07:18:41.012Z
866,inc-866-realtime-api-respoinding-with-500-code-02-06-2025,"*Generated by* @Filip Prosovsky *via [incident.io](https://app.incident.io/cloudtalkio/incidents/866) on* June 2, 2025 3:22 PM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Filip Prosovsky
- **Reporter:** @Filip Prosovsky
- **Incident commander:** @Josef Lapka
- **Active participants:** @Michal Popoviƒç, @Josef Lapka, @Filip Prosovsky
- **Observers:** @Peter Stanko, @Zoltan Viragh, @Erik Dvorcak, @Jaroslav Tomeƒçek, @Jakub Bober, @Michal Ustanik, Krist√≠na Barr√©, @Debby Wu, @Lucia Mlyn√°rov√°

### üìù Custom Fields

- **Affected teams:** [App Devs](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1Y13N5ZYW63255X4W)
- **Affected services:** [Realtime API](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDYE8FX397RV4NAKPJ2TJ)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/866)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C09096F0DQ8)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** June 2, 2025 2:32 PM (GMT+2)
- **Resolved at:** June 2, 2025 3:15 PM (GMT+2)
- **Impact started at:** June 2, 2025 2:04 PM (GMT+2)
- **Identified at:** June 2, 2025 2:38 PM (GMT+2)
- **Fixed at:** June 2, 2025 3:08 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 1 hours, 4 minutes

# üñäÔ∏è Summary

**Problem:** The realtime-api is responding with 500 errors due to issues starting containers in pods.

**Impact:** This problem results in 500 errors for live requests.

**Root cause:** The issue was caused by a change in the image that was not tested on development or staging environments, being assumed too trivial to require such testing.

**Steps to resolve:** All realtime-api, realtime-ws, and campaigns were affected by this change, but only realtime-api encountered issues with cors and starting containers.

# Leadup

Realtime-api and realtime-ws started to have 0 replicas on prod. This leaded to not functioning endpoints on Phone app. 

# Fault

Updating of CORS values for services that are used by Phone app. This change was important to make e2e tests running inside github actions - https://github.com/CloudTalk-io/argocd-kube/pull/4971

However fault values were released for those 2 service since they are using one line string for CORS config. This PR changed the configuration to multi line array. 

From that services couldnt start and behaved as they are ready. Kube removed old pods and tried to start new ones with faulty env variable.

# Detection

Realtime-api started to respond with 500 errors. 

# 5 why's analysis

Why there was 500 on realtime-api? 

Its because it had faulty deployment env variable.

Why it has faulty deployment variable? 

Because we moved from one line string for CORS to multiline array.

Why we moved to multiline array? 

Because we wanted to keep consistency accross our repos. This was also release on all env at once.

Why it was release to all env at once? 

Because it seemed like trivial change and dev didnt have knowledge of this config change might not be backward compatible

# Root cause

Issue is with wrong config for CORS for 2 services. It added there also fact that dev didnt have knowledge that migrating from one line string could break app. 

However there is seconds root cause, which is that app should be in ready state for kube if it doesnt have proper config.

# Mitigation and resolution

Revert back config for those 2 services - https://github.com/CloudTalk-io/argocd-kube/pull/4999

# Lessons learnt

We learned

- even trivial change should go gradually through dev ‚Üí stage ‚Üí prod env, not all at once
- services should not be in ready state if there is misconfiguration

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-06-02** |  |
| 15:06:13 | **Incident reported by Filip Pro≈°ovsk√Ω**
‚Äã
Filip Pro≈°ovsk√Ω reported the incident
Severity: P2
Status: Investigating |
| 15:14:21 | **Message from Filip Pro≈°ovsk√Ω**
‚Äã
Filip Pro≈°ovsk√Ω pinned their own message

All realtime-api, realtime-ws and campaigns were affected by this change of the image. Only realtime-api had a problem of not checking the cors, starting the containers in pods, resulting in 500 errors for healthy pods to live requests. Realtime-ws and Campaigns still had healthy pods.

Root cause: not tested on dev, stage - assumed to trivial change.
Action: Rollback |
| 15:15:28 | **Incident resolved and entered the post-incident flow**
‚Äã
Filip Pro≈°ovsk√Ω shared an update
Status: ~~Investigating~~ ‚Üí Documenting |
",P2,Realtime API,Realtime API,App Devs,"Insufficient testing, Release problems",https://www.notion.so/INC-866-Realtime-api-respoinding-with-500-code-2062bccf284f8191981defc559390b05,2025-06-02T12:32:00.000Z
912,inc-912-index-user_endpoints-is-corrupted-16-06-2025,"*Generated by* @Filip Prosovsky *via [incident.io](https://app.incident.io/cloudtalkio/incidents/912) on* June 18, 2025 4:22 PM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Database
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Filip Prosovsky
- **Reporter:** @Michal Ustanik
- **Active participants:** @Jakub Smadis, @Peter Stanko, @Michal Ustanik, @Elena Darriba, @Serhii Shevchenko, @Filip Prosovsky, @Jaroslav Tomeƒçek, @Roman Hartig, @Jozef Valko, @Jiri Srba
- **Observers:** @Anton Shyrokoborodov, @Lucia Mlyn√°rov√°, @Simona Drinkova, @Jergus Kacmar, @Debby Wu, @Arnaldo Tema, @Erik Dvorcak, @Zoltan Viragh, @Jakub Bober, @Teodor Lilov, @Tomas Sykora, @daniel malachovsky, @Andras Szabo

### üìù Custom Fields

- **Caused by:** [MariaDB database](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JWR7AZNDSRZRJ6AZMYDGVKKY)
- **Affected services:** [Realtime API](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDYE8FX397RV4NAKPJ2TJ), [Statistics API](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRE3S5Z7T4B8FJAMASSX8R), [Dashboard Backend](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDF811D4F519DG66R632Q) and [API V3](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRCZ6N3WW6B31CNBFVHJ42)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/912)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C091EJD2C9L)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** June 16, 2025 1:19 PM (GMT+2)
- **Resolved at:** June 18, 2025 3:10 PM (GMT+2)
- **Impact started at:** June 16, 2025 8:19 AM (GMT+2)
- **Identified at:** June 16, 2025 1:00 PM (GMT+2)
- **Fixed at:** June 16, 2025 1:24 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 5 hours, 5 minutes

# üñäÔ∏è Summary

**Problem:** The index on the table `user_endpoints` got corrupted.

**Impact:** Customers experienced increased error rates, resulting in 50x response codes across services such as dashboards and the realtime-api, affecting data processing and application responses.

**Root cause:** The root cause remains unidentified.

**Steps to resolve:** The corrupted index has been fixed.
Comprehensive check of all tables in mariadb db-1 was run ‚áí no other corrupted indexes found.
The mitigation plan is to upgrade mariadb to the latest minor version alongside rebooting db-1 to cleanup database cache.

# Leadup

**Affected System:** MariaDB InnoDB Storage Engine ct-prod-eu-db-1

**Affected Table:** `cloudtalk.user_endpoints`

**Affected Index:** `device_delete`

**Error Log Extract:**

```
2025-06-16 13:00:55 0 [ERROR] InnoDB: tried to purge non-delete-marked record in index `device_delete` of table `cloudtalk`.`user_endpoints`: tuple:
TUPLE (info_bits=0, 3 fields): {[4] (0x0005958D),[4] (0x80000001),[4] X9(0x80105839)}, record: COMPACT RECORD(info_bits=0, 3 fields): {[4]
(0x0005958D),[4] (0x80000001),[4] X9(0x80105839)}
2025-06-16 13:00:55 0 [ERROR] InnoDB: Flagged corruption of `device_delete` in table `cloudtalk`.`user_endpoints` in purge
```

# Fault

MariaDB InnoDB reported corruption in the secondary index `device_delete` of the `cloudtalk.user_endpoints` table during a purge operation. The corruption occurred when the InnoDB purge thread attempted to delete a record that was not marked as deleted, indicating a mismatch in the undo/redo logs or a corrupted index page.

Queries relying on the `device_delete` index returned incorrect results or cause errors.

# Detection

Monitoring / Datadog Application logs.

# Root cause

Unknown

Possible root causes:
- unclean session termination during the deleting data
- memory failure in database cache
- mariadb bugs
- filesystem corruption

# Mitigation and resolution

- Index and table was rebuilt successfully without further corruption
- Run data integrity `mysqlcheck` on all mariadb database table ‚áí no issue detected
- Ensure the mariadb database parameter `innodb_checksum_algorithm` is set to `crc32`
- Upgrade mariadb to latest stable version version `10.6.22`
- Reboot database server db-1 to cleanup memory

# Lessons learnt

Nothing

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-06-16** |  |
| 13:19:50 | **Incident reported by Michal Ustan√≠k**

Michal Ustan√≠k reported the incident
Severity: P2
Status: Investigating |
| 13:53:00 | **Message from Peter Stanko**

Peter Stanko's message was pinned by Filip Pro≈°ovsk√Ω

Regarding the archivation process we have started with phase 3 last week Tuesday.
Here are the logs for asterisk_queue_log (https://app.datadoghq.eu/logs?query=env%3Aprod%20service%3Adb-archiver%20%40table%3Aasterisk_queue_log&agg_m=count&agg_m_source=base&agg_t=count&clustering_pattern_field_path=message&cols=host%2Cservice%2C%40table&messageDisplay=inline&refresh_mode=sliding&storage=hot&stream_sort=desc&viz=stream&from_ts=1749469800934&to_ts=1750074600934&live=true)

We are trying to identify how this might be related to `user_endpoints` |
| 13:57:19 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

Tech situation room:

We will stop db-archivation job for now. There is a possibility that these indexes were corrupted because of deleting asterisk_queue_log table. We need to investigate further and avoid this happen again |
| 13:58:02 | **Update shared**

Filip Pro≈°ovsk√Ω shared an update
The corrupted index on the user_endpoints table has been fixed. We are investigating the root cause, as similar index issues occurred over the weekend and today.

There has been an increased error rate for the api-v3 service and slow queries in statistics-api since this morning. The team will pause the database archivation job, as it may be related to the index corruption, and is continuing to investigate to prevent recurrence. |
| 14:25:41 | **Message from Jirka Srba**

Jirka Srba's message was pinned by Filip Pro≈°ovsk√Ω

job `db-archiver` stopped |
| 14:26:39 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

@Jirka Srba Will create snapshot and replicate db-4 to db-1 so we can run check against our database to avoid decreasing performance on master DB. Db-archiver will be stopped until the DB is checked. There is still possibility that when we check it without issues, run archiver again, it might happen again. |
| 2d later |  |
| **2025-06-18** |  |
| 15:08:21 | **Message from Jirka Srba**

Jirka Srba pinned their own message

@Filip Pro≈°ovsk√Ω yes, we were discussed and archiver will be enabled tomorrow |
| 15:09:00 | **Message from Jirka Srba**

Jirka Srba pinned their own message

root cause - unknown until now

mitigation - upgrade mariadb to latest minor version together with reboot db-1 |
| 15:10:17 | **Incident resolved and entered the post-incident flow**

Filip Pro≈°ovsk√Ω shared an update
Status: ~~Investigating~~ ‚Üí Documenting
Verification of all tables on db-1 and db-4 is complete, with no corruption detected. We have not found a direct link between the db-archiver job and the user_endpoints table issue.
The db-archiver job will be re-enabled tomorrow. As a preventive measure, we plan to upgrade MariaDB to version 10.6.22 and restart db-1 to clean up memory. |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/912?tab=follow-ups)

- **Upgrade mariadb databases to latest minor version 10.6.22**
    - **Owner:** Unassigned
    - **JIRA ticket:** [INFRA-2607](https://cloudtalk.atlassian.net/browse/INFRA-2607)
- **Create database db-1 disk snapshot, mount it on db-4 and check for corruption**
    - **Owner:** @Jiri Srba
    - **JIRA ticket:** [INFRA-2603](https://cloudtalk.atlassian.net/browse/INFRA-2603)",P2,MariaDB database,"Realtime API, Statistics API, Dashboard Backend, API V3",,Database issues,https://www.notion.so/INC-912-Index-user_endpoints-is-corrupted-2162bccf284f8180adefcd810fecbd1e,2025-06-16T11:19:50.238Z
934,inc-934-sso-does-not-work-24-06-2025,"*Generated by* @Filip Prosovsky *via [incident.io](https://app.incident.io/cloudtalkio/incidents/934) on* June 24, 2025 9:57 AM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Infrastructure
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Filip Prosovsky
- **Reporter:** @Filip Prosovsky
- **Active participants:** @Filip Prosovsky
- **Observers:** @Jozef Valko, @Martin Malych, @Debby Wu, @Zoltan Viragh, @Lucia Mlyn√°rov√°, @Jaroslav Tomeƒçek

### üìù Custom Fields

- **Caused by:** [Infrastructure](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JX2GXECVANR13R466QF5AJ0J)
- **Affected services:** [Desktop](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDJCQKRHKB1C4P8RX3HRP), [Dashboard Frontend](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDG4Q3EZQF5XHJNKM8HSD), [Dashboard](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDETDDN3FY1ZGXQ91KH6A) and [Phone](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDWHE0076Q4RVHFWEJ9V2)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/934)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C092SM5BD1A)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** June 23, 2025 10:48 PM (GMT+2)
- **Resolved at:** June 23, 2025 11:58 PM (GMT+2)
- **Impact started at:** June 23, 2025 3:24 PM (GMT+2)
- **Identified at:** June 23, 2025 11:30 PM (GMT+2)
- **Fixed at:** June 23, 2025 11:57 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 8 hours, 33 minutes

# üñäÔ∏è Summary

On **23 June 2025**, SSO login stopped working across our applications due to an **expired certificate**. Users relying on SSO, especially via Google, were unable to log in from **15:24 until 23:50 CET**, affecting peak business hours. The certificate was replaced with an auto-renewing version to prevent future expirations.

# Leadup

The certificate used for SSO authentication was issued with a fixed validity period and configured manually in the system. There were **no active alerts** indicating upcoming expiry due to misconfigured alert pending times in Prometheus (24h pending window, only within working hours), and no existing automated renewal process for this certificate nor the cognito domain was set in IaC.

# Fault

The **SSO certificate expired** on **Monday, 23 June 2025, at 15:24 CET**. The expired certificate blocked authentication for all users using SSO (e.g., Google login) in our applications, causing:

- Login errors for users in Dashboard and Desktop app - but only for users which were not already logged in

# Detection

The issue was **first reported by Martin Malych** on 24 June 2025 at 10**:48 PM CET**, 7 **hours after initial expiry**, via Slack message.

As we did not receive any alert. No incident was created, luckily we noticed the message in the Slack. The process for incident was not followed which delayed the resolution.

# 5 why's analysis

1. **Why did users experience SSO login failure?**
    
    Because the authentication certificate expired.
    
2. **Why did the certificate expire?**
    
    Because it was manually issued with a fixed expiry date and was not renewed in time.
    
3. **Why was it not renewed in time?**
    
    Because there was no automated renewal process in place for this certificate and we already got rid of most of import places for manual certificate.
    
4. **Why was there no automated renewal process?**
    
    Because the certificate was manually created instead of leveraging auto-renewable ACM certificates and we did not know about this as it was done outside the process and not in IaC as well.
    
5. **Why was this oversight not caught by monitoring?**
    
    Because Prometheus alerts were configured with a **24h pending time within working hours only**, making it ineffective for expiry warnings within a shorter window, especially outside working hours.
    

# Root cause

The root cause was the **manual management of a critical SSO certificate without auto-renewal**, compounded by:

- **Ineffective monitoring alert thresholds and pending configurations**, leading to no actionable alert before expiry.
- Lack of clear ownership for cert rotation and lifecycle management.

# Mitigation and resolution

- At first as hotfix, we imported the renewed certificate.
- Then we replaced the manually issued certificate with an **automatically renewing certificate** to prevent future expirations in ACM.
    
    (Implemented by Filip Pro≈°ovsk√Ω at **11:33 AM CET** on 24 June 2025.)
    
- **Prometheus alert configuration was improved** by Jozef Vaƒæko to:
    - Reduce ACM pending time from **24h to 1h**

# Lessons learnt

‚úÖ **What went well**

- Quick response and mitigation once the issue was reported.
- Fast implementation of permanent fix (auto-renewal) within hours next day.

‚ö†Ô∏è **What could have gone better**

- The incident **was not detected proactively**, causing prolonged downtime (~8 hours during peak usage)
- Follow incident process to create incident which would immediately notify on-call person
- Properly plan the work for Infra to prepare IaC configuration of this SSO domain change

üí° **Other learnings**

- All certificates used for core authentication flows must be managed with auto-renewal where possible
- Better review critical configured alerts

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-06-24** |  |
| 09:32:19 | **Incident reported by Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω reported the incident
Severity: P2
Status: Investigating |
| 09:33:42 | **Incident resolved and entered the post-incident flow**

Filip Pro≈°ovsk√Ω shared an update
Status: ~~Investigating~~ ‚Üí Documenting |
| 09:57:19 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

https://cloudtalkio.slack.com/archives/C03FGGA6VQX/p1750711707232579 |",P2,Infrastructure,"Desktop, Dashboard Frontend, Dashboard, Phone",,"Infrastructure, Missing proactive alerting",https://www.notion.so/INC-934-SSO-does-not-work-21c2bccf284f8171a104cf11deb032d0,2025-06-23T20:48:00.000Z
965,inc-965-kamailio-deployment-failed-in-apse-1-11-07-2025,"*Generated by* @Michal Popoviƒç *via [incident.io](https://app.incident.io/cloudtalkio/incidents/965) on* July 15, 2025 9:50 AM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Michal Popoviƒç
- **Incident commander:** @Tomasz Niedziela-Brach
- **Team member:** @Jakub Smadis
- **Active participants:** @Michal Popoviƒç, @Jozef Valko, @Jakub Smadis, @Tomasz Niedziela-Brach
- **Observers:** @Lucia Mlyn√°rov√°, @Zoltan Viragh, @Jaroslav Tomeƒçek, @Elena Darriba

### üìù Custom Fields

- **Caused by:** [Terraform Automation](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRE6KTGBMFMF66ZGEN4Z2M)
- **Affected services:** [Kamailio](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JVW20A3X92K476GSSRXFSG6R)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/965)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C095AG9PLH4)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** July 11, 2025 3:16 PM (GMT+2)
- **Accepted at:** July 11, 2025 3:32 PM (GMT+2)
- **Resolved at:** July 11, 2025 5:17 PM (GMT+2)
- **Impact started at:** July 11, 2025 3:16 PM (GMT+2)
- **Fixed at:** July 11, 2025 3:35 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 30 minutes

# üñäÔ∏è Summary

Customers were not able to register, make or receive calls for 30 minutes in apse-1 and apse-2 regions because of stucked kamailio-users and kamailio-trunks deployment triggered by instance rotation in auto-scaling group.

# Leadup

Infra team wanted to remove **github access key** from instance template user-data for security reason. This leaded to change of lunch template of kamailio instances which triggered kamailio-users and kamailio-trunks instance rotation inside auto-scaling groups in apse1 and apse2 regions.

**Github commit**

https://github.com/CloudTalk-io/terraform/commit/7c99b6706ae559745104649f921aaf3afa34dbe1

**Pull request**

https://github.com/CloudTalk-io/terraform/pull/2047

**Terraform run**

https://app.terraform.io/app/cloudtalk/workspaces/prod-apse1-voice/runs/run-gpF1QEf7P4FbHvWM

Voice team was unaware that this change would trigger instance rotation. Otherwise they would request run to be triggered on the weekend.

# Fault

Kamailio deployment were triggered by ASG instance rotation. Deployment scripts failed for unknown reason when instance restart has been executed. This leaded to connecting and disconnecting of customers in affected regions.

Deployments has been triggered again and after 1-2 hours there were finished successfully.

Meanwhile customers from apse-1 and apse-2 regions has been migrated to euc-1 region. Because of kamailio-trunks failure we had to manually change the asterisk configuration in all asterisk instances to forward outbound calls to european kamailio-trunks.

Customers in apse-1 and apse-2 regions experienced difficutly to dial and receive calls for about 30 minutes. For the next 2 days, customers in these regions may experienced slightly higher dialing times and audio latency.

Because Twilio carrier allows only connection in specific region from regional IPs, customers calls has been failovered to another carrier for 2 days. This leaded to unconnected outbound calls for some specific destinations where carrier failover was not available.

Customers has been moved back to their original region on Sunday 13.7.2025 at 20:00 CEST.

# Detection

Issue has been detected by the alert **kamailio-users-prod-apse1@i-064c2a0336c2249c2 had 50% decrease of registered users.** This alert indicates that more than 50% of users has been disconnected from kamailio-users instance.

After quick discussion we were able to detect source of the issue and move customers away from affected regions.

# 5 why's analysis

| Problem | **Why 1** | **Why 2** | **Why 3** | **Why 4** | **Why 5** | **Root Cause** | **Recurrence Prevention** | Tasks |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Developers did not have a knowledge that this change will trigger instance rotation. | Teams missed in the list of items to change that kamailio lunch template change is also there. Even after double check there were convinced that it will have an impact only to streaming proxy. | The change related to kamailio has been included to merge without knowledge of approving person. | Person implementing the security fix was not aware of limitations and rules for kamailio deployments. | There was no approval from voice team in pull request. This code is managed by voice team and should be approved by this team. | Voice team approved changed when code has been merged to stage and production. | Missing PR review by voice team on voice component changed by infra team. | Approval in merge PR is not enought as we can miss some critical change due to amount of changes in merge. |  |
| Change has been deployed on Friday. |  |  |  |  |  | Change has been deployed on Friday. | Do not deploy on Friday. This issue has been already communicated multiple times.  |  |
| ASG removed old healthy instance before new instance has been fully deployed. | Kamailio instance started kamailio service immdiatelly after boot-up and ASG marked instance as healthy. | Kamailio with default configuration is replying to ASG health check as healthy even when it is not fully deployed. | Kamailio image is configured in way that it starts kamailio service automatically after boot-up. |  |  | Kamailio template image is configured in way that it starts kamailio service automatically after boot-up. | Remove automated kamailio service start in the image and enable it during the deployment. | [VOIP-2096](https://cloudtalk.atlassian.net/browse/VOIP-2096) |
| ASG was not able to finish deployment of instances and terminate them. | Deployment includes reboot of the instance after deployment is finished and ansible was not able to reconnect after restart. | We use reboot to refresh all services after deployment with possibility to check services health after reboot. |  |  |  | Reboot of the instance during deployment can failed. This can lead to looping of instance deployment. | Remove reboot phase from kamailio deployment and instead test reboot in stage environment separately. | [VOIP-2096](https://cloudtalk.atlassian.net/browse/VOIP-2096) |
| Outbound calls routed to  Twilio from apse-1 and apse-2 regions were failovered to another carrier. | Asterisks in regions define specific trunk in each region to be used. Twilio trunks IP access had been set to allows only connection from CloudTalk IPs in that region. | We were never count with scenario where we would be forced to reroute calls outside the region on carrier side. |  |  |  | Wrong decision to allows only regional IPs for regional trunks in Twilio configuration | Create one general IP list with all CloudTalk IPs and assign it to all trunks. | [VOIP-2100](https://cloudtalk.atlassian.net/browse/VOIP-2100) |

# Root cause

- missing PR review from team owning terraform code changed by another team
- insufficient review for terraform changes
- bugs in ansible deployment for kamailio instances

# Mitigation and resolution

- mandatory review for code owners in terraform repository
- redesign of ansible deployment scripts

# Lessons learnt

- terraform does not reports about all actions related to changes, we need to be more carefull with reviews
- improve ansible deployments for voice components including testing
- always require review of the code change owned by another team

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-07-11** |  |
| 15:16:52 | **Incident reported in triage by Prometheus Alertmanager alert**

Prometheus Alertmanager alert reported the incident
Severity: None
Status: Triage |
| 15:27:58 | **Message from Jozef Vaƒæko**

Jozef Vaƒæko pinned their own message

re-routed APSE1 kamailio-users and kamailio-trunks to EUC1 in Cloudflare |
| 15:32:57 | **Incident accepted**

Michal Popoviƒç shared an update
Severity: ~~None~~ ‚Üí P2
Status: ~~Triage~~ ‚Üí Investigating |
| 15:37:15 | **Message from Jozef Vaƒæko**

Jozef Vaƒæko pinned their own message

re-routed APSE2 kamailio-users and kamailio-trunks to EUC1 in Cloudflare |
| 16:06:56 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

I was applying terraform changes to APSE2 and APSE1.
My changes affected only parameter store of streaming proxy and there were no changes from the voice team.
I did not fully check  all the changes and I double checked with Filip that the terraform will only change streaming proxy.

However, there were changes (https://github.com/CloudTalk-io/terraform/pull/2047/files) from Adam changed all shell scripts from voice team. This triggered redeploy of kamailio-users and kamailio-trunks in both regions.
Our automation is flaky and the deployment triggered bug in kamailio deployment that caused a spiral where instances were not healthy and customers were not able to call. |
| 17:17:06 | **Incident resolved and entered the post-incident flow**

Tomasz Brach shared an update
Status: ~~Investigating~~ ‚Üí Documenting |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/965?tab=follow-ups)

- **Twilio IP whitelist does not allow connection from different regions**
    - **Owner:** @Michal Popoviƒç
    - **JIRA ticket:** [VOIP-2100](https://cloudtalk.atlassian.net/browse/VOIP-2100)
- **Kamailio deployment reboot is taking minutes instead of seconds**
    - **Owner:** @Elena Darriba
    - **JIRA ticket:** [VOIP-2096](https://cloudtalk.atlassian.net/browse/VOIP-2096)",P2,Terraform Automation,Kamailio,Voice,"Performance, Infrastructure",https://www.notion.so/INC-965-Kamailio-deployment-failed-in-apse-1-2312bccf284f81bb9eabf0692e874064,2025-07-11T13:16:52.618Z
979,inc-979-408-request-timeout-kamailio-when-attempting-to-route-calls-via-twi,"*Generated by* @Tomasz Niedziela-Brach *via [incident.io](https://app.incident.io/cloudtalkio/incidents/979) on* August 5, 2025 11:46 AM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Jozef Valko
- **Reporter:** @Daniela Zordova
- **Incident commander:** @Tomasz Niedziela-Brach
- **Active participants:** @Jozef Valko, @Daniela Zordova, @Jakub Smadis, @Ivan Hristov, @Ji≈ô√≠ Missbach, @Lucia Mlyn√°rov√°, @Elena Darriba, @Andr√© Carvalho, @Michal Popoviƒç, @Tomasz Niedziela-Brach
- **Observers:** @Mark Humeniuk, @Jaroslav Tomeƒçek, @Zoltan Viragh, @Martina Redajova, @Debby Wu, @Erica  Hoelper

### üìù Custom Fields

- **Caused by:** [Kamailio](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JVW20A3X92K476GSSRXFSG6R)
- **Affected services:** [Asterisk](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JVW3SK537304MXEZV80TMNND) and [Kamailio](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JVW20A3X92K476GSSRXFSG6R)
- **Affected teams:** [Voice](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1XQDA2RQWR0STCA5F)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/979)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C096HPAQMPF)
- [Status page: CloudTalk - Call connectivity issues in Europe region.](https://status.cloudtalk.io/incidents/01K0HR6H37ASSFJAE06FHD84CQ)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** July 19, 2025 4:45 PM (GMT+2)
- **Resolved at:** July 19, 2025 6:35 PM (GMT+2)
- **Impact started at:** July 19, 2025 4:45 PM (GMT+2)
- **Fixed at:** July 19, 2025 6:29 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 1 hours, 43 minutes

# üñäÔ∏è Summary

**Problem:** Call routing via Twilio results in 408 request timeouts from Kamailio, affecting outbound reachability.

**Impact:** Four European companies and a testing account experience call failures when routing calls via Twilio.

**Root cause:** Twilio confirms these calls do not reach them, suggesting a potential issue with call routing configurations.

**Steps to resolve:** A workaround was tested by changing the asterisk server setting for the test account, which successfully allowed previously failing calls to go through. Further investigation into Kamailio and asterisk logs is underway to identify the underlying cause, and Twilio was contacted for support.

# Leadup

No leadup as this issue has not been affected by changes on our side.

# Fault

Twilio carrier did not replied to INVITE SIP messages resulting in 408 request timeout on our carrier side kamailio sip proxy. Issue was visible only in euc1 region.

# Detection

Detected by customers calling specific countries where only Twilio can route calls successfully. Out voice monitoring was not able to detect such an issue as inbound calls were working correctly and our monitoring calls using route which can be routed by backup carriers.

# 5 why's analysis

| Problem | **Why 1** | **Why 2** | **Why 3** | **Why 4** | **Why 5** | **Root Cause** | **Recurrence Prevention** | Tasks |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Twilio did not respond to our INVITE messages. | We were not able to identify root cause. We believe the issue was either in AWS network or inside the Twilio. |  |  |  |  | External networking issue caused by AWS network or Twilio. | We believe switch to TCP for all possible carriers can mitigate this issue. | https://cloudtalk.atlassian.net/browse/VOIP-2203 |
| Our voice monitoring was not able to detect issue. | We are not monitoring routes which are routed by only one carrier. | It would be very complicated and we could not be able to have numbers from all such routes. |  |  |  | Voice monitoring does not check carrier availability for outbound calls as calls may be routed by backup carriers. | Improve voice monitoring outbound calls to route each call by only one carrier. Rotate carriers randomly to check all carriers. | https://cloudtalk.atlassian.net/browse/VOIP-2204 |

# Root cause

We believe this issue was caused by Twilio or AWS networking issues.

# Mitigation and resolution

We should move all relevant carriers to TCP as UDP does not seems to be reliable for carrier interconnection.

# Lessons learnt

We should improve voice monitoring outbound calls to route each call by only one carrier to detect these type of issues.

All voice team members should have write access for trunk_regions table so they can disable specific trunk for specific region. This issue has been already fixed.

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-07-19** |  |
| 16:45:13 | **Incident reported by Daniela Zordova**

Daniela Zordova reported the incident
Severity: P2
Status: Investigating |
| 16:45:18 | **Incident escalated**

Your workflow manually escalated the incident to the escalation path [Platform escalation path](https://app.incident.io/cloudtalkio/on-call/escalation-paths/01HYDHY88AJ9FPGQEVNCYAX78Y) |
| 17:33:58 | **Update shared**

Jozef Vaƒæko shared an update
Four European companies and our testing account are affected by call failures when routing via Twilio, resulting in 408 request timeouts from Kamailio. Calls connect successfully when routed through other carriers. Twilio confirms these calls do not reach them.
A workaround was tested by changing the asterisk server setting for the test account, which allowed previously failing calls to go through. Tracking of server changes is being maintained in the channel.
Further investigation into Kamailio and asterisk logs is underway to identify the underlying cause. |
| 17:44:46 | **Message from Ivan Hristov**

Ivan Hristov's message was pinned by Elena Darriba*Did anyone from <!subteam^S046SAS4A15> opened ticket with Twilio ? If so can you escalate the issue on Twilio side ? Please push them to check if they have network or fragmentation issue. Based on pcap, it looks like we have two issues. Twilio do not respond on our INVITEs from multiple IPs:
1. 35.156.191.128 and 35.156.191.129
Second issue, looks like fragmentation issue on their end:
1. We send Invite to Twilio -- 1460 bytes
2. Twilio Sends 407 (expected) 
3. We Send INVITE with Authorization header -- 1800 bytes
4. Twilio stopped responding.
<@UK6RJGGLC> Do we have any graphs that to see network traffic in EUC1 region ? I am interested in packet loss or possibility to have BGP re-route towards Twilio IPs.* |
| 17:47:28 | **Message from Elena Darriba**

Elena Darriba pinned their own message

Hi! Let me verify if fragmentation packets is active and is working, it should, we did it some months ago.

It is enabled and working. |
| 18:24:23 | **Image posted by Elena Darriba**

Elena Darriba posted an image to the channel

Routing in euc1 will not use Twilio, we remove the route as Popi did in similar situations (thank you @michal.popovic), removing the row in trunks_regions table related to twilio and euc1, id=123
[link to message](https://cloudtalkio.slack.com/archives/C096HPAQMPF/p1752942263476859?thread_ts=1752942263.476859&cid=C096HPAQMPF) |
| 18:27:17 | **Message from Ivan Hristov**

Ivan Hristov's message was pinned by Elena Darriba

Calls towards twilio are not visible anymore in kamailio-trunks in euc1. Outbound calls issue is solved üôÇ |
| 18:28:52 | **Message from Elena Darriba**

Elena Darriba pinned their own message

Task for Monday (or when the twilio issue is solved): add again the twilio routing to euc1 in database |
| 18:31:01 | **Message from Elena Darriba**

Elena Darriba pinned their own message

Additional task: add the action done to a runbook |
| 18:35:30 | **Incident resolved and entered the post-incident flow**

Jozef Vaƒæko shared an update
Status: ~~Investigating~~ ‚Üí Documenting
Outbound calls routed via Twilio in the EUC1 region were failing with 408 request timeouts. After confirming the problem affected multiple European customer accounts and the test account, the team systematically rerouted affected companies to the US region, which restored outbound calling.
Investigation identified Twilio as the common factor in failed calls. Twilio was removed as a carrier from the EUC1 region by updating the trunk configuration in the database. A support ticket was opened with Twilio for further investigation on their end.
Re-testing after removing Twilio confirmed that outbound calls are now working, and logs show no calls routed via Twilio in EUC1. A status page update was posted to communicate both the outage and its resolution. The issue is considered resolved for now, with plans to restore Twilio routing once their support confirms the underlying problem is fixed. |
| **2025-07-20** |  |
| 09:30:39 | **Message from Jozef Vaƒæko**

Jozef Vaƒæko pinned their own message

@michal.popovic I‚Äôve granted RW access for DB user `michal.popovic` into DB table `trunks_regions` so you can use that one to update the rows if needed. Please let me know if you need access to more tables. |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/979?tab=follow-ups)

- **Notion page with personal DB users**
    - **Owner:** @Jozef Valko
    - **JIRA ticket:** [INFRA-2705](https://cloudtalk.atlassian.net/browse/INFRA-2705)
- **Post-mortem call - scheduled on July 30th**
    - **Owner:** @Tomasz Niedziela-Brach",P2,Kamailio,"Asterisk, Kamailio",Voice,"Reported by customers before we learned, 3rd party, Missing proactive alerting",https://www.notion.so/INC-979-408-Request-timeout-kamailio-when-attempting-to-route-calls-via-Twilio-2462bccf284f81538559d308c77a154f,2025-07-19T14:45:13.893Z
1008,inc-1008-voxbone-interconnection-not-working-properly-signaling-not-being-r,"*Generated by* @Ivan Hristov *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1008) on* August 4, 2025 2:05 PM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Jakub Smadis
- **Reporter:** @Ji≈ô√≠ Missbach
- **Incident commander:** @Tomasz Niedziela-Brach
- **Active participants:** @Tomasz Niedziela-Brach, @Lucia Mlyn√°rov√°, @Jakub Smadis, @Filip Prosovsky, @Ivan Hristov, @Ji≈ô√≠ Missbach, @Jiri Srba, @Zoltan Viragh, @Michal Popoviƒç
- **Observers:** @Debby Wu, @Mark Humeniuk, @Tom√°≈° Sak√°l, @Elena Darriba, @Jaroslav Tomeƒçek, @Erica  Hoelper, @Erik Dvorcak, @Michal Ustanik, @Andr√© Carvalho

### üìù Custom Fields

- **Affected services:** [Kamailio](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JVW20A3X92K476GSSRXFSG6R) and [Asterisk](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JVW3SK537304MXEZV80TMNND)
- **Affected teams:** [Voice](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1XQDA2RQWR0STCA5F)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1008)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C098KQ0CNU8)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** July 29, 2025 9:26 AM (GMT+2)
- **Resolved at:** August 1, 2025 3:55 PM (GMT+2)
- **Impact started at:** July 29, 2025 9:26 AM (GMT+2)
- **Identified at:** July 29, 2025 10:11 AM (GMT+2)
- **Fixed at:** July 30, 2025 7:15 AM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 21 hours, 48 minutes

# üñäÔ∏è Summary

**Problem:** CloudTalk to Voxbone signaling is not working properly, causing outgoing calls to time out.

**Impact:** This issue impacts multiple countries, introducing an additional 5-second Post Dial Delay for routes redirected to an alternative carrier, while calls are failing completely in regions without alternative routing options.

**Root cause:** The problem could be related to issues in the network or firewall layer, with behavior indicating that Voxbone stops responding after a challenge greater than 1500 bytes.

**Steps to resolve:** The affected company was moved to another Asterisk instance, restoring call functionality, and the issue has been escalated to Voxbone with detailed call examples and a pcap file of a failed call.

# Leadup

Voxbone **stopped responding to SIP INVITE requests** that exceeded **1500 bytes in size**, which caused **outbound calls to fail** when routed through Voxbone. These failures occurred because of **packet fragmentation**, which the network path (likely at or near Voxbone) couldn't process properly.

- The issue was **visible only in the EU region**

# Fault

The¬†**Voxbone firewall or network infrastructure**¬†was likely¬†**dropping fragmented UDP packets**, which caused SIP INVITE requests to fail when their size exceeded the MTU (1500 bytes).

> üìå¬†Root cause confirmation is pending final RCA from Voxbone.
> 

# Detection

The issue was detected through¬†**customer reports**¬†of:

- **Increased Post Dial Delay (PDD)**¬†on outbound calls
- **Call failures**¬†in regions without alternative routing options

# 5 why's analysis

### **1. Why were the outgoing calls to Voxbone failing or experiencing long delays?**

Because **INVITE messages larger than 1500 bytes were not reaching Voxbone**, causing call timeouts or rerouting.

### **2. Why were the INVITE messages larger than 1500 bytes not reaching Voxbone?**

Because **UDP packets larger than the MTU were being fragmented**, and **these fragmented UDP packets weren't successfully reaching Voxbone**.

### **3. Why were the fragmented UDP packets not successfully reaching Voxbone?**

Because **somewhere along the network path (likely at Voxbone's firewall), fragmented packets were being silently dropped**. 

### **4. Why was UDP being used for SIP signaling without accounting for packet fragmentation?**

Because **Kamailio trunks in the EU were configured to use UDP without TCP fallback**, and SIP packets with large headers (due to Proxy and Identity) exceeded the MTU were fragmented

### **5. Why wasn't SIP over TCP already iplemented or fragmentation handled more robustly?**

Because:

- **Kamailio trunk configurations lacked SIP over TCP**, which handles large packets more reliably.
- **Routing options were limited**: Some countries had no alternate carrier, so call failures couldn't be mitigated through fallback routes.
- **Making Asterisk configuration changes in production during peak hours carries significant risk**, which limited our ability to quickly resolve the issue.

# Root cause

Voxbone likely¬†**dropped fragmented SIP UDP packets**¬†exceeding 1500 bytes due to network/firewall handling issues. The EU Kamailio trunks were using¬†**UDP without TCP fallback**, leading to¬†**unreliable delivery of large SIP INVITEs**, especially where fragmentation occurred.

# Mitigation and resolution

- **Traffic Migration**: Calls were rerouted from EU Kamailio trunks to **US Kamailio trunks** where packet delivery proved more reliable.
- **Company Relocation**: Affected companies were transferred to alternative **Asterisk instances** to restore service.
- **Escalation to Voxbone**: We shared detailed packet captures and call examples with Voxbone for their technical analysis.

# Lessons learnt

- **Need for TCP SIP Support**: SIP over TCP must be easy to enable to avoid fragmentation issues.
- **Carrier Redundancy**: Some countries still depend on a¬†**single carrier**, leaving no fallback in case of failure.
- **Better Deployment Strategies:** Making Asterisk changes in production during peak hours remains **complex and risky**, delaying mitigation efforts.
    - *This stems from the need for an Asterisk upgrade to enable **safe module reloading** without disrupting active calls*
    - Without this capability, configuration changes (such as trunk routing) typically require complete restarts or complex manual workarounds that are both time-intensive and operationally risky.

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-07-29** |  |
| 09:26:52 | **Incident reported by Jiri Missbach**

Jiri Missbach reported the incident
Severity: P2
Status: Investigating |
| 09:26:52 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 09:26:57 | **Incident escalated**

Your workflow manually escalated the incident to the escalation path [Platform escalation path](https://app.incident.io/cloudtalkio/on-call/escalation-paths/01HYDHY88AJ9FPGQEVNCYAX78Y) |
| 10:11:37 | **Status changed from Investigating ‚Üí Identified**

Tomasz Brach shared an update
Status: ~~Investigating~~ ‚Üí Identified |
| 10:11:37 | **Identified at**

Custom timestamp ""Identified at"" occurred |
| 10:13:34 | **Update shared**

Tomasz Brach shared an update
moving the affected company to another Asterisk instance helped and the calls were restored. We're still investigating  |
| 10:31:13 | **Status changed from Identified ‚Üí Monitoring**

Tomasz Brach shared an update
Status: ~~Identified~~ ‚Üí Monitoring
All automated tests passed for the affected companies, so we assume the issue is already solved. We're still monitoring and troubleshooting |
| 11:54:28 | **Status changed from Monitoring ‚Üí Investigating**

Tomasz Brach shared an update
Status: ~~Monitoring~~ ‚Üí Investigating |
| 11:55:03 | **Update shared**

Tomasz Brach shared an update
We're still waiting for any response from Voxbone - the calls are still not working in EMEA region for Voxbone clients |
| 11:58:28 | **Update shared**

Tomasz Brach (via @incident) shared an update
Calls to Voxbone in the EMEA region are still not working, with customers experiencing timeouts and failed calls where no alternate routing is available.
We've confirmed that the problem is specific to the EMEA region and does not affect customers on the USA infrastructure. Voxbone has been provided with additional call examples and packet captures, but we are still waiting for a response from their side. |
| 3h later |  |
| 15:26:23 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

[Update]

What we know so far:
- kamailio trunk is doing the fragmentation of the UDP packets
- If the packet is bigger than MTU (1500 bytes) the packet won‚Äôt reach the Voxbone (similar issue as in #inc-359-random-calls-in-use1-are-failing-22-11-2024)
    ‚ó¶ fragmented packets won‚Äôt reach it either
- @ivan.hristov tried traceroute from different our servers and he was able to reach voxbone from dev/stage and prod (APSE1, APSE2) with fragmented trace route packets but he was not able to do it in EU and US.
- fragmented UDP packets are causing the issue since anyone on the route of the packet can drop it without letting us know
Next steps:
- We are investigating an option that we will route voxbone calls through Kamailio-trunk in US
    ‚ó¶ This would allow EU customers to call but it doesn‚Äôt fix the root cause of the issue
    ‚ó¶ It would impact only voxbone outbound calls
- @ivan.hristov will restart kamailio-trunks tomorrow in the morning to see if it will help with the issue
- We need to start implementing and testing SIP over TCP instead of UDP
    ‚ó¶ As it was shared in #inc-359-random-calls-in-use1-are-failing-22-11-2024, the fragmented UDP packets can‚Äôt be handled reliably out in the world (https://blog.evaristesys.com/2016/02/04/sip-udp-fragmentation-and-kamailio-the-sip-header-diet/)
 |
| 15:42:22 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message*Hey <!subteam^S046SAS4A15>,

we will move all companies away from the asterisk-prod-euc1-22 (max number of concurrent calls was 19).
We will make changes to the asterisk where voxbone outbound calls would be redirected to kamailio trunks in US.
And we will test it together with <@U077Z8TTNRL> that it works.

If this works we will provide a set of asterisk servers where we can migrate companies that have issues so we can mitigate it for them.

(doing it for all 25 servers is very time consuming right now :confused: )* |
| 17:11:19 | **Status changed from Investigating ‚Üí Identified**

Tomasz Brach shared an update
Status: ~~Investigating~~ ‚Üí Identified
We've begun migrating affected companies to reconfigured asterisk servers that route Voxbone outbound calls through the US, as a workaround for ongoing EMEA call failures.

Initial call tests on these servers have been successful. We are coordinating with technical support to move customers to the updated servers and will apply this fix to 5 servers today, with remaining migrations planned for tomorrow. |
| 18:01:45 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

The following asterisk have fix for the voxbone issue:
- asterisk-euc1-16
- asterisk-euc1-17
- asterisk-euc1-19
- asterisk-euc1-21
- asterisk-euc1-22
cc @Zoli

@ivan.hristov will fix rest of the asterisk tomorrow in the morning |
| **2025-07-30** |  |
| 07:15:00 | **Fixed at**

Custom timestamp ""Fixed at"" occurred |
| 2h later |  |
| 09:45:29 | **Status changed from Identified ‚Üí Monitoring**

Tomasz Brach shared an update
Status: ~~Identified~~ ‚Üí Monitoring |
| 09:49:00 | **Update shared**

Tomasz Brach shared an update
To mitigate the issue and make all the clients using Voxbone available to make calls, the Voice team moved all voxout trunks in Europe to point to USA servers. The issue is not happening anymore and could be considered as solved.
However, we should still rollback this solution and move the traffic back to European based servers as the workaround solution is not the standard flow. |
| 2d later |  |
| **2025-08-01** |  |
| 15:55:02 | **Incident resolved and entered the post-incident flow**

Jakub Smadis shared an update
Status: ~~Monitoring~~ ‚Üí Documenting |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/1008?tab=follow-ups)

- **Move Voxbone EU back to EU kamailio-trunks**
    - **Owner:** @Ivan Hristov
    - **JIRA ticket:** [VOIP-2134](https://cloudtalk.atlassian.net/browse/VOIP-2134)
- **Change outbound Voxbone trunk to TCP**
    - **Owner:** @Ivan Hristov
    - **JIRA ticket:** [VOIP-2129](https://cloudtalk.atlassian.net/browse/VOIP-2129)
- **Change inbound Voxbone trunk to TCP**
    - **Owner:** @Ivan Hristov
    - **JIRA ticket:** [VOIP-](https://cloudtalk.atlassian.net/browse/VOIP-2129)[2128](https://cloudtalk.atlassian.net/browse/VOIP-2128)
- **Create spike to investigate failover carrier setup**
    - **Owner**: @Michal Popoviƒç
    - **JIRA ticket**: https://cloudtalk.atlassian.net/browse/VOIP-2161",P2,,"Kamailio, Asterisk",Voice,3rd party,https://www.notion.so/INC-1008-VOXBONE-interconnection-not-working-properly-signaling-not-being-responded-2452bccf284f8164b8f8fd3c8bd8ba8b,2025-07-29T07:26:52.709Z
1036,inc-1036-calls-not-ringing-on-available-agents-voip-2157-07-08-2025,"*Generated by* @Michal Ustanik *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1036) on* August 12, 2025 1:05 PM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Michal Ustanik
- **Reporter:** @Ji≈ô√≠ Missbach
- **Incident commander:** @Allison Chihak
- **Active participants:** @Erik Dvorcak, @Debby Wu, @Michal Ustanik, @Tomas Sykora, @Jiri Srba, @Tomasz Niedziela-Brach, @Michal Popoviƒç, @Zoltan Viragh, @Allison Chihak, @Ji≈ô√≠ Missbach
- **Observers:** @Laura Misencikova, @Jaroslav Tomeƒçek, @Lucia Mlyn√°rov√°, @Andr√© Carvalho, @Mark Humeniuk, @Miguel Pecsi

### üìù Custom Fields

- **Caused by:** [Realtime WS](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDYW12EFZTJBHQRZM1N1M)
- **Affected services:** [Phone](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDWHE0076Q4RVHFWEJ9V2) and [Realtime WS](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDYW12EFZTJBHQRZM1N1M)
- **Affected teams:** [Voice](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1XQDA2RQWR0STCA5F) and [App Devs](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1Y13N5ZYW63255X4W)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1036)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C099JM3AP8U)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** August 7, 2025 7:31 PM (GMT+2)
- **Resolved at:** August 10, 2025 12:52 PM (GMT+2)
- **Impact started at:** August 7, 2025 7:31 PM (GMT+2)
- **Fixed at:** August 8, 2025 12:02 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 16 hours, 30 minutes

# üñäÔ∏è Summary

We are observing a continuous influx of customer reports indicating that calls are not ringing to agents, despite the agents being available.

This issue was previously tracked under [VOIP-2157](https://cloudtalk.atlassian.net/browse/VOIP-2157), but further analysis suggests the impact may be broader than initially assessed.

Following a discussion with L2 (Zoli), we are escalating this as a P2 incident.

After further investigation issue was caused by realtime-ws service not correctly updating all user endpoints on status change.

# Leadup

Clients started reporting calls are not ringing on their side. Originally issue triaged as Voice issue and different issue was fixed during evening on Voice side. However after further tickets investigation from L2, there were still reports, users are not getting incoming notifications about calls, even when their status was set to online.

# Fault

With new release for realtime-ws, we removed for setting user status in HTTP requests in realtime-api.

Newer solution did this on websockets where we compare first connected client. Once client is connected we set online status. However this implementation is only setting first matched user_endpoints, in this case on 01 endpoint - SIP endpoints.

The fault was in implementation, where we need to update all available user endpoints 01 - SIP, 02 - App, 03 - mobile so they are available / unavailable for calling. 

# Detection

Following was done to detect unreliable user status setting

1. Phone - set agent status to idle
2. close the tab, this will result in:
    1. `idle_off`¬†only on 01 endpoint in¬†`agent_statuses`¬†table ‚Üí this is faulty implementation
    2. `online_status = idle`¬†in¬†`users`¬†table
3. open the tab, this will result in:
    1. no change in¬†`agent_statuses` ‚Üí this is faulty as well, but its tied to user_endpoints SQL UPDATE command
    2. `online_status = online`¬†in¬†`users`¬†table

# 5 why's analysis

### Why users were not getting incoming calls notification?

Their **endpoint for Phone app (02) was not correctly set to online** 

### Why **endpoint for Phone app (02) was not correctly set to online?**

Because of **realtime-ws not updating all users endpoints only 01?**

### Why was **realtime-ws not updating all users endpoints, only 01?**

This function was implemented previously in past, not reviewed nor correctly tested.

# Root cause

While migrating user status change from HTTP request to websockets implementation used old logic for updating user endpoints, where only first user endpoints was set. After that all the remaining endpoints remained unchanged resulting for users not being able to receive incoming calls.

# Mitigation and resolution

We immediately started to compare previous versions of release for agent status and preparing for rollback, but in the meantime we figured out there is missing implementation on realtime-ws. After fixing we deployed fixed version to the dev environment to test functionality and proceed with prod release. 

# Lessons learnt

Even though this release included canary release with percentage rollout and we included automated e2e tests, we learned that our services are very critical to any small changes. 

Next time we should triplecheck legacy code implementation and specifically focus even more legacy part code testing with significant release like this.

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-08-07** |  |
| 19:31:34 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 19:31:34 | **Incident reported by Jiri Missbach**

Jiri Missbach reported the incident
Severity: P2
Status: Investigating |
| 19:31:39 | **Incident escalated**

Your workflow manually escalated the incident to the escalation path [Platform escalation path](https://app.incident.io/cloudtalkio/on-call/escalation-paths/01HYDHY88AJ9FPGQEVNCYAX78Y) |
| **2025-08-08** |  |
| 10:04:34 | **Message from Michal Ustan√≠k**

Michal Ustan√≠k pinned their own message

Investigation: with new release (1 week ago), we switched for setting user status in HTTTP requests in realtime-api.
Newer solution includes requires do this on websockets where we compare first connected client. Once client is connected we set online status. However this implementation is only setting first matched user_endpoints, in this case on 01 endpoint.
We need to update the implementation to match previous one and update agent status on all user endpoints. This will result into updating agent_statuses table as well. |
| 12:02:14 | **Status changed from Investigating ‚Üí Monitoring**

Michal Ustan√≠k shared an update
Status: ~~Investigating~~ ‚Üí Monitoring |
| 12:02:14 | **Fixed at**

Custom timestamp ""Fixed at"" occurred |
| 2d later |  |
| **2025-08-10** |  |
| 12:52:49 | **Incident resolved and entered the post-incident flow**

Michal Ustan√≠k shared an update
Status: ~~Monitoring~~ ‚Üí Documenting |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/1036?tab=follow-ups)

- **[Desktop] Review agent statuses table**
    - **Owner:** @Michal Ustanik
- **[Desktop] App Preserving But Not Displaying Idle Status**
    - **Owner:** Unassigned
    - **JIRA ticket:** [APPS-1801](https://cloudtalk.atlassian.net/browse/APPS-1801)",P2,Realtime WS,"Phone, Realtime WS","Voice, App Devs","Missing proactive alerting, Reported by customers before we learned, Insufficient testing, Regression, Data loss",https://www.notion.so/INC-1036-Calls-Not-Ringing-on-Available-Agents-VOIP-2157-24d2bccf284f819db270eb35881165b8,2025-08-07T17:31:34.123Z
1045,inc-1045-hubspot-cti-initiating-outbound-call-from-all-browser-tabs-11-08-2,"*Generated by* @Michal Ustanik *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1045) on* August 18, 2025 11:16 AM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Michal Ustanik
- **Reporter:** @Allison Chihak
- **Incident commander:** @Allison Chihak
- **Active participants:** @Allison Chihak, @Michal Popoviƒç, @Jaroslav Tomeƒçek, @Michal Ustanik
- **Observers:** @Tom√°≈° Sak√°l, @Lucia Mlyn√°rov√°, @Josef Lapka, @Debby Wu, @Zoltan Viragh, @Andras Szabo

### üìù Custom Fields

- **Affected services:** [Phone](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDWHE0076Q4RVHFWEJ9V2)
- **Affected teams:** [App Devs](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1Y13N5ZYW63255X4W)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1045)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C09A748EKK3)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** August 11, 2025 1:36 PM (GMT+2)
- **Resolved at:** August 14, 2025 11:02 AM (GMT+2)
- **Impact started at:** August 7, 2025 12:14 PM (GMT+2)
- **Identified at:** August 11, 2025 1:42 PM (GMT+2)
- **Fixed at:** August 11, 2025 3:13 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 37 minutes

# **üñäÔ∏è Summary**

**Problem:** HubSpot CTI initiates an outbound call from every browser tab due to duplicated events triggered by user actions.

**Impact:** Users experience outbound calls being initiated from all their browser tabs when they attempt to make a call from a specific tab.

**Root cause:** Initial investigations suggest that HubSpot may be rolling out multiple tab support, causing events from one tab to be propagated to others via a broadcast channel.

**Steps to resolve:** We switched the CloudTalk BETA app within HubSpot to ‚Äòcalling window only‚Äô mode, which allows users to make calls without experiencing the issue.

# **Leadup**

The issue with HubSpot duplicating calls was reported by one of our internal teams. It then escalated as a recheck to the Apps team to confirm if CTI was behaving correctly. We identified that when a user initiated a call in the HubSpot interface, it was duplicated in all the opened tabs. This led to strange audio quality, calls interfering with other calls, and SIP registrations not working correctly.

# **Fault**

HubSpot internally released a new behavior where, on user action inside the active tab, all the tabs received the performed user action. I.e., a user clicks to dial a number inside HubSpot, and this event was propagated to all active tabs rather than just the focused one.

Due to architectural changes to CTI behavior, our app correctly dialled a call on the triggered action.

# **Detection**

We investigated this by opening the CTI app inside HubSpot while keeping the browser developer console open. We noticed that even in an inactive tab, we received an event for an outgoing call:

```json
[calling-extensions-cross-tab]: Received message from Remote sdk event OUTGOING_CALL_STARTED
```

Our app, on that trigger, performed the default action for call dialling. This was not specific only to dialling the call; we also received all other actions performed in the active tab.

# **5 why‚Äôs analysis**

### **Why were there multiple calls dialled in the CTI App?**

Because HubSpot released a broadcast of the event to all opened tabs, rather than sending it to the actively focused one.

### **Why did HubSpot release a broadcast of the event to all opened tabs?**

They did so without any prior notice to affected apps.

### **Why did they do so without any prior notice to affected apps?**

This is not known from their communication with us, and they quickly reverted the fix.

### **Why were we not notified about this change?**

HubSpot only provided updates monthly via their website but no information about future breaking releases.

# **Root cause**

There were two root causes investigated after the incident:

- No prior information from our partner, HubSpot, about an upcoming breaking change to app behavior.
- Our app not mitigating multiple call dialling. We should consider a mechanism where we don‚Äôt allow a specific user to make multiple calls with more tabs open. We should inform the user about active calls in other tabs and cancel those before dialling any new one.

# **Mitigation and resolution**

Once we identified the root cause as broadcasting messages from HubSpot, we concluded to switch the behavior of our app to run only in a dedicated calling window, which prevents using it in multiple tabs. However, this solution was not perfect for our users, as this was running in a background window and sometimes caused confusion.

In parallel, we started reaching out to our HubSpot contacts, as well as submitting a support ticket. Based on that, we scheduled a meeting at the earliest possible date to navigate this issue. After three days of communication exchange, HubSpot finally reverted their change and our CTI app started to work normally again.

# Communication

Incident was communicated via the [internal status page](https://app.incident.io/cloudtalkio/status) where CloudTalk employees can subscribe for email updates and to the [public status page](https://status.cloudtalk.io/) which notifies subscribed customers and also posts in the [#sal-outage-alarm](https://cloudtalkio.slack.com/archives/C010E1SM91S) channel.

After the outbound calling issue was fixed, both incidents were closed, pending next steps for an ideal solution from HubSpot. 

The incident solution moved calling to a separate window which made inbound calls hard to identify and answer. The inbound call usability in the separate calling window had significant issues (missing notification sounds, user must keep calling window visible on their screen at all times) and was misunderstood as continuation of the incident. 

After receiving this feedback from the Support and Sales teams, the internal status page incident was reopened to communicate updates from the discussions between CloudTalk and HubSpot to re-release the ideal CTI experience. Internal status page updates are not posted to the [#sal-outage-alarm](https://cloudtalkio.slack.com/archives/C010E1SM91S) channel and had to be posted manually by the incident commander.

Throughout the incident, communication by the incident commander to internal teams, and on the public status page was hours behind information from the engineer working on the incident. Questions were being shared across 5 to 6 different channels by different people during those delays, adding to the confusion of this incident.

# **Lessons learnt**

We need to prevent this behavior in our CTI app, but also globally, so that a user cannot try to dial multiple calls separately in different tabs. This way we could have prevented this incident, since all tabs would ignore the trigger for dialling the call.

Another high priority issue was the communication with internal teams. The incident and updates on a permanent fix were communicated with a delay. The usability issues with the incident fix were not clearly communicated internally so Sales & Support could advise customers with confidence. After reviewing [similar public incidents](https://status.cloudtalk.io/incidents/01K0Y8NS2M4TJB15ET7AT1YM9P) where a carrier needed to make changes to deliver an ideal permanent solution. It's recommended that we keep the [customer facing status page](https://status.cloudtalk.io/) in monitoring state when an interim solution is in place but the experience is subpar.

Another lower-priority follow-up is to check our CTI providers for any breaking changes. While this would not have prevented the incident, it would at least help us stay informed about upcoming changes from our providers.

The last thing we discussed is some kind of alerting for such behavior, but for now, we concluded it is not applicable for this situation, as we were not sure what should be tracked. The incident was not caused by any changes on our side.

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-08-11** |  |
| 13:36:23 | **Incident reported by Allison Chihak**

Allison Chihak reported the incident
Severity: P2
Status: Investigating |
| 13:36:23 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 13:36:28 | **Incident escalated**

Your workflow manually escalated the incident to the escalation path [Platform escalation path](https://app.incident.io/cloudtalkio/on-call/escalation-paths/01HYDHY88AJ9FPGQEVNCYAX78Y) |
| 13:42:00 | **Identified at**

Custom timestamp ""Identified at"" occurred |
| 14:08:24 | **Update shared**

Allison Chihak shared an update
Incident posted to public status page. Subscribers notified. 

Public Incident: https://status.cloudtalk.io/incidents/01K2CG72ZS6ZQKM4J2RF9R0J3Y |
| 14:12:16 | **Incident paused**

Allison Chihak shared an update
Status: ~~Investigating~~ ‚Üí Paused
**Investigation**

‚Ä¢ in Hubspot CTI users are getting duplicated events from each tab that they trigger specific event
‚Ä¢ it impacts users to point, if they click on dial in specific tab it will get dialled in all other tabs as well
‚Ä¢ from first investigation **no change** to this was done on APPS side, we suspect Hubspot is rolling out multiple tabs support, where events from one tab are propagated probably through broadcast channel to other tabs:
‚Ä¢ 

‚Ä¢ Problem reported to partner. Requested additional details.

 |
| 15:12:17 | **Incident resumed**

incident.io shared an update
Status: ~~Paused~~ ‚Üí Investigating |
| 15:13:49 | **Fixed at**

Custom timestamp ""Fixed at"" occurred |
| 15:13:49 | **Status changed from Investigating ‚Üí Monitoring**

Michal Ustan√≠k shared an update
Status: ~~Investigating~~ ‚Üí Monitoring
We switched to calling window only.
Users are now able to make call through calling window.
https://developers.hubspot.com/docs/guides/apps/extensions/calling-extensions/receive-incoming-calls#3.-not-recommended-use-the-calling-window-only |
| **2025-08-12** |  |
| 08:31:16 | **Update shared**

Michal Ustan√≠k shared an update
Hubspot support team replied for additional info for ticket - 27730987085.  We notified them about incorrect behavior. Waiting for reply. |
| 09:59:01 | **Incident resolved and entered the post-incident flow**

Allison Chihak shared an update
Status: ~~Monitoring~~ ‚Üí Documenting
Solution released on 2025-08-11 is working and allows inbound and outbound calling via HubSpot CTI (embedded dialer).

We are in contact with the partner to replace this solution with a version that works within the HubSpot app dock as opposed to in a separate calling window. This will be included in the post-mortem action items once HubSpot either rolls back their release or explains how we can adjust to their change. |
| **2025-08-13** |  |
| 17:24:52 | **Incident re-opened**

Allison Chihak shared an update
Status: ~~Documenting~~ ‚Üí Monitoring |
| 17:26:03 | **Update shared**

Allison Chihak shared an update
**What happened?**

‚Ä¢ Last week, HubSpot released a change in their product. They didn‚Äôt announce it in their changelog or release notes.
‚Ä¢ HubSpot‚Äôs release broke outbound calling. 1 call button click initiated 7-10 calls are once - 1 for every open HubSpot browser tab.
‚Ä¢ On Monday, we contacted HubSpot‚Äôs PM and technical teams. They didn‚Äôt reply within an hour so we released a temporary fix.
‚Ä¢ Last night, HubSpot released & then reverted another unannounced change that broke the temporary fix.
‚Ä¢ Temp fix is working at the moment but has other limitations and HubSpot is not being transparent about their releases.
**What‚Äôs the temporary fix?**

‚Ä¢ We switched to the old option that opens the CTI in a separate window.
‚Ä¢ HubSpot‚Äôs depreciating this option 31 Aug and it doesn‚Äôt work properly with inbound calling & notifications
**What‚Äôs the current state?**

‚Ä¢ We‚Äôre pushing them from all angles to tell us what to change or fix their mistake.
‚Ä¢ Dev & Allison
‚Ä¢ Partnerships & David |
| **2025-08-14** |  |
| 16:00:55 | **Incident resolved and entered the post-incident flow**

Allison Chihak shared an update
Status: ~~Monitoring~~ ‚Üí Documenting
**Resolution**

‚Ä¢ HubSpot released a solution for the incident reported on Aug 11. 
‚Ä¢ The Apps team confirmed that the fix works correctly for outbound and inbound calling and re-released HubSpot CTI in the embedded calling window. 
‚Ä¢ The temporary solution using a separate calling window is no longer in place |",P2,,Phone,App Devs,"3rd party, Reported by customers before we learned, Missing proactive alerting",https://www.notion.so/INC-1045-HubSpot-CTI-initiating-outbound-call-from-all-browser-tabs-2532bccf284f81f6a2edc4d23041b774,2025-08-11T11:36:23.121Z
1049,inc-1049-outgoing-calls-over-voxbone-failing-no-response-to-the-initial-inv,"Outgoing calls over voxbone failing - no response to the initial invite
Problem
Carrier voxout did not responded to SIP INVITE messages in use1 region. This is similar situation as we had with euc1 region in incident
INC-1008.

Fixed by routing all voxout traffic through euc1 region for euc1 and use1 asterisks as this region has been already fixed. This has been done by changing outboundproxy in sip.conf from kamailio-trunks in use1 to kamailio-trunks in euc1.
Impact
from 6am until 9am CEST routes depending only on voxout may not work properly.
400 disconnected calls at 9am CEST to reload configuration.

We are experiencing issues with calling through Voxbone via US kamailio-trunks. The affected regions are US and EU since whole EU traffic for voxbone was moved to US in #inc-1008-voxbone-interconnection-not-working-properly-signaling-not-being-r.
We found out that calling Voxbone through EU kamailio-trunks works so we decided to move asterisk in EU and US to call via EU kamailio-trunk.
The US was moved already by @michal.popovic. (we have no calls there)
The EU is being moved currently where we decided to do it the quick change on asterisk. We have to drop all the ongoing calls by restarting asterisks with a new configuration that will allow our customers to make calls.
We are not able to do it in a safe way because it would take a 6+ hours to gracefully shutdown all the asterisks and let all calls finish. We lack a way how we can put a region into maintanance mode and move the calls to another region.
The result of this change is:
US and EU asterisks will route voxbone calls through kamailio-trunks in EU region",P2,,Call Service,Voice,3rd party,,2025-08-14T06:40:54.855Z
1062,inc-1062-outbound-calls-failures-when-calling-us-numbers-30-08-2025,"*Generated by* @Michal Popoviƒç *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1062) on* September 2, 2025 9:43 AM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Michal Popoviƒç
- **Reporter:** @Michal Popoviƒç
- **Incident commander:** @Michal Popoviƒç
- **Active participants:** @Daniela Zordova, @Michal Popoviƒç, @Allison Chihak, @Tomasz Niedziela-Brach, @Ji≈ô√≠ Missbach, @Erica  Hoelper, @Jaroslav Tomeƒçek
- **Observers:** @Jiri Srba, @Cynthia Bausing, @Charmaine Olivar, @Zoltan Viragh, @Lucia Mlyn√°rov√°, @Peter Stanko, @Andr√© Carvalho, @Jakub Smadis, @Josef Lapka

### üìù Custom Fields

- **Caused by:** [Numbers Management](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDTP2MZHCR7V1DYXV21GE)
- **Affected services:** [Asterisk](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JVW3SK537304MXEZV80TMNND) and [Agi](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRCXBPP9MHS0S1NQV5BE9M)
- **Affected teams:** [Voice](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1XQDA2RQWR0STCA5F)
- **Root Cause Label:** Missing proactive alerting, Human error and Reported by customers before we learned

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1062)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C09CR3NJDPX)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** August 30, 2025 5:25 PM (GMT+2)
- **Resolved at:** August 30, 2025 6:18 PM (GMT+2)
- **Impact started at:** August 30, 2025 12:30 PM (GMT+2)
- **Identified at:** August 30, 2025 5:25 PM (GMT+2)
- **Fixed at:** August 30, 2025 6:00 PM (GMT+2)
- **Documented at:** August 30, 2025 6:00 PM (GMT+2)
- **Reviewed at:** August 30, 2025 6:00 PM (GMT+2)
- **Closed at:** August 30, 2025 6:00 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 5 hours, 30 minutes

# üñäÔ∏è Summary

Increased numbers of failed outbound calls caused by new entries in the table numbers_blocklist_global from 12:30 30th of August CEST.

At 16:50 CEST all incorrect records has been removed from numbers_blocklist_global table by filtering with creation time.

Because some of our slaves are replicating data using statement replication, creation date was different on them as it follows real time when records has been inserted by replication flow.

Issue has been fixed at 18:00 CEST by removing all entries from all DB nodes by filtering DELETE with id instead of creation time.

List of blocked numbers included 470 US national 3-digit area codes which means we have blocked basically whole US for outbound calls.

# Leadup

At 12:30 on 30th of August CEST Account approval specialist Karla Ysabel Montemayor has been adding new entry to global blacklist and by mistake set the filter way that it included all US numbers. This blacklist is global and valid for all customers.

![image.png](attachment:4f7e0c81-eca3-4b35-a804-a8b5d122c892:image.png)

# Fault

Global blacklist blocked almost all outbound calls to US.

# Detection

At 12:35 we received alert CallMonitoringExternalCallsFailing created by voice monitoring service. Because this alert has only warning severity, on-call person has not been notified.

Voice team has been notified by support team around 16:30 CEST by customer complains.

# 5 why's analysis

| Problem | **Why 1** | **Why 2** | **Why 3** | **Why 4** | **Why 5** | **Root Cause** | **Recurrence Prevention** | Tasks |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Why we blocked almost all US outbound calls? | Person from carrier operations team inserted wrong data to global blacklist. | She created the rule incorrectly and confused terminology between the number import form and the block list.
She only intended to add +14038724743, but thought she had to parse it similarly to the number import. | Form uses two columns with name National prefix start and national prefix end. She inserted national prefix 403 to the first column and rest of the number 8724743 to the second column. This created list for all US numbers with prefixes from 403 until 872. | Bad decision. I do not see any issue with wording in these columns. |  | Incorrect understanding of column names in blacklist form. | Repeated training for carrier operations and change in documentation to assure correct usage. | [https://www.notion.so/cloudtalkio/Detecting-Frauds-13e2bccf284f813d85bff8bcb3d31e92?source=copy_link#23a2bccf284f80e48fb5e0edb8b41dc9](https://www.notion.so/13e2bccf284f813d85bff8bcb3d31e92?pvs=21) |
| Why we were not able to detect issue sooner? | We were not aware of triggering alert from voice monitoring reporting failed outbound calls. | Severity of alert CallMonitoringExternalCallsFailing which is monitoring number of failed automated outbound calls using voice monitoring has been set to warning. | Alert has been set to warning severity in 2024 after discussion with on-call people complaining for false positives. |  |  | Alert for failed calls were not detected as it was not reported to on-call person. | Raise alert severity back to critical. | https://github.com/CloudTalk-io/argocd-kube/pull/7145 |
| Why first removal of records did not removed records on slave nodes? | We used delete operation with filter by creation time. | Replication to slave is different on different slave nodes. Some of them has mixed strategy and some of them statement strategy which creates these collumns by using current time on slave node. |  |  |  | Wrong replication strategy on some slave database nodes. | Set mixed replication strategy on all slave nodes. | https://cloudtalk.atlassian.net/browse/INFRA-2795 |

# Root cause

Incorrect understanding of column names in blacklist form.

Alert for failed calls were not detected as it was not reported to on-call person.

Wrong replication strategy on some slave database nodes.

# Mitigation and resolution

Repeated training for carrier operations and change in documentation to assure correct usage.

Raise alert severity back to critical.

Set mixed replication strategy on all slave nodes.

# Lessons learnt

N/A

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-08-30** |  |
| 12:30:00 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 4h later |  |
| 17:25:50 | **Identified at**

Custom timestamp ""Identified at"" occurred |
| 17:25:50 | **Incident reported by Michal Popoviƒç**

Michal Popoviƒç reported the incident
Severity: P3
Status: Investigating |
| 17:25:55 | **Incident escalated**

Your workflow manually escalated the incident to the escalation path [Platform escalation path](https://app.incident.io/cloudtalkio/on-call/escalation-paths/01HYDHY88AJ9FPGQEVNCYAX78Y) |
| 17:26:46 | **Severity upgraded from P3 ‚Üí P2**

Michal Popoviƒç shared an update
Severity: ~~P3~~ ‚Üí P2 |
| 18:00:00 | **Documented at**

Custom timestamp ""Documented at"" occurred |
| 18:00:00 | **Reviewed at**

Custom timestamp ""Reviewed at"" occurred |
| 18:00:00 | **Fixed at**

Custom timestamp ""Fixed at"" occurred |
| 18:18:46 | **Incident resolved and entered the post-incident flow**

Michal Popoviƒç shared an update
Status: ~~Investigating~~ ‚Üí Documenting |",P2,Numbers Management,"Asterisk, Agi",Voice,"Missing proactive alerting, Human error, Reported by customers before we learned",https://www.notion.so/INC-1062-Outbound-calls-failures-when-calling-US-numbers-2622bccf284f81068080cc0c41813944,2025-08-30T15:25:50.709Z
1068,inc-1068-pipedrive-synchronisation-not-working-significant-disruption-int-3,"*Generated by* @Marian Nociar *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1068) on* September 2, 2025 8:35 AM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Marian Nociar
- **Reporter:** @Ji≈ô√≠ Missbach
- **Incident commander:** @Tomasz Niedziela-Brach
- **Active participants:** @Elena Darriba, @Zoltan Viragh, @Ji≈ô√≠ Missbach, @Marian Nociar, @Jaroslav Tran, @Tomasz Niedziela-Brach, @Jaroslav Tomeƒçek
- **Observers:** @Lucia Mlyn√°rov√°, @Vlad Cirstean, @Margareta, @Martin Cermak, @Mateusz Bazydlo, @Debby Wu, @Miguel Pecsi, @Daniela Zordova, @Allison Chihak, @Tom√°≈° Sak√°l

### üìù Custom Fields

- **Root Cause Label:** Architecture weakness and Missing proactive alerting

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1068)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C09CK5AH1AB)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** September 1, 2025 4:37 PM (GMT+2)
- **Impact started at:** September 1, 2025 4:37 PM (GMT+2)

# üñäÔ∏è Summary

**Problem:** Pipedrive synchronization is not working, starting from August 29 at 3 a.m., causing a significant disruption in exporting calls and affecting the initial and periodic sync of created and edited entities.

**Impact:** All queued events (~300,000 events including calls, sms, contacts) have been delayed in synchronization.

**Root cause:** The issue was caused by the pods entering a crash loop due to an out-of-memory issue.

**Steps to resolve:** The Pipedrive pods are now memory-stable, and all queued events have been processed. The initial and periodic sync is in progress, aiming to synchronize all missed events since the beginning of the outage. A plan is in place to re-export all affected events, with an anticipated completion for the majority of companies by the following morning.

# Leadup

The incident was not caused by any recent release. Instead, it resulted from a high number of ongoing events that accumulated over time and remained in the pods' memory without being completed. This led to a gradual and simultaneous increase in memory usage until it reached the memory limit, causing the pods to enter a crash loop due to an out-of-memory (OOM) condition.

![Screenshot 2025-09-01 at 17.22.09.png](attachment:c53a95f0-0123-4860-9236-706da173f8f0:Screenshot_2025-09-01_at_17.22.09.png)

![Screenshot 2025-09-02 at 09.01.41.png](attachment:2e4c3ea5-e3ee-477d-8d55-1555c1544da3:Screenshot_2025-09-02_at_09.01.41.png)

# Fault

The current architecture processes events immediately after receiving an SQS or Kafka message. This approach works well under normal conditions, but it does not account for situations where immediate processing is not possible, such as when requests are throttled by external rate limits (e.g., from CRMs or our own databases).

This leads to a scenario where event processing begins and prepares data for import or export, but cannot complete due to exhausted limits. As a result, the partially processed data remains in memory, waiting until resources become available.

This issue is typically triggered by large clients with a high volume of calls or a significant number of external entities, which increases the likelihood of hitting external or internal limits.

# Detection

The issue was first brought to our attention by client reports, which indicated anomalies in event processing. This was then validated by reviewing Kubernetes pod logs and Prometheus metrics, which confirmed high memory usage and crash loops affecting Pipedrive related pods.

# 5 why's analysis

1. **Why did the pods enter a crashing loop due to an out-of-memory (OOM) issue?**
Because too many uncompleted event-processing tasks were stored in memory, eventually exhausting the available memory and triggering the crash loop.
2. **Why were there so many uncompleted event-processing tasks?**
Because event processing was started immediately upon receiving messages, even when external resources were unavailable due to throttling or rate limits.
3. **Why was event processing started without checking for resource availability or rate limits?**
Because the current architecture does not include any throttling awareness or backpressure mechanisms. It assumes that events can always be processed immediately.
4. **Why did this affect even clients with low traffic?**
Because all client events were processed using shared resources, and the high memory/worker usage from large clients blocked event processing for others. Without per-tenant isolation or throttling mechanisms, lower-traffic clients were indirectly impacted by the behavior of higher-traffic ones.

# Root cause

The incident was caused by an architectural limitation where events are processed immediately upon message receipt, without awareness of external throttling or system resource limits. When event processing is delayed, the events accumulate in memory, eventually triggering out-of-memory (OOM) crashes.

In this case, the Pipedrive pods were overwhelmed mainly by traffic from **company 227479**, which generated a high volume of events (~175,000 unprocessed).

![Screenshot 2025-09-02 at 11.14.37.png](attachment:a6277758-af3f-4f94-8dc8-8760fc698e47:Screenshot_2025-09-02_at_11.14.37.png)

Due to strict Pipedrive API rate limits, we were unable to process these events in real-time. As a result, only a small fraction of their API calls were handled, while the rest remained in memory, consuming resources until the pods crashed.

![Screenshot 2025-09-02 at 10.59.22.png](attachment:2a499445-3f4a-405d-bcbf-03f80601a568:Screenshot_2025-09-02_at_10.59.22.png)

The lack of per-tenant throttling, deferred processing, and memory-aware queuing made the system vulnerable to overload from high-volume clients, impacting the stability of the entire shared environment.

# Mitigation and resolution

To immediately mitigate the issue, we identified and removed all scheduled or ongoing Pipedrive event-processing tasks from the execution loop. This was done by updating their status in the MongoDB collection, effectively marking them as inactive.

![Screenshot 2025-09-02 at 09.43.22.png](attachment:e4238e13-7733-4540-aee5-3f723ca3c166:Screenshot_2025-09-02_at_09.43.22.png)

Next, we forcefully restarted all Pipedrive-related pods to clear memory and reset their state. This ensured that no leftover in-memory event data would continue to contribute to memory pressure or trigger crash loops.
Once the pods were stable and no longer experiencing out-of-memory issues, we began a gradual reprocessing of the previously dequeued events. This controlled replay allow us to restore event handling without overwhelming the system or reintroducing the original failure conditions.

<aside>
üöß

We are fully aware of this overload issue and are already working on architectural changes to prevent pods from being overwhelmed by events that cannot be processed immediately. The new design will introduce mechanisms to defer or queue such events safely, avoiding memory buildup and improving resilience under load or throttling conditions.

https://cloudtalk.atlassian.net/browse/INT-3182

</aside>

# Lessons learnt

What went well? What could have gone better? What else did you learn?5

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-09-01** |  |
| 16:37:03 | **Incident reported by Jiri Missbach**

Jiri Missbach reported the incident
Severity: P2
Status: Investigating |
| 16:37:03 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 16:37:08 | **Incident escalated**

Your workflow manually escalated the incident to the escalation path [Platform escalation path](https://app.incident.io/cloudtalkio/on-call/escalation-paths/01HYDHY88AJ9FPGQEVNCYAX78Y) |
| 16:43:40 | **Message from incident**

incident's message was pinned by Jaroslav Tomeƒçek

Zolt√°n Ill√©s added a follow-up |
| 17:23:58 | **Image posted by Marian Nociar**

Marian Nociar posted an image to the channel

The issue started on August 29 around 3 a.m. The pods entered a crash loop due to an out-of-memory issue.
[link to message](https://cloudtalkio.slack.com/archives/C09CK5AH1AB/p1756740238832619) |
| 17:26:47 | **Image posted by Marian Nociar**

Marian Nociar posted an image to the channel

At the moment, the Pipedrive pods are memory-stable. All queued events have been moved out of the queue (~300K events).
[link to message](https://cloudtalkio.slack.com/archives/C09CK5AH1AB/p1756740407719029) |
| 17:31:12 | **Image posted by Marian Nociar**

Marian Nociar posted an image to the channel

The initial and periodic sync is in progress. It will take some time to catch up and synchronize all missed events since the beginning of the outage.
[link to message](https://cloudtalkio.slack.com/archives/C09CK5AH1AB/p1756740672031359) |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/1068?tab=follow-ups)

- **[Pipedrive] Problem with Exports ‚úÖ**
    - **Owner:** @Martin Cermak
    - **JIRA ticket:** [INT-3376](https://cloudtalk.atlassian.net/browse/INT-3376)
- **Implement automatic escalation to the Incident based on a given threshold for event processing alerts - maybe not needed or even a duplicate of INT-3239**
    - **Owner:** @Tomasz Niedziela-Brach
    - **JIRA ticket:** https://cloudtalk.atlassian.net/browse/INT-3409
- **Cleanup our alerting channel - #dev-int-reporting is too noisy**
    - Owner: TBD
- **Observability regarding rate limits - throughput metrics - in progress**
    - JIRA ticket: https://cloudtalk.atlassian.net/browse/INT-3304
- **Alerting improvements Epic - to plan!**
    - JIRA ticket: https://cloudtalk.atlassian.net/browse/INT-3239",P2,,Integrations,Integrations Dev,"Architecture weakness, Missing proactive alerting, Reported by customers before we learned",https://www.notion.so/INC-1068-Pipedrive-synchronisation-not-working-significant-disruption-INT-3376-2622bccf284f8144b161ed459634ed1f,2025-09-01T14:37:03.939Z
1090,inc-1090-clients-are-facing-issues-with-ivr-10-09-2025,"*Generated by* @Michal Popoviƒç *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1090) on* September 11, 2025 9:25 AM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Infrastructure
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Michal Popoviƒç
- **Team member:** @Jakub Smadis
- **Incident commander:** @Michal Popoviƒç
- **Reporter:** @Zoltan Viragh
- **Active participants:** @Tomasz Niedziela-Brach, @Jiri Srba, @Zoltan Viragh, @Jakub Smadis, @Jaroslav Tomeƒçek, @Tom√°≈° Sak√°l, @Michal Popoviƒç
- **Observers:** @Lucia Mlyn√°rov√°, @Diego Sotorivero, @Miguel Pecsi

### üìù Custom Fields

- **Caused by:** [Asterisk](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JVW3SK537304MXEZV80TMNND)
- **Affected services:** [Asterisk](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JVW3SK537304MXEZV80TMNND)
- **Affected teams:** [Voice](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1XQDA2RQWR0STCA5F)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1090)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C09EHEPPW5B)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** September 10, 2025 9:49 PM (GMT+2)
- **Resolved at:** September 11, 2025 12:17 AM (GMT+2)
- **Impact started at:** September 10, 2025 6:10 PM (GMT+2)
- **Fixed at:** September 11, 2025 12:14 AM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 6 hours, 4 minutes

# üñäÔ∏è Summary

Clients are facing issues with IVR as we do not seem to be accepting/processing DMTF input from Twilio. DTMF input is used to select specific option in **Interactive Voice Response** by pressing specific number on dialpad. DTMF is responsible to send this number either binary encoded in audio stream (RFC2833) or by specific frequency beep (inband).

Our asterisk diaplan is using SIP headers to read call identifiers for call processing. To distinguish how the specific number carries DTMF codes we are reading destination number from *To* header.

Twilio started adding display name ""sipout"" to *To* SIP headers for unknown reason.

example: To: ‚Äú**sipout**‚Äù +421901234567 <sip:cloudtalk@sip-trunks.euc1.cloudtalk.io>

This quotes broke condition inside asterisk diaplan which was responsible for setting alternative inband DTMF mode for specific numbers. Condition failure triggered change of DTMF mode to **inband (specific frequency beep)** for all Twilio numbers instead to be applied only for numbers in condition.

# Leadup

The incident resulted from two independent changes

**Dialplan code to enable inband DTMF for specific numbers**

- specific german numbers obtained from BICS were using inband (audio) DTMF signalisation instead coded DTMF defined by rfc2833
- we have implemented a check where we set inband DTMF for these specific numbers
- check used =~ operator and was written syntactically correct but asterisk was not able to compare To header with regex if it contains double quotes because of build in quotation issue

**Addition of Display Name ‚Äúsipout‚Äù for To header in INVITEs sent from Twilio carrier**

- Twilio started using display name ‚Äúsipout‚Äù without any reason
- even when it is according the specification, we never received display name parameter in To header in the past as it is not used anywhere

# Fault

Failed regex operator evaluation combined with limitation of diaplan code where failure of condition causes execution of code that should be executed only when condition is met.

Example

```bash
same => n,GotoIf($[""${variable}"" =~ ""\+421901234567""]?code_to_execute:end)
same => n(code_to_execute), Noop(code to execute)
same => n(end), Exit
```

In following example we cant prevent execution of the second line if GotoIf function failed.

Failure is reported by following warning message

```c
ast_expr2.fl: ast_yyerror():  syntax error: syntax error, unexpected '<token>', expecting $end; Input:
```

In opposite in classic C style code failed evaluation does not triggers code

```c
if(condition) {
   code to exectute;
}
```

# Detection

Incident has been detected by customers reporting inability to send DTMF code for inbound calls owned by Twilio carrier. 

# 5 why's analysis

| Problem | **Why 1** | **Why 2** | **Why 3** | **Why 4** | **Why 5** | **Root Cause** | **Recurrence Prevention** | Tasks |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Callers were not able to send DTMF codes to inbound calls from Twilio carrier | Asterisk set DTMF mode to inband and was not able to process incoming DTMF codes. | Asterisk condition that switch DTMF mode to inband for some numbers failed. | Even if condition was syntactically correct it was not able to process SIP header with double quotes which appers in recent Twilio INVITEs |  |  | **Wrong condition handling for basic regex operator caused by asterisk implementation** | Refactor diaplan to check for similar issues with simple regex operator and fix them. | https://cloudtalk.atlassian.net/browse/VOIP-2219 |
|  |  | Code setting DTMF inband mode was executed. | Asterisk dialplan is using very primitive BASIC style of code structure and cant skip execution of code in condition if condition fails and code is defined after condition |  |  | **Condition logic in asterisk dialplan allows to execute code behind the condition even when condition evaluation fails** | Create a long term initiative to move dialplan functionality to current agi or rewrite agi implementation to golang. Changes will be iterative based on changes we are doing and decision to move logic from dialplan to php or golang will be based on complexity of the change. Define boundaries between dialplan and agi responsibilities. | https://cloudtalk.atlassian.net/browse/VOIP-2226 |
|  |  |  |  |  |  | **Implementation has been coded as a hotfix. Implementing it through database, agi and binary variable would be more robust and fault-tolerant.** | Try to push BICS to convert all DTMF tones to RFC2833 as we agreed in the contract, so we do not need to implement anything.
As an alternative implement permanent DTMF mode handling for specific numbers using database, agi and use admin GUI to set this value. | https://cloudtalk.atlassian.net/browse/VOIP-2220 |
| We were not able to detect isssue by monitoring | We were not able to catch and report WARNING message from asterisk log | We are not monitoring and reporting alerts from asterisk log | We are implementing asterisk log export this sprint and planning to implement automated alerting in the future |  |  | **Missing asterisk log export to datadog (currently under implementation) and missing prioritisation for automated alerting and detection** | Continue with implementation of asterisk logs export and prioritize automated alering and detection in datadog | https://cloudtalk.atlassian.net/browse/VOIP-2010

https://cloudtalk.atlassian.net/browse/VOIP-2011 |
|  |  |  |  | It took us 4 months to process and prioritise tasks related to voice components logs export to be able to monitor and alert issues from them | Late implementation of tasks related to incidents. | **Late implementation of tasks related to incidents** | Issues causing L1 and L2 incidents should be prioritose according our guidelines | Not available |
|  |  | Asterisk logs contains lot of WARNING messages related to old dialplan implementation | Asterisk dialplan has never been refactored |  |  | **Lot of warnings from dialplan execution which do not have impact on execution but fill logs with false warnings** | Refactor diaplan to remove unwanted warnings and alerts for better visibility and use dialplan linter to check the code before delivery | https://cloudtalk.atlassian.net/browse/VOIP-2221 |
|  |  |  |  |  |  |  | For regression tests add step to check dialplan logs for warnings | https://cloudtalk.atlassian.net/browse/VOIP-2222 |
| We were not able to detect such an issue during developemnt process | Our test cases are based on typical SIP traffic from carrier or agent side and does not include cases where we are testing SIP message processing against all posibilities valid in RFC |  |  |  |  | **Missing tests to check all SIP message format variances and its processing by our system** | As a part of regression tests implement stress testing to check if our system is able to process all valid SIP messages according to RFC | After discussion within the voice team we decided that this is not possible because of almost unlimited number of combinations and missing standard library for these tests. |
|  |  | Voice stack testing was never a priority. We are testing mainly only test cases limited to changed functionality. We lack proper smoke tests, regression tests are basic and stress testing is done only for large changes.  | Prioritisation of feature deliveries over the system stability. |  |  | **Prioritisation of feature deliveries over the system stability.** | We had an discussion inside the voice team how to prioritise and prepare tasks related to system stability. | Not available |
|  | We have never received a To SIP header from carrier side with display name | They are correct but never as there is no use case that would use display name in recipient number |  |  |  | **Missing knowledge of Twilio change in SIP message format.** | Check if Twilio does not have changelog for these types of changes. | https://cloudtalk.atlassian.net/browse/VOIP-2223 |

# Root cause

Wrong condition handling for basic regex operator caused by asterisk implementation.

Condition logic in asterisk dialplan allows to execute code behind the condition even when condition evaluation fails.

Implementation has been coded as a hotfix. Implementing it through database, agi and binary variable would be more robust and fault-tolerant.

Missing asterisk log export to datadog (currently under implementation) and missing prioritisation for automated alerting and detection.

Late implementation of tasks related to incidents.

Lot of warnings from dialplan execution which do not have impact on execution but fill logs with false warnings.

Missing tests to check all SIP message format variances and its processing by our system.

Prioritisation of feature deliveries over the system stability.

Missing knowledge of Twilio change in SIP message format.

# Mitigation and resolution

See Recurrence Prevention column in 5-why table.

# Lessons learnt

**What went well?**

Cooperation between L2 support and voice has been quick and efficient. We were able to found the issue fast using our logs.

**What could have gone better?**

Automated detection was not possible because of log alerting limitations. We are already working on improvements.

**What else did you learn?**

We have discuss the improvements on how to prioritise and manage technical debt initiatives.

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-09-10** |  |
| 18:10:00 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 19:30:00 | L1 Received First Ticket |
| 20:00:00 | L1 Raised Issue With Twilio |
| 21:30:00 | Escalated to L2 |
| 21:49:25 | **Incident reported by Zolt√°n Ill√©s**

Zolt√°n Ill√©s reported the incident
Severity: P2
Status: Investigating |
| 21:49:30 | **Incident escalated**

Your workflow manually escalated the incident to the escalation path [Platform escalation path](https://app.incident.io/cloudtalkio/on-call/escalation-paths/01HYDHY88AJ9FPGQEVNCYAX78Y) |
| 22:41:45 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

As far as we know so far this issue is only affecting Twilio numbers. We are currently investigating if the issue is on our side.

So far it looks like issue on Twilio side, Popi is doing tests together with Zoli |
| **2025-09-11** |  |
| 00:09:53 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

The root cause:

For the past 7 months we had a code where we were changing DTMF mode (from RFC2833 to Inband).
This code was working correctly for the 2  numbers that required this special mode.

For some unknown reason the code started malfunctioning today and the DTMF was changed for only twilio numbers.

We have removed this code temporarily and we will fix it tomorrow for the 2 specific numbers.

```
    -- <SIP/incoming-trunks-00000003>AGI Script /usr/lib/cloudtalk/agi/incoming.php completed, returning 0
    -- Executing [+12023018804@incoming:5] Macro(""SIP/incoming-trunks-00000003"", ""in_test"") in new stack
    -- Executing [s@macro-in_test:1] NoOp(""SIP/incoming-trunks-00000003"", ""macro-in_test"") in new stack
[2025-09-10 23:48:55] WARNING[20970][C-00000003]: ast_expr2.fl:470 ast_yyerror: ast_yyerror():  syntax error: syntax error, unexpected '<token>', expecting $end; Input:
""""sipout"" sip:+12023018804@sip-trunks-euc1.dev.cloudtalk.io:5060"" =~ ""\+43720511333""
  ^
[2025-09-10 23:48:55] WARNING[20970][C-00000003]: ast_expr2.fl:474 ast_yyerror: If you have questions, please refer to https://wiki.asterisk.org/wiki/display/AST/Channel+Variables
    -- Executing [s@macro-in_test:2] GotoIf(""SIP/incoming-trunks-00000003"", """"""?4:3"") in new stack
    -- Goto (macro-in_test,s,4)
    -- Executing [s@macro-in_test:4] SIPDtmfMode(""SIP/incoming-trunks-00000003"", ""inband"") in new stack
    -- Executing [s@macro-in_test:5] MacroExit(""SIP/incoming-trunks-00000003"", """") in new stack
```
 |
| 00:13:26 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

We have release fix to all regions and we successfully tested that it is working.

@michal.popovic will update status page

@Zoli will reach out to L1 to notify them that we managed to fix the issue |
| 00:14:00 | **Fixed at**

Custom timestamp ""Fixed at"" occurred |
| 00:17:32 | **Incident resolved and entered the post-incident flow**

Jakub Smadis shared an update
Status: ~~Investigating~~ ‚Üí Documenting |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/1090?tab=post-incident)",P2,Asterisk,Asterisk,Voice,"Missing proactive alerting, Insufficient testing, Reported by customers before we learned",https://www.notion.so/INC-1090-Clients-Are-Facing-Issues-with-IVR-26b2bccf284f81b68e8ec5fa36e6507c,2025-09-10T19:49:25.887Z
1091,inc-1091-voice-agent-are-not-working-due-to-the-vendor-issue-billing-10-09,"*Generated by* @Peter Bakos *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1091) on* September 12, 2025 5:01 PM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Peter Bakos
- **Reporter:** @Jakub Smadis
- **Incident commander:** @Peter Bakos
- **Active participants:** @Jaroslav Tomeƒçek, @Peter Bakos, @Jiri Srba, @Jakub Smadis, @Lukas Masar
- **Observers:** @Filip Prosovsky, @Martina Redajova, @Dino Trojak, @Zoltan Viragh, @Lucia Mlyn√°rov√°, @Tom√°≈° Sak√°l, @Jakub Bober

### üìù Custom Fields

- **Affected services:** [Voice Agent](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRE8R8AVNVBE7PP53S4349)
- **Affected teams:** [AI Team](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1GS8XEFEM2EXF9CN2)
- **Root Cause Label:** 3rd party and Missing proactive alerting

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1091)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C09FDU7KSU8)
- [Status page: CloudTalk - VoiceAgent Calls not executing](https://status.cloudtalk.io/incidents/01K4TP84A9MNCQJ0XNJVE7KNJJ)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** September 10, 2025 10:31 PM (GMT+2)
- **Resolved at:** September 11, 2025 1:30 AM (GMT+2)
- **Impact started at:** September 10, 2025 10:31 PM (GMT+2)
- **Identified at:** September 10, 2025 10:31 PM (GMT+2)
- **Fixed at:** September 11, 2025 1:30 AM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 34 minutes

# üñäÔ∏è Summary

**Problem**: All VoiceAgents have stopped working due to a vendor billing issue.

**Impact**: None of the VoiceAgents are working. No customers have reported issues yet, but customers will be unable to use VoiceAgent services until the vendor resolves the billing issue.

**Root cause**: We ran out of credits and overage allowance with our vendor, 11labs, which has stopped service until credits are restored.

**Steps to resolve**: We escalated the issue to our vendor contact at 11labs and are waiting for them to restore service. There is currently no workaround.

# Leadup

- MoM increase of VoiceAgent usage caused that we‚Äôve used up all of our 11Labs credits

# Fault

- There is an ""overage"" setting turned on
    - so that we still can consume credits above the 13M but they will charge us extra
- What got f*cked up is that the overage was capped at 1M tokens (around 100‚Ç¨) for some reason and couldn't be changed

# Detection

- @Lukas Masar noticed the VA calls are not running and called @Peter Bakos as he saw the logs that the cause is missing credits.

# 5 why's analysis

# Root cause

# Mitigation and resolution

- Short term
    - We‚Äôve messaged our contact at 11Labs in the evening but he operates in CET timezone so we didn‚Äôt expect him to react ‚Üí but he did, he topped up our credit at 1:30AM
    - Then we scheduled a call for early morning, where we found a workaround how to increase our overage cap
        - So now we can consume twice our limit if we need to, we would just be charged extra
        - I can increase this limit even more now but I don't want to just yet because that created higher possible risk that someone would use up tens of thousands of dollars without paying us
- Long term
    - We need to switch to Enterprise plan either way (for higher concurency limits) so this credit issue is only relevant for next couple of weeks

# Lessons learnt

What went well? What could have gone better? What else did you learn?5

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-09-10** |  |
| 22:31:00 | **Identified at**

Custom timestamp ""Identified at"" occurred |
| 22:31:57 | **Incident reported by Jakub Smadis**

Jakub Smadis reported the incident
Severity: P2
Status: Investigating |
| 22:31:57 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 22:32:02 | **Incident escalated**

Your workflow manually escalated the incident to the escalation path [Platform escalation path](https://app.incident.io/cloudtalkio/on-call/escalation-paths/01HYDHY88AJ9FPGQEVNCYAX78Y) |
| 22:32:55 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

https://cloudtalkio.slack.com/archives/C01R3694B6E/p1757535919928349 |
| 23:06:32 | **Incident paused**

Peter Bakos shared an update
Status: ~~Investigating~~ ‚Üí Paused
We have nothing to do than to wait for the vendor to get back to us in the morning (EU time). |
| **2025-09-11** |  |
| 01:30:00 | **Fixed at**

Custom timestamp ""Fixed at"" occurred |
| 5h later |  |
| 07:17:34 | **Incident resumed**

Peter Bakos shared an update
Status: ~~Paused~~ ‚Üí Documenting
AS of 1:30AM our all VAs are working again. I have a call with 11labs today in the morning to find a solution to avoid this in the future. |",P2,,Voice Agent,AI Team,"Missing proactive alerting, 3rd party",https://www.notion.so/INC-1091-Voice-Agent-are-not-working-due-to-the-vendor-issue-billing-26c2bccf284f81899059f38a07c56da5,2025-09-10T20:31:57.954Z
1113,inc-1113-customers-not-recieving-calls-23-09-2025,"*Generated by* @Dino Trojak *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1113) on* September 23, 2025 5:48 PM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Filip Prosovsky
- **Reporter:** @Dino Trojak
- **Incident commander:** @Dino Trojak
- **Active participants:** @Filip Prosovsky, @Jakub Smadis, @Dino Trojak
- **Observers:** @Jaroslav Tomeƒçek, @Lucia Mlyn√°rov√°, @Zoltan Viragh, @Tom√°≈° Sak√°l, @Michal Ustanik, @Erik Dvorcak, @Michal Popoviƒç

### üìù Custom Fields

- **Caused by:** [Realtime WS](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDYW12EFZTJBHQRZM1N1M)
- **Affected services:** [Realtime WS](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDYW12EFZTJBHQRZM1N1M)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1113)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C09H394BS49)

### ‚è±Ô∏è Key Timestamps

- **Issue started at:** September 20, 2025
- **Reported at:** September 23, 2025 5:18 PM (GMT+2)
- **Impact started at:** September 23, 2025 5:18 PM (GMT+2)
- **Fixed at:** September 23, 2025 5:34 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration: 4** days

# üñäÔ∏è Summary

Since Saturday, September 20th, all of our non-EU customers were unable to receive calls.

In total, this affected approximately 8,300 out of 15,400 users since Saturday morning.

# Leadup

With releasing regional Kubernetes and multi-mesh Istio with failover, we introduced new wasy of geolocation routing. This was properly tested on stage for EU and Sydney regions across clusters. We did not notice any unusual behaviour during testing, the traffic either stayed in the regional cluster or was failover to EU region with our custom routing design.

Once everything was deployed on Saturday 20.9.2025, websocket connections in regions stopped working as sticky session was broken.

# Fault

The flow of failover to europe region caused, that header used for stickiness was omitted at some place in the flow and every request was forwarded to different pod in Kubernetes cluster. [Socket.io](http://Socket.io) requires 3 HTTP requests to end up in the same pod to establish socket connection, thus needing the stickiness.

We noticed that after making request to realtime-ws we received response to one of those requests  ‚ÄúSession ID unknown‚Äù.  Also there was increased amount of errors for realtime-ws in datadog ‚Äú[SocketService]: Failed to connect to socket server at [https://realtime-ws.cloudtalk.io](https://realtime-ws.cloudtalk.io/)‚Äù

# Detection

Customers reported that [**agents were seeing a circle instead of a dot for their status, preventing them from receiving calls](https://cloudtalk.atlassian.net/jira/software/c/projects/APPS/boards/89?selectedIssue=APPS-1943).**

Our auto alerting and monitoring failed.

# 5 why's analysis

**Why 1:** Why were agents seeing a circle instead of a dot for their status?

Because the realtime web socket connection was not establishing properly for non-EU customers.

**Why 2:** Why were customers unable to receive calls?

Because the realtime web socket connection was not establishing properly for non-EU customers.

**Why 3:** Why wasn't the web socket connection establishing properly?

Because there was an issue with the Realtime WS service that caused high disconnect rates for non-EU customers, as evidenced by the RealtimeSocketsHighDisconnectRate30 alert.

**Why 4:** Why did the Realtime WS service have high disconnect rates?

Because certain probes in the service were causing connection instability for users outside the EU region.

**Why 5:** Why wasn't this issue detected earlier?

Because our monitoring and alerting systems failed to detect the problem affecting non-EU customers specifically, allowing the issue to persist for several days before being reported by customers directly.

# Root cause

NON EU customer that were logged in to phone app were automatically routed through our infra to eu service. However at one point there was failure for sticky session and consecutive requests were led to different pods in kubernetes. This translated to Session ID unknown and response code 400. After that users didnt have any connection to realtime-ws. 

Session should be preserved even with using our new traffic routing.

# Mitigation and resolution

What steps did you take to resolve this incident?

We have rerouted **analytics-realtime** and **realtime-ws** endpoints from wildcard to actual DNS records directly to EU loadbalancer instead of Global Accelerator geolocation to keep the stickiness. This solved the issue and we put the hotfix changes to the code to keep it stable for the future.

# Lessons learnt

We cannot always count only on alerting, there needs to be backup if it fails. 

We need to improve monitoring and alerting in the apps team, next actionable steps:

- All our services should be monitored and alerting from infra perspective
- All our services and their responses will be monitored - any unusual number of error requests/responses triggers alert
- We will add more logs to all the services
- We will have a unified apps datadog dashboard where you could observe all of the things mentioned above in a single place

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-09-23** |  |
| 17:18:06 | **Incident reported by Dino Trojak**

Dino Trojak reported the incident
Severity: P2
Status: Investigating |
| 17:18:06 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 17:18:11 | **Escalated to Platform escalation path**

Your workflow manually escalated the incident to the escalation path [Platform escalation path](https://app.incident.io/cloudtalkio/on-call/escalation-paths/01HYDHY88AJ9FPGQEVNCYAX78Y) |
| 17:34:35 | **Fixed at**

Custom timestamp ""Fixed at"" occurred |
| 17:34:35 | **Status changed from Investigating ‚Üí Monitoring**

Filip Pro≈°ovsk√Ω shared an update
Status: ~~Investigating~~ ‚Üí Monitoring
We estimate around 700 customers have not been receiving calls, with approximately 200,000 failed attempts reported in the last few hours. The issue is affecting the outbound call delivery service.
A critical alert ([RealtimeSocketsHighDisconnectRate30](https://thanos.cloudtalk.io/graph?g0.expr=rate%28socket_io_disconnect_total%5B1m%5D%29+%3E+30&g0.tab=1)) has been triggered due to a high disconnect rate in the realtime-ws service. Filip removed certain probes in an effort to restore realtime functionality.
Efforts are underway to verify that the application is functioning correctly and to coordinate an update to the status page. |",P2,Realtime WS,Realtime WS,,"Missing proactive alerting, Infrastructure, Insufficient testing, Reported by customers before we learned",https://www.notion.so/INC-1113-Customers-not-recieving-calls-2772bccf284f81bba7e4da4cebca1bca,2025-09-23T15:18:06.028Z
1147,inc-1147-incoming-calls-on-mobile-are-not-working-06-10-2025,"*Generated by* @Anton Shyrokoborodov *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1147) on* October 8, 2025 4:25 PM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Michal Ustanik
- **Reporter:** @Jiri Srba
- **Incident commander:** @Dino Trojak
- **Active participants:** @Michal Ustanik, @Jiri Srba, @Elena Darriba, @Debby Wu, @Peter Stanko, @Dino Trojak, @Jaroslav Tomeƒçek, @Tom√°≈° Sak√°l, @Tomas Sykora, @Simona Drinkova, @Filip Prosovsky, @Anton Shyrokoborodov
- **Observers:** @Roman Hartig, @Jakub Smadis, @Lucia Mlyn√°rov√°, @Zoltan Viragh, @Arnaldo Tema, @Jakub Bober

### üìù Custom Fields

- **Caused by:** [Push Notifications](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDXZY5G06TBEVGB0EZVWF)
- **Affected services:** [Mobile](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDRSB964EKMG3YK0NW7PR)
- **Affected teams:** [App Devs](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1Y13N5ZYW63255X4W)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1147)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C09K9CG5E2D)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** October 6, 2025 8:51 AM (GMT+2)
- **Resolved at:** October 6, 2025 2:33 PM (GMT+2)
- **Impact started at:** October 6, 2025 8:51 AM (GMT+2)
- **Identified at:** October 4, 2025 2:33 PM (GMT+2)
- **Fixed at:** October 6, 2025 9:58 AM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 1 hours, 6 minutes

# üñäÔ∏è Summary

**Problem**: Push notifications related to incoming calls on mobile devices were failing due to a configuration error.

**Impact**: Push notifications about incoming calls on mobile devices were not working for all users until the fix was deployed.

**Root cause**: The push notifications service was misconfigured to use an outdated, unauthenticated Redis instance, which blocked required connections for incoming mobile calls.

**Steps to resolve**: We corrected the Redis connection parameters, updated deployment configurations, released new code, and confirmed that both push notifications and calls now work as expected.

# Leadup

As part of the infrastructure migration, all services were being transitioned from the legacy **non-auth Redis** to the new **AUTH Redis with TLS**.

The push-notifications service had already been correctly configured to use the new Redis authentication flags (PUSH_NOTIFICATIONS_REDIS_TLS_ENABLED, REDIS_AUTH_ENABLED) and updated Helm chart.

However, the **non-auth Redis** was still running and reachable from production during testing. Because of that, even though the service configuration still pointed to the **old endpoint**, all health checks and functional tests passed ‚Äî masking the misconfiguration and giving the impression that the migration was complete.

# Fault

After the **non-auth Redis** instance was later disabled in production, the push-notifications service continued attempting to connect to it (10.1.8.125:6379).

This resulted in repeated i/o timeout errors when establishing a Redis connection.

Since the service relies on Redis for **storing and dispatching push-notification events** to Firebase (FCM), it became unable to send **any push notifications**.

As a result, **no mobile push notifications were delivered to customers**, which meant **users were not notified about incoming calls** on their mobile devices.

# Detection

The issue was detected manually at **08:51 UTC on 2025-10-06** by **Jirka Srba**, who observed Redis connection timeouts in the service logs.

No automated alerts were triggered at the time.

The incident was resolved at **14:33 UTC** after configuration and redeployment.

# 5 why's analysis

1. Why were customers not notified about incoming calls?
    
    ‚Üí Because the push-notifications service stopped sending push notifications.
    
2. **Why** did the service stop sending notifications?
    
    ‚Üí It couldn‚Äôt connect to Redis.
    
3. **Why** couldn‚Äôt it connect to Redis?
    
    ‚Üí The configuration still pointed to the old, non-auth Redis endpoint that was later disabled.
    
4. **Why** was this not caught during testing?
    
    ‚Üí The non-auth Redis was still active during testing, so connectivity checks passed even with the wrong endpoint.
    
5. **Why** was there no early alert?
    
    ‚Üí No Redis connectivity or push-notification delivery monitoring existed for this service.
    

# Root cause

The push-notifications service was properly configured to support AUTH Redis, but its environment variables still referenced the **deprecated non-auth Redis endpoint**.

Because that endpoint remained available during testing, the issue went unnoticed.

When the non-auth Redis was decommissioned, the service lost Redis connectivity and was unable to send any push notifications, preventing customers from being notified about incoming calls.

# Mitigation and resolution

- Engineers confirmed the service was attempting to connect to the deprecated non-auth Redis.
- Helm chart and configuration were updated to point explicitly to the **AUTH Redis** endpoint.
- The service was redeployed with the corrected configuration.
- Redis connectivity and push notification delivery were verified after redeploy.
- The incident was declared resolved at **14:33 UTC**.

# Lessons learnt

- Ensure **legacy infrastructure (e.g. non-auth Redis)** is disabled or isolated before testing new configuration changes, to avoid false positives.
- Introduce **monitoring alerts** for Redis connection failures and push-notification delivery anomalies.
- Update the **migration checklist** to explicitly verify endpoint references for each service before decommissioning legacy components.

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-10-04** |  |
| 14:33:00 | **Identified at**

Custom timestamp ""Identified at"" occurred |
| 2d later |  |
| **2025-10-06** |  |
| 08:51:32 | **Incident reported by Jirka Srba**

Jirka Srba reported the incident
Severity: P3
Status: Investigating |
| 08:51:32 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 08:51:37 | **Escalated to Platform escalation path**

Your workflow manually escalated the incident to the escalation path [Platform escalation path](https://app.incident.io/cloudtalkio/on-call/escalation-paths/01HYDHY88AJ9FPGQEVNCYAX78Y) |
| 09:26:31 | **Severity upgraded from P3 ‚Üí P2**

Jaroslav Tomeƒçek shared an update
Severity: ~~P3~~ ‚Üí P2 |
| 09:27:56 | **Message from Peter Stanko**

Peter Stanko pinned their own message

Fix Pr: https://github.com/CloudTalk-io/ctgo/pull/932 |
| 09:42:47 | **Message from Peter Stanko**

Peter Stanko pinned their own message

Release PR: https://github.com/CloudTalk-io/argocd-kube/pull/8030 |
| 09:44:00 | **Message from Jaroslav Tomeƒçek**

Jaroslav Tomeƒçek pinned their own message

For post-mortem: During weekend we received a related alert, which was, however, not clearly named and also seemed to be one-timer. Cc @Jirka Srba, can you please send the alert? |
| 09:52:50 | **Message from Peter Stanko**

Peter Stanko pinned their own message

Fix Chart PR: https://github.com/CloudTalk-io/argocd-kube/pull/8034 |
| 09:58:00 | **Fixed at**

Custom timestamp ""Fixed at"" occurred |
| 09:58:02 | **Message from Peter Stanko**

Peter Stanko pinned their own message

Looks like kafka deploymnet was also fixed and deployment is green |
| 11:26:05 | **Status changed from Investigating ‚Üí Monitoring**

Dino Trojak shared an update
Status: ~~Investigating~~ ‚Üí Monitoring |
| 3h later |  |
| 14:33:03 | **Incident resolved and entered the post-incident flow**

Dino Trojak shared an update
Status: ~~Monitoring~~ ‚Üí Documenting |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/1147?tab=post-incident)

- **Remove non-auth redis instances on DEV and STAGE to find another connection issue**
    - **Owner:** @Jiri Srba
    - **JIRA ticket:** [INFRA-2856](https://cloudtalk.atlassian.net/browse/INFRA-2856)",P2,Push Notifications,Mobile,App Devs,"Insufficient testing, Infrastructure, Missing proactive alerting, Human error",https://www.notion.so/INC-1147-Incoming-calls-on-mobile-are-not-working-2862bccf284f818681a6df4f69974c77,2025-10-06T06:51:32.700Z
1151,inc-1151-call-information-processing-kafkaconsumerlagoffsetsveryhighcip-07,"call-information-processing KafkaConsumerLagOffsetsVeryHighCip
Problem: Kafka consumer lag exceeded 15,000 offsets for the call-information-processing service, causing messages to not be processed as expected.
Impact: High Kafka consumer lag (over 15,000 offsets) built up in the call-information-processing service in the prod-eu-central-1 cluster. This may have delayed processing of call statistics events, but no customer-facing impact has been confirmed.
Root cause: A missing redis-auth reference from parameter store prevented the call-information-processing service from starting after a release.
Steps to resolve: We restored the missing redis-auth reference, restarted the affected deployment, and confirmed that the Kafka consumer lag alert was resolved. Monitoring shows the lag offset is now decreasing.

What's going on?
The call-information-processing application's Kafka consumer group is experiencing a critical backlog with over 15,000 unprocessed messages in the call-statistics-events-prod topic. This lag is directly impacting analytics and reporting functionality. The team has taken several remediation steps including rolling back to version 1.44, aligning the service replica count to match Kafka partitions (12 replicas), removing Kubernetes probes, and increasing the rebalancing timeout to 10 minutes.
What can I do next?
1. Monitor Kafka consumer lag metrics for the call-statistics-events-group on the call-statistics-events-prod topic over the next 15-30 minutes to assess impact of recent changes [1]

Examining the alert
The alert indicates a critical Kafka consumer lag issue in the call-information-processing application, where the consumer group 'call-statistics-events-group' has accumulated over 15,000 unprocessed messages in the production Kafka cluster. This suggests the consumer is unable to keep up with the rate of incoming call statistics events, potentially causing delays in analytics and reporting functionality.


Recommendation to the future will be:
to use predictive scaling and use our KEDA. This will ensure the max replicas will be the max partition number as described in https://keda.sh/docs/2.17/scalers/apache-kafka/
Increase rebalancing time

We also identified missing redis-auth reference from parameter store. CIP could not start after release
Roman Hartig confirmed that the team has reconfigured the service to use the new authenticated Radius, replacing the decommissioned one that caused the initial crash loop, and expects the service to recover shortly.

Summary
The incident involved a Kafka consumer application that was unable to start after a redeploy. The root cause was identified as the application referencing an old, decommissioned Redis instance, which was no longer accessible due to updated security groups. The team updated the configuration to point to the new authenticated Redis, applied Terraform changes to update the parameter store, and refreshed external secrets. After these changes, the deployment was restarted and the application began booting correctly. The team discussed the importance of ensuring correct configuration for Redis and related secrets in both staging and production environments. They also noted that this incident served as a useful exercise for handling future maintenance windows and migrations, such as upcoming MongoDB migrations. The health of the deployment is being monitored, and no further issues are currently reported.
Next steps
Continue monitoring application health after redeployment
Ensure configuration and secrets are correct in all environments
Prepare for future maintenance and migrations (e.g., MongoDB migration)

sum up:

new release of cip caused new non-healthy-yet PODs creation in k8s

those PODs triggered Kafka consumer group rebalancing

rebalancing may close connection for the client which produces in our case ‚ÄúConnection is closed‚Äù error

we don‚Äôt have cip Kafka client configured to retry the connection today but it‚Äôs not generally needed as the POD is restarted automatically

in this case new app init happened where redis connection is initialized however cip was still configured to use old decommisioned non-auth redis instance

result was crash loop back of all cip PODs

probes times updated.",P2,Call Information Processing,Call Information Processing,,"Missing proactive alerting, Infrastructure, Insufficient testing, Release problems",,2025-10-07T08:16:44.324Z
1152,inc-1152-ai-analytics-voice-agent-api-07-10-2025,"What's going on?
The AI analytics tab failed to display any calls and showed errors for about one hour because the ai-calls endpoint in the Statistics API returned HTTP 404 errors. This occurred because the production Statistics API was misconfigured to connect to the development Voice agent API endpoint instead of the production endpoint.
What caused it?
The production environment configuration variables for the Statistics API were incorrectly set to point to the development Voice agent API endpoint rather than the production endpoint.
What can I do next?
1. Update the production Statistics API configuration to point to the correct Voice agent API endpoint and apply changes through a service restart or redeployment if required.
2. Manually test the ai-calls endpoint in production after the configuration change to confirm that 404 errors are resolved and call data displays as expected.
3. Monitor the AI analytics tab and endpoint for at least 15 minutes post-fix to ensure errors do not recur and normal operations are restored.
",P2,,Statistics API,"AI Team, Dashboard Devs","Human error, Insufficient testing, Release problems",https://www.notion.so/INC-1152-AI-Analytics-Voice-agent-api-2852bccf284f810fad37d24820b4e406,
1155,inc-1155-cfd-not-working-for-customers-08-10-2025,"- When customer removes the nodes of existing Call Flow, whole CFD screen will become blank, and even the initial number node will disappear. 
-newly added  numbers  do not have the initial node - preventing customers from  configuring the call flow


Only workaround was to contact Customer support  -> we can use phadmin to add the 1st node. Causing major  disruption


Root cause was swiftly identified - we bumped our Angular version earlier this week, and this part of the CFD flow is not covered with tests, so it caused a regression that we weren't able to catch during manual testing.
We reverted the Angular patch which solved it immediately
We're fixing the bug locally, using the new Angular version, and attempt to deploy again
(later) We'll invest in covering the CFD flow with automation tests

upd:

Looks like upgrade to Angular 19 wasn't the root cause, actually it's the bug in ngx-graph version 10, which was upgraded as well to resolve conflicts in dependencies.
Workaround, for now, is to add dummy link when graph contains single node.",P2,Call Flow Designer,Call Flow Designer,Dashboard Devs,,https://www.notion.so/INC-1155-CFD-not-working-for-customers-2872bccf284f8198ae6dfaaa7a4286b3,2025-10-08T07:51:53.514Z
1158,inc-1158-playback-sound-is-not-played-on-inbound-calls-workarounds-are-not,"*Generated by* @Jakub Smadis *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1158) on* October 9, 2025 4:39 PM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Tomasz Niedziela-Brach
- **Reporter:** @Ji≈ô√≠ Missbach
- **Incident commander:** @Tomasz Niedziela-Brach
- **Active participants:** @Jiri Srba, @Arnaldo Tema, @Ji≈ô√≠ Missbach, @Michal Popoviƒç, @Jaroslav Tomeƒçek, @Filip Prosovsky, @Jakub Smadis, @Tomasz Niedziela-Brach, @daniel malachovsky
- **Observers:** @Laura Misencikova, @Debby Wu, @Lucia Mlyn√°rov√°, @Zoltan Viragh, @Roman Hartig, @Dino Trojak, @Tom√°≈° Sak√°l, @Andr√© Carvalho

### üìù Custom Fields

- **Caused by:** [Music Sync](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDSPVWTTDQNF97F88X8AS)
- **Affected services:** [Asterisk](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JVW3SK537304MXEZV80TMNND)
- **Affected teams:** [Voice](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1XQDA2RQWR0STCA5F)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1158)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C09KKK6429G)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** October 9, 2025 9:09 AM (GMT+2)
- **Resolved at:** October 9, 2025 3:17 PM (GMT+2)
- **Impact started at:** October 4, 2025 7:00 AM (GMT+2)
- **Fixed at:** October 9, 2025 2:20 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 5 days, 7 hours

# üñäÔ∏è Summary

A legacy **music-sync process in old production** was still running and processing uploaded sound messages. Applications and dashboards were **still sending SQS messages to the old production queue**, and the old music-sync was copying and processing them into the new production.

However, the **old Redis used by music-sync was deleted** (not intended to be migrated), causing the process to fail and message processing to stall. Since the old music-sync was scheduled for decommission and lacked proper monitoring, the issue went unnoticed for days.

Customers successfully uploaded the sounds to the dashboard, but the sound was never played during the call.

# Leadup

### **1. Migration to New Production Began**

The organization started migrating workloads from the old production environment to a new one. As part of this effort, a plan was made to eventually decommission the **old music-sync service**, which handles sound-processing tasks and relays messages between systems.

### **2. Assumption: Old music-sync would be removed soon**

Because music-sync in the old production was considered temporary, the team decided **not to maintain full monitoring or alerting** on it. It was believed that this service would be turned off shortly after migration.

### **3. Reality: Old music-sync remained active in production**

However, the decommission never happened. The old music-sync service **continued to run in the background**, still processing sound-related events.

### **4. Applications still publishing to old SQS**

Multiple teams (Apps, Dashboard, BIT) **continued sending SQS messages to the old production queue**.

Even though the new SQS was available, the application migrations were delayed or deprioritized. As a result:

- Messages flowed into the old SQS.
- The old music-sync picked them up.
- It then forwarded them to the new SQS and processed them.

This effectively made the old music-sync a **critical part of the live pipeline.**

### **5. Hidden dependency: Old Redis**

The old music-sync still used the **old production Redis instance** for locking and state tracking.

This dependency was **not documented or tracked**, and was **not communicated to teams performing infrastructure cleanup**.

### **6. Cleanup event: Old Redis was deleted**

As part of migration cleanup, the old Redis instance was removed. The teams performing the cleanup believed no active services were using it.

### **7. Silent failure due to missing monitoring**

Once Redis was removed, the old music-sync began failing to process or forward messages.

However, because monitoring and alerting had been removed (based on the assumption that the service was no longer in use), **no one was notified of the failure**.

### **8. Message backlog silently built up for ~5 days**

For several days (Oct 4 to Oct 9), uploaded sound messages accumulated in the system without being processed.

No automatic alerts were triggered, and no one noticed immediately.

### **9. Incident only discovered when delays became obvious**

On **2025-10-09**, customers recognized that sounds were not played when they tested playback in inbound calls.

# Fault

The core fault was a **hidden, unintended dependency on music-sync in old production** that was still part of the live message-processing pipeline.

Although the organization had migrated to a new production environment, several critical components were **still writing SQS messages to the old production queue**. The old music-sync service would read these messages, process them, and then forward them to the *new* production SQS.

@Jakub Smadis raised the necessity to migrate to new production SQS and delete the old prod music sync in april, but the tasks were deprioritized and none of the 3 teams implemented it.

# Detection

Customers reported the issue to support and support escalated it through L2 to the L3.

# 5 why's analysis

### **1. Why did sound processing stop?**

Because the **music-sync service in old production stopped working**, so messages were no longer being forwarded or processed.

---

### **2. Why did the old music-sync service stop working?**

Because it **depended on old Redis**, which was **unavailable**, causing music-sync to fail.

---

### **3. Why was Redis unavailable while music-sync still depended on it?**

Because the **dependency on old Redis was not documented**, and it was assumed Redis could be safely removed once the old music-sync was no longer needed.

---

### **4. Why did music-sync in old production still rely on old Redis, and why did it impact new production?**

During the migration to new production (until March 2025), the **voice team needed to run music-sync in both old and new environments** to keep messages synced, so the old Redis remained in use.

---

### **5. Why was this dependency not removed after the voice stack migration?**

Because although the team **communicated in April 2025** that all applications should switch to the new production SQS, the related tasks were **created but deprioritized and never implemented**, leaving the old music-sync dependent on legacy Redis.

---

---

# Root cause

The incident occurred due to **legacy dependencies in the old production environment combined with incomplete migration follow-through**:

1. **Old music-sync failure:** The old music-sync service stopped processing messages because it depended on **old Redis**, which became unavailable.
2. **Undocumented dependency:** The dependency on old Redis was **not documented**, and it was assumed Redis could be removed safely once the old music-sync service was no longer needed.
3. **Legacy environment still in use:** During migration to new production (until March 2025), music-sync had to run in **both old and new environments** to keep messages synchronized. This kept old Redis in active use.
4. **Incomplete migration follow-up:** In April 2025, teams communicated that all applications should switch to the **new production SQS**, but the related tasks were **created, deprioritized, and never implemented**, leaving the old music-sync dependent on legacy infrastructure.

# Mitigation and resolution

The infra team enabled a security group that allowed the music-sync service to access the old production Redis.

That enabled the music sync processing, but since we had 5 days of unprocessed messages (362), the old messages contained files that were already deleted. This caused the music sync to spend time on checking if the file exists, this triggered a hidden bug where music sync crashed and was restarted.

Since we used Redis lock and SQS visibility timeouts, the restart caused music sync to be stuck, waiting for redis lock to expire (33 minutes).

We have removed the Redis lock dependency but during the deployment, we found out that port action to deploy the service crashed the service. This created uncertainty in the developers since they couldn‚Äôt trust the automation.

They had to be extra careful to test manual deployments in dev and stage deployments to not cause more harm.

Once they managed to release it to production, they found out that music sync is still crashing because of the bug, and the music sync is stuck again.

This time, music sync was setting a visibility timeout on a message to 10 minutes, which meant that when music sync crashed and was restarted, it had to spend 10 minutes waiting to start processing again.

The team wanted to speed up the processing, so they released (with the manual deployment and double checks) a new version with 60 seconds of visibility timeout (the value was hardcoded).

The team found out that messages are not being processed because the missing file in S3 triggered a retry mechanism that caused the message would be processed in 90 seconds. This caused another issue where music sync was stuck because it couldn‚Äôt process the message.

Team had to release yet another change where they set the visibility timeout to 5 minutes and let the music sync process the messages.

This resulted in a success where the messages were processed in a fast pace.

# Lessons learnt

- Do not let initiatives for clean up die because other teams are not prioritizing it
- Maintain applications over time, so in case of an inciden,t we can react faster
- Music sync needs a lot more love

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-10-04** |  |
| 07:00:00 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 5d later |  |
| **2025-10-09** |  |
| 09:09:57 | **Incident reported by Jiri Missbach**

Jiri Missbach reported the incident
Severity: P2
Status: Investigating |
| 09:10:02 | **Escalated to Platform escalation path**

Your workflow manually escalated the incident to the escalation path [Platform escalation path](https://app.incident.io/cloudtalkio/on-call/escalation-paths/01HYDHY88AJ9FPGQEVNCYAX78Y) |
| 10:27:50 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

Added back rule for legacy prod subnet where music-sync runs to Elasticache so music-sync can connect to old redis |
| 10:50:06 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

Root cause:
- Dashboard and apps are sending SQS messages about uploaded sounds to old production SQS
- Music sync in the old production is still running. It is copying the messages to the new prod SQS and it is still processing the sounds
- Music sync in the old prod doesn‚Äôt have proper monitoring since it was planned to be decommissioned 
    ‚ó¶ I have reached out to the teams in april where we proposed a plan to delete the music sync https://cloudtalkio.slack.com/archives/C03FGGA6VQX/p1745311600057759
- We have forgotten that music sync in old prod still uses old prod redis, it was never meant to be migrated but we were not aware when we were working on music sync auth redis that old redis will be deleted right away
Next steps:
- Thanks to Filip help we are processing the messages
- Remove redis lock to process the messages faster
- Post mortem items:
    ‚ó¶ Apps, Dashboard and BIT teams needs to start writing the SQS messages to the new production SQS as was discussed https://cloudtalkio.slack.com/archives/C03FGGA6VQX/p1745311600057759
- Once we migrated to new prod we will delete music sync in the old prod |
| 11:36:04 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

@jiri.missbach We had found many other issues that are blocking us, the processing will take up to 4 hours (pessimistic guess).

Processing is happening but it is very slow |
| 13:12:05 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

We have removed the redis lock and we updated the SQS timeouts. Currently the processing should work without any major issues.

I will have a lunch and I will keep monitoring it |
| 14:08:20 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

processing in old production is done.

New production has 70 more messages to process |
| 14:20:00 | **Fixed at**

Custom timestamp ""Fixed at"" occurred |
| 14:20:13 | **Image posted by Jakub Smadis**

Jakub Smadis posted an image to the channel

We have processed all the messages in the new prod and the incident is no longer active :party-kiwi:

@jiri.missbach you can update the status page
@Tomasz Brach you can mark the incident as solved

I will spend the rest of today creating the tasks and I will create a post mortem too
[link to message](https://cloudtalkio.slack.com/archives/C09KKK6429G/p1760012413662929) |
| 15:17:31 | **Incident resolved and entered the post-incident flow**

Jakub Smadis shared an update
Status: ~~Investigating~~ ‚Üí Documenting |
| 15:47:52 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

Next steps:
- Migrate all applications to use SQS from new production
    ‚ó¶ BIT team  (https://cloudtalk.atlassian.net/browse/BIT-3095) @arnaldo.tema 
    ‚ó¶ Dashboard team (https://cloudtalk.atlassian.net/browse/DSH-6330) @arnaldo.tema 
    ‚ó¶ Apps team  (https://cloudtalk.atlassian.net/browse/APPS-1354) @dino 
    ‚ó¶ cc @Jarda Tomeƒçek FYI, the tasks were deprioritized before
    ‚ó¶ You will be changing it in production, make sure you test if after release since you are not able to test it in dev/stage properly.
- Remove old prod music sync (https://cloudtalk.atlassian.net/browse/VOIP-2292) @Tomasz Brach 
- Fix automation (our deployment failed during the incident) (https://cloudtalk.atlassian.net/browse/VOIP-2293) @Tomasz Brach 
    ‚ó¶ We could not trust the automation and caused a significant deal in fixing the issue
- Remove redis from music sync (https://cloudtalk.atlassian.net/browse/VOIP-2294) @Tomasz Brach 
    ‚ó¶ We have removed it only partially, we need to clean it up |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/1158?tab=post-incident)

- **Old-Dashboard: Change the Media Prod SQS to new production SQS**
    - **Owner:** @Jakub Smadis
    - **JIRA ticket:** [BIT-3095](https://cloudtalk.atlassian.net/browse/BIT-3095)
- **Change the Media Prod SQS to new production SQS**
    - **Owner:** @Jakub Smadis
    - **JIRA ticket:** [DSH-6330](https://cloudtalk.atlassian.net/browse/DSH-6330)
- **[Realtime API]:  Change the Media Prod SQS to new production SQS**
    - **Owner:** @Jakub Smadis
    - **JIRA ticket:** [APPS-1354](https://cloudtalk.atlassian.net/browse/APPS-1354)
- **[Music sync] Remove redis from music sync**
    - **Owner:** Unassigned
    - **JIRA ticket:** [VOIP-2294](https://cloudtalk.atlassian.net/browse/VOIP-2294)
- **[Music sync] Make sure that port automation works properly and we are able to deploy music sync via Port**
    - **Owner:** Unassigned
    - **JIRA ticket:** [VOIP-2293](https://cloudtalk.atlassian.net/browse/VOIP-2293)
- **[Music sync] Remove music sync from old production**
    - **Owner:** Unassigned
    - **JIRA ticket:** [VOIP-2292](https://cloudtalk.atlassian.net/browse/VOIP-2292)
- **[music-sync] If file is not in S3 the processing fails and the service crashes**
    - **Owner:** @Jakub Smadis
    - **JIRA ticket:** [VOIP-2290](https://cloudtalk.atlassian.net/browse/VOIP-2290)
- **[DSH] Playback sound is not played on inbound calls**
    - **Owner:** @daniel malachovsky
    - **JIRA ticket:** [DSH-6313](https://cloudtalk.atlassian.net/browse/DSH-6313)
- **[Music sync] Port automation doesn't work for music sync. It deploys version that is not working**
    - **Owner:** Unassigned
    - **JIRA ticket:** [VOIP-2291](https://cloudtalk.atlassian.net/browse/VOIP-2291)",P2,Music Sync,Asterisk,Voice,,https://www.notion.so/INC-1158-Playback-sound-is-not-played-on-inbound-calls-workarounds-are-not-working-anymore-Deta-2872bccf284f81d7a686f0ce2c04dc06,2025-10-09T07:09:57.736Z
716,inc-716-realtime-ws_realtime-ws-cloudtalk-io-01-04-2025,"*Generated by* @Jakub Smadis *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1158) on* October 9, 2025 4:39 PM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P2

### üë™ Team

- **Incident owner:** @Tomasz Niedziela-Brach
- **Reporter:** @Ji≈ô√≠ Missbach
- **Incident commander:** @Tomasz Niedziela-Brach
- **Active participants:** @Jiri Srba, @Arnaldo Tema, @Ji≈ô√≠ Missbach, @Michal Popoviƒç, @Jaroslav Tomeƒçek, @Filip Prosovsky, @Jakub Smadis, @Tomasz Niedziela-Brach, @daniel malachovsky
- **Observers:** @Laura Misencikova, @Debby Wu, @Lucia Mlyn√°rov√°, @Zoltan Viragh, @Roman Hartig, @Dino Trojak, @Tom√°≈° Sak√°l, @Andr√© Carvalho

### üìù Custom Fields

- **Caused by:** [Music Sync](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDSPVWTTDQNF97F88X8AS)
- **Affected services:** [Asterisk](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JVW3SK537304MXEZV80TMNND)
- **Affected teams:** [Voice](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1XQDA2RQWR0STCA5F)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1158)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C09KKK6429G)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** October 9, 2025 9:09 AM (GMT+2)
- **Resolved at:** October 9, 2025 3:17 PM (GMT+2)
- **Impact started at:** October 4, 2025 7:00 AM (GMT+2)
- **Fixed at:** October 9, 2025 2:20 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 5 days, 7 hours

# üñäÔ∏è Summary

A legacy **music-sync process in old production** was still running and processing uploaded sound messages. Applications and dashboards were **still sending SQS messages to the old production queue**, and the old music-sync was copying and processing them into the new production.

However, the **old Redis used by music-sync was deleted** (not intended to be migrated), causing the process to fail and message processing to stall. Since the old music-sync was scheduled for decommission and lacked proper monitoring, the issue went unnoticed for days.

Customers successfully uploaded the sounds to the dashboard, but the sound was never played during the call.

# Leadup

### **1. Migration to New Production Began**

The organization started migrating workloads from the old production environment to a new one. As part of this effort, a plan was made to eventually decommission the **old music-sync service**, which handles sound-processing tasks and relays messages between systems.

### **2. Assumption: Old music-sync would be removed soon**

Because music-sync in the old production was considered temporary, the team decided **not to maintain full monitoring or alerting** on it. It was believed that this service would be turned off shortly after migration.

### **3. Reality: Old music-sync remained active in production**

However, the decommission never happened. The old music-sync service **continued to run in the background**, still processing sound-related events.

### **4. Applications still publishing to old SQS**

Multiple teams (Apps, Dashboard, BIT) **continued sending SQS messages to the old production queue**.

Even though the new SQS was available, the application migrations were delayed or deprioritized. As a result:

- Messages flowed into the old SQS.
- The old music-sync picked them up.
- It then forwarded them to the new SQS and processed them.

This effectively made the old music-sync a **critical part of the live pipeline.**

### **5. Hidden dependency: Old Redis**

The old music-sync still used the **old production Redis instance** for locking and state tracking.

This dependency was **not documented or tracked**, and was **not communicated to teams performing infrastructure cleanup**.

### **6. Cleanup event: Old Redis was deleted**

As part of migration cleanup, the old Redis instance was removed. The teams performing the cleanup believed no active services were using it.

### **7. Silent failure due to missing monitoring**

Once Redis was removed, the old music-sync began failing to process or forward messages.

However, because monitoring and alerting had been removed (based on the assumption that the service was no longer in use), **no one was notified of the failure**.

### **8. Message backlog silently built up for ~5 days**

For several days (Oct 4 to Oct 9), uploaded sound messages accumulated in the system without being processed.

No automatic alerts were triggered, and no one noticed immediately.

### **9. Incident only discovered when delays became obvious**

On **2025-10-09**, customers recognized that sounds were not played when they tested playback in inbound calls.

# Fault

The core fault was a **hidden, unintended dependency on music-sync in old production** that was still part of the live message-processing pipeline.

Although the organization had migrated to a new production environment, several critical components were **still writing SQS messages to the old production queue**. The old music-sync service would read these messages, process them, and then forward them to the *new* production SQS.

@Jakub Smadis raised the necessity to migrate to new production SQS and delete the old prod music sync in april, but the tasks were deprioritized and none of the 3 teams implemented it.

# Detection

Customers reported the issue to support and support escalated it through L2 to the L3.

# 5 why's analysis

### **1. Why did sound processing stop?**

Because the **music-sync service in old production stopped working**, so messages were no longer being forwarded or processed.

---

### **2. Why did the old music-sync service stop working?**

Because it **depended on old Redis**, which was **unavailable**, causing music-sync to fail.

---

### **3. Why was Redis unavailable while music-sync still depended on it?**

Because the **dependency on old Redis was not documented**, and it was assumed Redis could be safely removed once the old music-sync was no longer needed.

---

### **4. Why did music-sync in old production still rely on old Redis, and why did it impact new production?**

During the migration to new production (until March 2025), the **voice team needed to run music-sync in both old and new environments** to keep messages synced, so the old Redis remained in use.

---

### **5. Why was this dependency not removed after the voice stack migration?**

Because although the team **communicated in April 2025** that all applications should switch to the new production SQS, the related tasks were **created but deprioritized and never implemented**, leaving the old music-sync dependent on legacy Redis.

---

---

# Root cause

The incident occurred due to **legacy dependencies in the old production environment combined with incomplete migration follow-through**:

1. **Old music-sync failure:** The old music-sync service stopped processing messages because it depended on **old Redis**, which became unavailable.
2. **Undocumented dependency:** The dependency on old Redis was **not documented**, and it was assumed Redis could be removed safely once the old music-sync service was no longer needed.
3. **Legacy environment still in use:** During migration to new production (until March 2025), music-sync had to run in **both old and new environments** to keep messages synchronized. This kept old Redis in active use.
4. **Incomplete migration follow-up:** In April 2025, teams communicated that all applications should switch to the **new production SQS**, but the related tasks were **created, deprioritized, and never implemented**, leaving the old music-sync dependent on legacy infrastructure.

# Mitigation and resolution

The infra team enabled a security group that allowed the music-sync service to access the old production Redis.

That enabled the music sync processing, but since we had 5 days of unprocessed messages (362), the old messages contained files that were already deleted. This caused the music sync to spend time on checking if the file exists, this triggered a hidden bug where music sync crashed and was restarted.

Since we used Redis lock and SQS visibility timeouts, the restart caused music sync to be stuck, waiting for redis lock to expire (33 minutes).

We have removed the Redis lock dependency but during the deployment, we found out that port action to deploy the service crashed the service. This created uncertainty in the developers since they couldn‚Äôt trust the automation.

They had to be extra careful to test manual deployments in dev and stage deployments to not cause more harm.

Once they managed to release it to production, they found out that music sync is still crashing because of the bug, and the music sync is stuck again.

This time, music sync was setting a visibility timeout on a message to 10 minutes, which meant that when music sync crashed and was restarted, it had to spend 10 minutes waiting to start processing again.

The team wanted to speed up the processing, so they released (with the manual deployment and double checks) a new version with 60 seconds of visibility timeout (the value was hardcoded).

The team found out that messages are not being processed because the missing file in S3 triggered a retry mechanism that caused the message would be processed in 90 seconds. This caused another issue where music sync was stuck because it couldn‚Äôt process the message.

Team had to release yet another change where they set the visibility timeout to 5 minutes and let the music sync process the messages.

This resulted in a success where the messages were processed in a fast pace.

# Lessons learnt

- Do not let initiatives for clean up die because other teams are not prioritizing it
- Maintain applications over time, so in case of an inciden,t we can react faster
- Music sync needs a lot more love

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-10-04** |  |
| 07:00:00 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 5d later |  |
| **2025-10-09** |  |
| 09:09:57 | **Incident reported by Jiri Missbach**

Jiri Missbach reported the incident
Severity: P2
Status: Investigating |
| 09:10:02 | **Escalated to Platform escalation path**

Your workflow manually escalated the incident to the escalation path [Platform escalation path](https://app.incident.io/cloudtalkio/on-call/escalation-paths/01HYDHY88AJ9FPGQEVNCYAX78Y) |
| 10:27:50 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

Added back rule for legacy prod subnet where music-sync runs to Elasticache so music-sync can connect to old redis |
| 10:50:06 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

Root cause:
- Dashboard and apps are sending SQS messages about uploaded sounds to old production SQS
- Music sync in the old production is still running. It is copying the messages to the new prod SQS and it is still processing the sounds
- Music sync in the old prod doesn‚Äôt have proper monitoring since it was planned to be decommissioned 
    ‚ó¶ I have reached out to the teams in april where we proposed a plan to delete the music sync https://cloudtalkio.slack.com/archives/C03FGGA6VQX/p1745311600057759
- We have forgotten that music sync in old prod still uses old prod redis, it was never meant to be migrated but we were not aware when we were working on music sync auth redis that old redis will be deleted right away
Next steps:
- Thanks to Filip help we are processing the messages
- Remove redis lock to process the messages faster
- Post mortem items:
    ‚ó¶ Apps, Dashboard and BIT teams needs to start writing the SQS messages to the new production SQS as was discussed https://cloudtalkio.slack.com/archives/C03FGGA6VQX/p1745311600057759
- Once we migrated to new prod we will delete music sync in the old prod |
| 11:36:04 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

@jiri.missbach We had found many other issues that are blocking us, the processing will take up to 4 hours (pessimistic guess).

Processing is happening but it is very slow |
| 13:12:05 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

We have removed the redis lock and we updated the SQS timeouts. Currently the processing should work without any major issues.

I will have a lunch and I will keep monitoring it |
| 14:08:20 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

processing in old production is done.

New production has 70 more messages to process |
| 14:20:00 | **Fixed at**

Custom timestamp ""Fixed at"" occurred |
| 14:20:13 | **Image posted by Jakub Smadis**

Jakub Smadis posted an image to the channel

We have processed all the messages in the new prod and the incident is no longer active :party-kiwi:

@jiri.missbach you can update the status page
@Tomasz Brach you can mark the incident as solved

I will spend the rest of today creating the tasks and I will create a post mortem too
[link to message](https://cloudtalkio.slack.com/archives/C09KKK6429G/p1760012413662929) |
| 15:17:31 | **Incident resolved and entered the post-incident flow**

Jakub Smadis shared an update
Status: ~~Investigating~~ ‚Üí Documenting |
| 15:47:52 | **Message from Jakub Smadis**

Jakub Smadis pinned their own message

Next steps:
- Migrate all applications to use SQS from new production
    ‚ó¶ BIT team  (https://cloudtalk.atlassian.net/browse/BIT-3095) @arnaldo.tema 
    ‚ó¶ Dashboard team (https://cloudtalk.atlassian.net/browse/DSH-6330) @arnaldo.tema 
    ‚ó¶ Apps team  (https://cloudtalk.atlassian.net/browse/APPS-1354) @dino 
    ‚ó¶ cc @Jarda Tomeƒçek FYI, the tasks were deprioritized before
    ‚ó¶ You will be changing it in production, make sure you test if after release since you are not able to test it in dev/stage properly.
- Remove old prod music sync (https://cloudtalk.atlassian.net/browse/VOIP-2292) @Tomasz Brach 
- Fix automation (our deployment failed during the incident) (https://cloudtalk.atlassian.net/browse/VOIP-2293) @Tomasz Brach 
    ‚ó¶ We could not trust the automation and caused a significant deal in fixing the issue
- Remove redis from music sync (https://cloudtalk.atlassian.net/browse/VOIP-2294) @Tomasz Brach 
    ‚ó¶ We have removed it only partially, we need to clean it up |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/1158?tab=post-incident)

- **Old-Dashboard: Change the Media Prod SQS to new production SQS**
    - **Owner:** @Jakub Smadis
    - **JIRA ticket:** [BIT-3095](https://cloudtalk.atlassian.net/browse/BIT-3095)
- **Change the Media Prod SQS to new production SQS**
    - **Owner:** @Jakub Smadis
    - **JIRA ticket:** [DSH-6330](https://cloudtalk.atlassian.net/browse/DSH-6330)
- **[Realtime API]:  Change the Media Prod SQS to new production SQS**
    - **Owner:** @Jakub Smadis
    - **JIRA ticket:** [APPS-1354](https://cloudtalk.atlassian.net/browse/APPS-1354)
- **[Music sync] Remove redis from music sync**
    - **Owner:** Unassigned
    - **JIRA ticket:** [VOIP-2294](https://cloudtalk.atlassian.net/browse/VOIP-2294)
- **[Music sync] Make sure that port automation works properly and we are able to deploy music sync via Port**
    - **Owner:** Unassigned
    - **JIRA ticket:** [VOIP-2293](https://cloudtalk.atlassian.net/browse/VOIP-2293)
- **[Music sync] Remove music sync from old production**
    - **Owner:** Unassigned
    - **JIRA ticket:** [VOIP-2292](https://cloudtalk.atlassian.net/browse/VOIP-2292)
- **[music-sync] If file is not in S3 the processing fails and the service crashes**
    - **Owner:** @Jakub Smadis
    - **JIRA ticket:** [VOIP-2290](https://cloudtalk.atlassian.net/browse/VOIP-2290)
- **[DSH] Playback sound is not played on inbound calls**
    - **Owner:** @daniel malachovsky
    - **JIRA ticket:** [DSH-6313](https://cloudtalk.atlassian.net/browse/DSH-6313)
- **[Music sync] Port automation doesn't work for music sync. It deploys version that is not working**
    - **Owner:** Unassigned
    - **JIRA ticket:** [VOIP-2291](https://cloudtalk.atlassian.net/browse/VOIP-2291)
",P3,Infrastructure,Realtime WS,,"Performance, Infrastructure",,2025-04-01T07:43:12.238Z
720,inc-720-asteriskactivecalls150-03-04-2025,"AsteriskActiveCalls150
Asterisk instance asterisk-prod-euc1-0 has too many active calls (currently at 156)
AsteriskActiveCalls150
View in incident.io
Team: voice
Affected instance: asterisk-prod-euc1-0
Prometheus Graph: https://thanos.cloudtalk.io/graph?g0.expr=aster...
Job: asterisk-exporter
Severity: critical
Runbook url: https://github.com/CloudTalk-io/runbooks/blob/m...
Kubernetes cluster: prod-eu-central-1


root cause: caused by yesterday company migration.
https://github.com/CloudTalk-io/db-support-l2/pull/1629/files
Companies have been distributed by number of seats and not number of calls.
Many other asterisk servers have now half load.",P3,,,Voice,Performance,,2025-04-03T08:02:47.881Z
722,inc-722-analytics-calls-arent-being-processed-04-04-2025,"*Generated by* @Jaroslav Tomeƒçek *via [incident.io](https://app.incident.io/cloudtalkio/incidents/722) on* April 7, 2025 9:03 AM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P3

### üë™ Team

- **Incident owner:** @Jiri Srba
- **Reporter:** @Ji≈ô√≠ Missbach
- **Incident commander:** @Jaroslav Tomeƒçek
- **Team member:** @Roman Hartig
- **Active participants:** @Ji≈ô√≠ Missbach, @Zoltan Viragh, @Serhii Shevchenko, @Allison Chihak, @Jaroslav Tomeƒçek, @Roman Hartig, @Jiri Srba, @Jakub Smadis
- **Observers:** @Filip Prosovsky, @Miguel Pecsi, @Jergus Kacmar, @Teodor Lilov, @daniel malachovsky, @Jakub Gawro≈Ñski, @Arnaldo Tema, @Jozef Valko, @Adam Sikora, @Peter Stanko, @Lukas Masar

### üìù Custom Fields

- **Component name:** [Global ‚Üí Analytics](https://app.incident.io/cloudtalkio/catalog/01HYDF5K1N2ZFV4EVGFQKQAXTT/01JMY62JR0YNPA636EPVGEFFCC)
- **Affected teams:** [Voice](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1XQDA2RQWR0STCA5F), [Infrastructure Team](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1WFPMZ3T5GRSCG61J) and [Dashboard Devs](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1DKFB0JJSKCNGRN5K)
- **Affected services:** [Call Information Processing](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRD20SNF2JS61WFAGFAYBM)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/722)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C08LM107JUE)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** April 4, 2025 6:15 PM (GMT+2)
- **Resolved at:** April 7, 2025 8:59 AM (GMT+2)
- **Impact started at:** April 4, 2025 6:15 PM (GMT+2)
- **Identified at:** April 4, 2025 7:23 PM (GMT+2)

# üñäÔ∏è Summary

**Problem:** Clients are experiencing issues with missing calls in Analytics, causing a massive spike in support queues due to a significant drop in call processing, which started around 14:30 on the 4th of April.

**Impact:** Clients are facing disruptions in accessing their call data through Analytics.

**Root cause:** The problem was caused when CIP instances lost contact with AP Redis due to new security groups blocking access, which was a side effect of yesterday's Kamailio redeploys.

**Steps to resolve:** We identified the root cause, adjusted security groups to restore access to AP and SY Redis, and are processing the backlog of calls into Analytics.

# Leadup

CIP lost connection to AP Redis instance ct-prod-ap-redis

# Fault

The CIP deployment was not able to fully start because of missing connection to AP redis. Conection to all redises is a part of application health check

# Detection

Customers experienced issue with missing calls in Analytics app.

# Root cause

The AP redis was decommissioned and not in use. CIP application has a heathcheck connection to check it and was stuck in a loop.

# Mitigation and resolution

The connection to redis was restored.

# Lessons learnt

- create better observability for monitoring on stuck deployment
- remove non-existing voice AP redis connection

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-04-04** |  |
| 18:15:57 | **Incident reported by Jiri Missbach**
‚Äã
Jiri Missbach reported the incident
Severity: P3
Status: Investigating |
| 18:57:29 | **Message from Jirka Srba**
‚Äã
Jirka Srba's message was pinned by Jaroslav Tomeƒçek

yes .. because I restarted the deployment .. but it still fails on errors. At least I see 2 errors to check it

- connection to AP REDIS - not sure whether it is needed but I will check it
- kafka
lot of errors like a
`KafkaJSProtocolError: The group is rebalancing, so a rejoin is needed` |
| 18:58:05 | **Message from Roman Hartig**
‚Äã
Roman Hartig's message was pinned by Jaroslav Tomeƒçek

I've joined the meet so if anyone wanna share some findings, go there üôè |
| 19:02:07 | **Message from Jirka Srba**
‚Äã
Jirka Srba's message was pinned by Jaroslav Tomeƒçek

scaling down the deployment to 2 replicas to help kafka |
| 19:16:16 | **Message from Jaroslav Tomeƒçek**
‚Äã
Jaroslav Tomeƒçek pinned their own message

Update:
- For some reasons CIP lost contact to AP Redis. 
- Each CIP instance is checking the connection. When it is not available, the CIP gets stuck.
- We investigate what happened and whether we can potentially disable access to AP for some time |
| 19:19:02 | **Message from Jaroslav Tomeƒçek**
‚Äã
Jaroslav Tomeƒçek pinned their own message

Update 2:
- Root cause found - as a side effect of yesterdays Kamailio redeploys, new security groups were applied which blocked access to AP and SY redis data. |
| 19:19:46 | **Message from Jaroslav Tomeƒçek**
‚Äã
Jaroslav Tomeƒçek pinned their own message

What is means is that most probably we are missing data from AP and SY since yesterday afternoon. |
| 19:21:26 | **Message from Jaroslav Tomeƒçek**
‚Äã
Jaroslav Tomeƒçek pinned their own message

@Jirka Srba is manually fixing it, the services will catch up then |
| 19:23:03 | **Status changed from Investigating ‚Üí Identified**
‚Äã
Jaroslav Tomeƒçek shared an update
Status: ~~Investigating~~ ‚Üí Identified |
| 19:24:15 | **Incident paused**
‚Äã
Jaroslav Tomeƒçek shared an update
Status: ~~Identified~~ ‚Üí Paused |
| 19:26:09 | **Incident resumed**
‚Äã
Allison Chihak shared an update
Status: ~~Paused~~ ‚Üí Investigating
Dev team has identified root cause and is fixing it. Affected services will need time to catch up with call processing into Analytics.

**Identification**

‚Ä¢ CIP lost contact to AP Redis.

‚Ä¢ Each CIP instance is checking the connection. When it is not available, the CIP gets stuck.
‚Ä¢ As a side effect of yesterday's Kamailio redeploys, new security groups were applied which blocked access to AP and SY redis data.
‚Ä¢  |
| 19:26:31 | **Incident paused**
‚Äã
Allison Chihak shared an update
Status: ~~Investigating~~ ‚Üí Paused |
| 19:31:38 | **Message from Jirka Srba**
‚Äã
Jirka Srba pinned their own message

yes, I assign correct Security Groups to redis instances in APSE1 and APSE2 regions |
| 20:26:31 | **Incident resumed**
‚Äã
incident.io shared an update
Status: ~~Paused~~ ‚Üí Investigating |
| 20:34:27 | **Incident paused**
‚Äã
Allison Chihak shared an update
Status: ~~Investigating~~ ‚Üí Paused

‚Ä¢ Calls continuing to process normally in [logs](https://app.datadoghq.eu/logs?query=service%3Acall-information-processing&agg_m=count&agg_m_source=base&agg_q=status&agg_q_source=base&agg_t=count&clustering_pattern_field_path=message&cols=host%2Cservice&fromUser=true&messageDisplay=inline&refresh_mode=sliding&sort_m=count&sort_m_source=base&sort_t=count&step=600000&storage=hot&stream_sort=desc&top_n=1000&top_o=top&viz=timeseries&x_missing=true&from_ts=1743776893381&start=1743785202458&end=1743788802458&to_ts=1743791293381&live=true&paused=false). No new instances of reported error.
‚Ä¢ Calls from affected time period continue to gradually process into Analytics. |
| **2025-04-05** |  |
| 13:23:08 | **Update shared**
‚Äã
Allison Chihak shared an update
Backlog log of unprocessed calls appears to have completed and service is running normally based on [logs](https://app.datadoghq.eu/logs?query=service%3Acall-information-processing&agg_m=count&agg_m_source=base&agg_q=status&agg_q_source=base&agg_t=count&clustering_pattern_field_path=message&cols=host%2Cservice&fromUser=true&messageDisplay=inline&refresh_mode=sliding&sort_m=count&sort_m_source=base&sort_t=count&step=600000&storage=hot&stream_sort=desc&top_n=1000&top_o=top&viz=timeseries&x_missing=true&from_ts=1743765571448&start=1743785202458&end=1743788802458&to_ts=1743851971448&live=true&paused=false). |
| 2d later |  |
| **2025-04-07** |  |
| 08:59:44 | **Incident resumed**
‚Äã
Jaroslav Tomeƒçek shared an update
Status: ~~Paused~~ ‚Üí Documenting |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/722?tab=follow-ups)

- **[CIP] Remove connection to the redis in old production**
    - **Owner:** @Jakub Smadis
    - **JIRA ticket:** [DSH-5491](https://cloudtalk.atlassian.net/browse/DSH-5491)
",P3,Kamailio,Call Information Processing,"Voice, Infrastructure Team, Dashboard Devs","Release problems, Human error, Architecture weakness, Data loss, Insufficient testing",https://www.notion.so/INC-722-Analytics-Calls-Arent-Being-Processed-1ce2bccf284f815d8823ee5ce8f95453,2025-04-04T16:15:57.476Z
829,inc-829-asteriskactivecalls200-01-05-2025,"Customer 305009 set business hours on CT number to be redirected to the same CT number which created look and overloaded server.
This issue has been already reported to dashboard team. I will check with them if they are going to fix this issue by blocking of using same numbers.


how the customers were affected during the incident?
What is the impact of overloaded server on other companies?

Only 251733 may had been affected by asterisk overload. Company may experienced bad audio quality or problem with dialing. Support is checking them.

But any company that used that asterisk during the incident was affected no?  Because we had more than 200 calls

According to kibana there were few other companies made just few calls during 30 minutes time. Only 251733 made 80 calls and it is high MRR customer so I decided to contact only them.


I wanted to just know the impact, we don‚Äôt need to contact the customers.
The impact is necessary to understand the implication and why we need to fix it sooner :slightly_smiling_face:

--- how it was solved etc ----

Saving the form with a number identical to the one being edited results in a backend error ‚ÄúRedirect to same number is not allowed‚Äù

Screenshot attached 

Automated test added here - https://github.com/CloudTalk-io/dashboard-frontend-monorepo/pull/3129Connect your Github account 







Jergus Kacmar

June 17, 2025 at 1:55 PM
(edited)
Steps to test:

In the number form set redirect to the same number as the one that is being edited - this notifies you about not being able to redirect to itself

Test a deep loop error - From number1 redirect to number2 and then from number2 try setting redirect to number1 - should work even if you create long logical loop out of many numbers

Note: I made this validation to disregard number extension







Jergus Kacmar

June 17, 2025 at 1:52 PM
End of sprint report:

My local tests were successful in restricting creating of loops through out of business hours number‚Äôs settings

PRs are up for review



thumbs up
1




Jergus Kacmar

May 6, 2025 at 11:03 AM
Grooming:

Redirection to a number in the same company should be allowed but conditions need to be deeply checked to prevent looping back to original number

No measures to be added for loops caused by cfd







Jergus Kacmar

May 5, 2025 at 4:15 PM
(edited)
ED:

to mutation updateCallNumber and updateBusinessHours add a backend check that will throw error when redirect is to a number assigned to company calling these mutations

then add translation strings on fe for this error







Arnaldo Tema

May 2, 2025 at 10:07 AM
Issue happened again, re-opening it.

",P3,Asterisk,Asterisk,Voice,Architecture weakness,,2025-05-01T21:21:47.739Z
857,inc-857-callanalyzervisiblemessages500-20-05-2025,"*Generated by* @Roman Hartig *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1128) on* October 1, 2025 3:48 PM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P3

### üë™ Team

- **Incident owner:** @Elena Darriba
- **Reporter:** @Zoltan Viragh
- **Active participants:** @Elena Darriba, @Zoltan Viragh, @Roman Hartig, @Filip Prosovsky, @Arnaldo Tema
- **Observers:** @Serhii Shevchenko, @Debby Wu, @Jakub Bober

### üìù Custom Fields

- **Caused by:** [Dashboard Backend](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDF811D4F519DG66R632Q)
- **Affected services:** [Dashboard Frontend](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDG4Q3EZQF5XHJNKM8HSD) and [Dashboard Backend](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDF811D4F519DG66R632Q)
- **Affected teams:** [Dashboard Devs](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1DKFB0JJSKCNGRN5K)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1128)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C09HKNCK5R9)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** September 29, 2025 7:41 PM (GMT+2)
- **Accepted at:** September 29, 2025 7:45 PM (GMT+2)
- **Resolved at:** September 29, 2025 8:28 PM (GMT+2)
- **Impact started at:** September 29, 2025 4:00 PM (GMT+2)
- **Identified at:** September 29, 2025 8:14 PM (GMT+2)
- **Fixed at:** September 29, 2025 8:18 PM (GMT+2)
- **Closed at:** September 30, 2025 4:24 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 4 hours, 17 minutes

# üñäÔ∏è Summary

**Problem**: The Integrations section of the new dashboard failed to load, displaying a network error message to users.

**Impact**: The Integrations section of the new dashboard would not load for all users, showing a network error. The issue affected all customers using this dashboard section.

**Root cause**: Frontend changes were released that depended on backend updates which had not been successfully deployed. The backend release failed, so the frontend expected new data that was not available, resulting in errors.

**Steps to resolve**: We re-ran the backend release pipeline to create and deploy the missing version 170, restoring compatibility between frontend and backend.

# Fault

`dashboard-backend`  github action generating docker image with service failed

# Detection

Customer raised a support ticket.

# 5 why's analysis

Why do we haven‚Äôt got notification earlier ‚áí there are no alerts for PODs on prod in pending state (see action items)

Why we have to do manual releases as we have already Port driven releases with success/failure notifications ‚áí those are pending stories in DSH backlog (see action items) 

# Root cause

Frontend changes were released that depended on backend updates which had not been successfully deployed. The backend release failed, so the frontend expected new data that was not available, resulting in errors.

# Mitigation and resolution

We re-ran the backend release pipeline to create and deploy the missing version 170, restoring compatibility between frontend and backend.

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-09-29** |  |
| 16:00:38 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 3h later |  |
| 19:41:38 | **Incident reported by Zolt√°n Ill√©s**

Zolt√°n Ill√©s reported the incident
Severity: P3
Status: Investigating |
| 19:41:43 | **Escalated to Platform escalation path**

Your workflow manually escalated the incident to the escalation path [Platform escalation path](https://app.incident.io/cloudtalkio/on-call/escalation-paths/01HYDHY88AJ9FPGQEVNCYAX78Y) |
| 19:53:35 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

Seems like failed deployment and bump of image:
https://github.com/CloudTalk-io/dashboard-backend/actions/runs/18092090781
https://github.com/CloudTalk-io/argocd-kube/commit/3e5f61d04ac6ab85a200fea368592d49daf8f900 |
| 19:58:00 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

However, this does not seem like the root cause, this is something Kubernetes can handle and will not delete wrong replicas or add the ones with the wrong image until they are ready, healthy to serve traffic |
| 19:58:19 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message*<!subteam^S08SC236HPB>  This is from the previous pod (restarted one)
```{""level"":""error"",""time"":1759147983462,""pid"":1,""hostname"":""dashboard-backend-api-5cf5c758fd-wtmkz"",""req"":{""id"":""req-1mx6"",""method"":""GET"",""url"":""/health"",""query"":{},""headers"":{""host"":""10.150.191.123:3000"",""user-agent"":""kube-probe/1.33+"",""accept"":""*/*"",""connection"":""close""},""remoteAddress"":""10.150.170.240"",""remotePort"":47898},""dd"":{""trace_id"":""5794663285581314548"",""span_id"":""5794663285581314548"",""service"":""dashboard-backend"",""version"":""v0.169.0"",""env"":""prod""},""moduleName"":""ExceptionFilter"",""exceptionWithMetadata"":{""requestMetadata"":{""reqId"":""req-1mx6"",""url"":""/health""},""requestTags"":{""nodeVersion"":""v20.18.1"",""requestType"":""http"",""httpMethod"":""GET""},""requestUser"":{},""err"":{""message"":""connect ECONNREFUSED 0.0.0.0:3000"",""name"":""Error"",""stack"":""Error: connect ECONNREFUSED 0.0.0.0:3000\n    at AxiosError.from (/usr/src/app/node_modules/axios/dist/node/axios.cjs:857:14)\n    at RedirectableRequest.handleRequestError (/usr/src/app/node_modules/axios/dist/node/axios.cjs:3169:25)\n    at RedirectableRequest.emit (node:events:518:28)\n    at eventHandlers.&lt;computed&gt; (/usr/src/app/node_modules/follow-redirects/index.js:49:24)\n    at ClientRequest.emit (node:events:518:28)\n    at req.emit (/usr/src/app/node_modules/dd-trace/packages/datadog-instrumentations/src/http/client.js:97:25)\n    at emitErrorEvent (node:_http_client:101:11)\n    at Socket.socketErrorListener (node:_http_client:504:5)\n    at Socket.emit (node:events:530:35)\n    at Socket.emit (/usr/src/app/node_modules/dd-trace/packages/datadog-instrumentations/src/net.js:61:25)\n    at Axios.request (/usr/src/app/node_modules/axios/dist/node/axios.cjs:4258:41)\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at async Axios.request (/usr/src/app/node_modules/axios/dist/node/axios.cjs:4253:14)\n    at async Axios.request (/usr/...* |
| 20:05:24 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

Maybe some changes on the frontend today? https://github.com/CloudTalk-io/dashboard-frontend-monorepo/commits/main/ |
| 20:06:01 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

@Harry Can it be related that changes added to the frontend are fine, but as 170 tag was not built and released, it fails now? |
| 20:10:15 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

Re-running dashboard release to 170 |
| 20:14:00 | **Identified at**

Custom timestamp ""Identified at"" occurred |
| 20:16:11 | **Message from Roman Hartig**

Roman Hartig's message was pinned by Filip Pro≈°ovsk√Ω

```
GraphQLError: Cannot query field ""logTickets"" on type ""ZendeskConfig"". Did you mean ""importTickets"", ""logMissed"", or ""logMissedAs""?
```

this is the main issue - dd (https://app.datadoghq.eu/apm/traces?query=env%3Aprod%20service%3Adashboard-backend%20status%3Aerror%20%40error.message%3A%22Cannot%20query%20field%20%5C%22logTickets%5C%22%20on%20type%20%5C%22ZendeskConfig%5C%22.%20Did%20you%20mean%20%5C%22importTickets%5C%22%2C%20%5C%22logMissed%5C%22%2C%20or%20%5C%22logMissedAs%5C%22%3F%22&agg_m=count&agg_m_source=base&agg_t=count&cols=core_service%2Ccore_resource_name%2Clog_duration%2Clog_http.method%2Clog_http.status_code&fromUser=false&graphType=waterfall&historicalData=true&messageDisplay=inline&query_translation_version=v0&refresh_mode=sliding&shouldShowLegend=true&sort=desc&spanID=2886556523556202347&spanType=all&storage=hot&timeHint=1759169676112&trace=AwAAAZmWrwNQO91s-gAAABhBWm1Xcnd0aUFBQk5ydEFYY0w1SkJWdy0AAAAkZjE5OTk2YWYtMWU5NC00MGI5LWE1NjItMGYyZjliMzUzMGVmAAAAZA&traceID=4493717891091403425&traceQuery=&view=spans&start=1759083338715&end=1759169738715&paused=false) -> looking more into it |
| 20:16:32 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

@Harry then it is related to changes I sent above if that is the main error |
| 20:18:00 | **Fixed at**

Custom timestamp ""Fixed at"" occurred |
| 20:26:53 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

Summary:
- Changes released on frontend (v1.161.0 (https://github.com/CloudTalk-io/dashboard-frontend-monorepo/releases/tag/dashboard-v1.161.0)) without properly released backend caused the issue
1. Failed release of dashboard-backend
    a. This issue was handled by infrastructure and Kubernetes did not allow not existing image to be deployed, keeping the app (version 169) up
    b. However, as frontend was released, it was expected from backend to receive some data (introduced in version 170)
        i. As the app version was older, this caused error on the frontend side
Recovery: Rerun the release pipeline to create 170 app version
Root cause:
- If app would be on unified release process, failed backend deployment would be received as slack notif and team could handle the errors sooner.
- Did not verified functionality after release
- Version bumped before checked it really exists and is functional |
| 20:28:57 | **Incident resolved and entered the post-incident flow**

Filip Pro≈°ovsk√Ω shared an update
Status: ~~Investigating~~ ‚Üí Documenting |
| 20:31:38 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

I also see failed deployments on frontend, so actual release of the changes and impact was approx from 4pm today (cc @Harry):
https://github.com/CloudTalk-io/dashboard-frontend-monorepo/actions/workflows/prod-build.yml |
| **2025-09-30** |  |
| 16:24:51 | **Opted out of post-incident flow**

Elena Darriba shared an update
Status: ~~Documenting~~ ‚Üí Closed |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/1128?tab=post-incident)

- **[DSH] Integrations Wont Load**
    - **Owner:** Unassigned
    - **JIRA ticket:** [DSH-6245](https://cloudtalk.atlassian.net/browse/DSH-6245)
- https://cloudtalk.atlassian.net/browse/DSH-4963
- https://cloudtalk.atlassian.net/browse/DSH-5018
- https://cloudtalk.atlassian.net/browse/DSH-6277",P3,Dbcache,CT Call Analyzer,AI Team,"Performance, Infrastructure",,2025-05-20T14:08:49.616Z
860,inc-860-rtpenginefreeports250-26-05-2025,"RCA Summary - INC-860 (rtpengine free ports issue, 26-05-2025)

Root Cause:
The incident was most likely caused by overloading of kamailio-trunks, the SIP (Session Initiation Protocol) proxy responsible for managing trunk traffic. Just before the incident, the number of active transactions spiked from the usual ~50 to ~80, which may have filled internal module memory registers or otherwise stressed Kamailio beyond capacity. There was no corresponding spike in CPS (calls per second) across the system, suggesting the overload was localized to Kamailio‚Äôs internal handling rather than a systemic traffic surge.

Ruled Out Causes:
	‚Ä¢	SQL connection issues ‚Äì investigated and eliminated.
	‚Ä¢	Broader traffic anomalies ‚Äì no system-wide CPS peak detected.

Resolution & Follow-ups:
	1.	Migration: Move all inbound carrier connections to the new production infrastructure (VOIP-1773) to improve load distribution and resilience.
	2.	Observability: Improve Kamailio-trunks logging for better future diagnostics (Kamailio-trunks - logs refactor).",P3,Kamailio,RTPengine,Voice,"Infrastructure, Architecture weakness",,2025-05-26T15:28:45.285Z
863,inc-863-dashboard-api_dashboard-api-cloudtalk-io-02-06-2025,"*Generated by* @Arnaldo Tema *via [incident.io](https://app.incident.io/cloudtalkio/incidents/863) on* June 6, 2025 3:01 PM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P3

### üë™ Team

- **Incident owner:** @Arnaldo Tema
- **Active participants:** @Ji≈ô√≠ Missbach, @Filip Prosovsky, @Jiri Srba, @Arnaldo Tema, @Roman Hartig
- **Observers:** @Hugo Benavides, @Serhii Shevchenko

### üìù Custom Fields

- **Caused by:** [Dashboard Backend](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDF811D4F519DG66R632Q)
- **Affected services:** [Dashboard Backend](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDF811D4F519DG66R632Q)
- **Affected teams:** [Dashboard Devs](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1DKFB0JJSKCNGRN5K)
- **Component name:** [Global ‚Üí CloudTalk Dashboard](https://app.incident.io/cloudtalkio/catalog/01HYDF5K1N2ZFV4EVGFQKQAXTT/01JMY62JR080JARJS4RP3HWVZH)
- **Data breach:** False

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/863)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C08V96VBFFB)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** June 2, 2025 1:32 AM (GMT+2)
- **Accepted at:** June 2, 2025 1:44 AM (GMT+2)
- **Resolved at:** June 2, 2025 1:44 AM (GMT+2)
- **Impact started at:** June 2, 2025 1:32 AM (GMT+2)
- **Identified at:** January 1, 1970 1:00 AM
- **Fixed at:** January 1, 1970 1:00 AM
- **Documented at:** January 1, 1970 1:00 AM
- **Reviewed at:** January 1, 1970 1:00 AM
- **Closed at:** January 1, 1970 1:00 AM

# üñäÔ∏è Summary

[dashboard-api.cloudtalk.io](http://dashboard-api.cloudtalk.io/) had multiple backend pod crashes due to excessive memory usage (OOM). Investigation showed a single customer (ID 161817) was triggering an inefficient greeting processing path. Their account contained ~16,300 greetings, representing about 1/3 of the total prod greetings.

The greeting loading logic performed Promise.all on thousands of S3 existence checks simultaneously, causing a memory spike that led to pod crashes. The root cause was traced to old, unoptimized code not designed to handle this scale.

# Leadup

No recent deployments or configuration changes were made. The issue was triggered by a user accessing the ‚ÄúNumber Details‚Äù page in the Dashboard. The system tried to load all greetings for the account at once, leading to memory exhaustion.

# Fault

The greeting loader logic performed uncontrolled concurrent S3 requests (1 request per greeting) using Promise.all, which crashed the Node.js process due to excessive memory and open connections.

# Detection

Incident was auto-detected by health checks and reported as a 503 error. Jirka restarted the pods and temporarily removed the liveness probe. Harry later pinpointed a correlation with CallNumber GraphQL requests. Heap size was increased, but it didn‚Äôt help. Final root cause was confirmed by @Serhii Shevchenko through logs and code inspection

# 5 why's analysis

**Why was the service unhealthy?**
‚Üí Because the pod crashed with OOM.

**Why did the pod run out of memory?**
‚Üí Because it was handling thousands of concurrent S3 requests.

**Why were there thousands of S3 requests?**
‚Üí Because each greeting triggered an S3 file existence check.

**Why was that not limited or batched?**
‚Üí The code used Promise.all on all items without batching.

**Why was that logic in place?**
‚Üí Legacy code had no concurrency limits or safeguards for high-volume accounts.

# Root cause

Inefficient greeting loader logic performing unbounded concurrent S3 file checks, triggered by a high-volume customer.

# Mitigation and resolution

- Increased memory and heap size (temporary workaround).
- Rewrote greeting and number file check loaders to process in **batches** of 10 (configurable).
- Added test coverage for batching behavior.

# Lessons learnt

- Legacy code can‚Äôt be trusted to scale. Needs regular audits and limits
- Large customer data profiles can introduce critical edge cases.
- Batching external calls (S3, DB, etc.) should be a default practice in all high-load services

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-06-02** |  |
| 01:32:02 | **Incident reported in triage by Cloudflare alert**
‚Äã
Cloudflare alert reported the incident
Severity: None
Status: Triage |
| 01:44:05 | **Incident accepted**
‚Äã
Jirka Srba shared an update
Severity: ~~None~~ ‚Üí P3
Status: ~~Triage~~ ‚Üí Investigating |
| 01:44:19 | **Incident resolved and entered the post-incident flow**
‚Äã
Jirka Srba shared an update
Status: ~~Investigating~~ ‚Üí Documenting |
| 01:45:37 | **Message from Serhii Shevchenko**
‚Äã
Serhii Shevchenko's message was pinned by Jirka Srba

@Serhii Shevchenko has joined the channel |
| 02:01:36 | **Message from Jirka Srba**
‚Äã
Jirka Srba pinned their own message

temporary removed liveness probe of deployment `dashboard-backend-api` |
| 02:01:44 | **Message from Jirka Srba**
‚Äã
Jirka Srba pinned their own message

and increased memory limit to 4 Gi |
| 2d later |  |
| **2025-06-04** |  |
| 09:50:03 | **Message from Roman Hartig**
‚Äã
Roman Hartig's message was pinned by Jirka Srba

Update:
- as part of INFRA-2568 node heap size was also increased to 4G so we should be fine now (previous size was 2G)
- logs show correlation (https://app.datadoghq.eu/logs?agg_m_a=count&agg_m_b=count&agg_m_source_a=base&agg_m_source_b=base&agg_t_a=count&agg_t_b=count&clustering_pattern_field_path_a=message&clustering_pattern_field_path_b=message&cols=host%2Cservice&ff_display=a%2Cb&messageDisplay=inline&query_a=env%3Aprod%20service%3Adashboard-backend%20%40operationType%3ACallNumber%20%40metadata.requestUser.userId%3A123777&query_b=env%3Aprod%20service%3Adashboard-backend%20FATAL&refresh_mode=sliding&storage=hot&stream_sort=desc&viz=timeseries&from_ts=1748762369374&to_ts=1749021569374&live=true) with `GraphQl CallNumber` request as the last one before `dashboard-backend` POD crashes on OOM
    ‚ó¶ this query is generated when going to number details page (ie https://dashboard.cloudtalk.io/menu/numbers/edit/225827)
        ‚ñ™Ô∏é I'm not able to reproduce using impersonation
        ‚ñ™Ô∏é happening just for `companyId: 161817, userId: 123777`
            - I would propose to ask support to contact this company and check if they are having issues with our dashboard particularly with Numbers edit page (doesn't need to be exactly this page but some following action which could be eventually missing in our logs because of the crash) -> wdyt @arnaldo.tema  |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/863?tab=follow-ups)

- **Increase dashboard-backend memory pod limit to 4Gi**
    - **Owner:** @Jiri Srba
    - **JIRA ticket:** [INFRA-2568](https://cloudtalk.atlassian.net/browse/INFRA-2568)
- Add batching (default 10 at a time) for S3 existence checks in both greetings and call number data loaders, limiting concurrency, reducing memory pressure, and preventing OOM.",P3,Dashboard Backend,Dashboard Backend,Dashboard Devs,"Insufficient testing, Architecture weakness, Regression",https://www.notion.so/INC-863-Dashboard-API_dashboard-api-cloudtalk-io-20a2bccf284f81c7964ecb2690c2f24e,2025-06-01T23:32:02.159Z
869,inc-869-dbcache-did-not-process-asterisk_cdr-29-5-2025-03-06-2025,"RCA Summary - INC-869 (DBCache did not process asterisk_cdr, 29-05-2025)

Root Cause:
The incident was triggered by DBCache failing to process asterisk_cdr (Call Detail Records) into the main database. This prevented some call logs from being inserted and made them temporarily unavailable. The most probable cause was a database-level limitation or retry failure in the DBCache pipeline, which prevented successful inserts during a period of high activity.

Immediate Impact:
	‚Ä¢	Missing or delayed call logs in Analytics (AN) due to unprocessed CDRs.
	‚Ä¢	No data corruption; logs were recoverable from Redis backups.

Mitigation & Recovery:
	‚Ä¢	Restored missing call data from Redis backups via documented recovery procedure (handled by Roman Hartig).
	‚Ä¢	Raised DB limit max_prepared_stmt_count to 40k to prevent insert saturation (done by Jirka Srba).
	‚Ä¢	Follow-up fixes:
	1.	Add alerting to trigger on-call when asterisk_cdr records fail to insert.
	2.	Improve retry strategy for asterisk_cdr and queue log tables.
(Both assigned to Fugo.)",P3,Dbcache,"Dbcache, Call Information Processing",Voice,"Human error, Missing proactive alerting, Data loss",,2025-06-03T15:36:50.164Z
890,inc-890-automated-calls-were-created-without-uuid-05-06-2025,"RCA Summary - INC-890 (Automated calls created without UUID, 05-06-2025)

Root Cause:
A subset of automated calls was created without a valid UUID (universally unique identifier), causing related asterisk_cdr (Call Detail Records) to be missing proper linkage and preventing their call recordings from being processed. The issue originated from a temporary inconsistency in DBCache write logic, which failed to assign UUIDs during automated call creation.

Impact:
	‚Ä¢	Call recordings for affected calls were not initially processed or linked to the correct CDRs.
	‚Ä¢	No permanent data loss; all affected records were recoverable.

Mitigation & Recovery:
	‚Ä¢	All impacted asterisk_cdr records were successfully updated with correct UUIDs.
	‚Ä¢	Call recordings were reprocessed over the next four hours, fully restoring data integrity.
	‚Ä¢	Temporary fix was applied and later disabled via VOIP-1994 once all data was verified and consistent.

Status:
	‚Ä¢	Incident resolved and verified by Jakub Smadis.
	‚Ä¢	Follow-up (‚ÄúFix missing call recordings based on incident INC-890‚Äù) completed by Fugo.
	‚Ä¢	No remaining technical debt; incident closed after successful post-incident validation",P3,,"Dbcache, Asterisk",Voice,"Regression, Human error, Missing proactive alerting, Insufficient testing, Data loss",,2025-06-05T08:14:36.702Z
896,inc-896-hosthighcpuload95-10-06-2025,"RCA Summary - INC-896 (Host High CPU Load 95%, 10-06-2025)

Root Cause:
The incident was caused by a bug in Asterisk‚Äôs ChanSpy module, which kept channels alive indefinitely when no bridge event occurred. This behavior stemmed from Asterisk‚Äôs configuration to ‚Äúspy only on channels involved in a bridged call.‚Äù When a call failed to bridge, ChanSpy waited indefinitely for a non-existent event, leaving orphaned processes active.
Given that CloudTalk‚Äôs Asterisk version is 8 years past end-of-life, the issue likely arises from an unpatched defect in legacy code.

Impact:
	‚Ä¢	Orphaned streaming sessions accumulated on Asterisk, streaming proxies, and VDS (Voice Delivery Service).
	‚Ä¢	Resulted in high CPU utilization (95%) across affected hosts.
	‚Ä¢	Performance degradation risked service instability under higher load.

Mitigation & Fix:
	‚Ä¢	A temporary safeguard was released to automatically terminate streaming sessions after 90 seconds, preventing infinite loops.
	‚Ä¢	Investigation showed that certain parallel dialer scenarios (involving missing X-CT-Parallel-Dialer headers) could also trigger similar non-bridged states, though the bug wasn‚Äôt reproducible in testing.
	‚Ä¢	Long-term fixes planned:
	‚Ä¢	VDS enhancement to kill unbridged call streams when a call is canceled.
	‚Ä¢	Streaming proxy timeout refinement via VOIP-1998, potentially reducing the 90s limit further after stability validation.

Status:
	‚Ä¢	Temporary fix deployed and verified effective.
	‚Ä¢	Follow-up task (‚ÄúChanSpy running infinite loop if call doesn‚Äôt bridge during VD feature‚Äù) completed.
	‚Ä¢	Incident resolved and closed after system stability confirmed.",P3,Phone,Asterisk,Voice,"Performance, Insufficient testing",,2025-06-10T11:43:47.389Z
898,inc-898-possible-problem-with-call-registrations-10-06-2025,"RCA Summary - INC-898 (Possible Problem with Call Registrations, 10-06-2025)

Root Cause:
A bug in the phone application triggered infinite reconnection attempts during call registration. This caused registration instability on both production and end-to-end (E2E) environments, particularly affecting users running an older or E2E-specific app version. The issue was environment-specific ‚Äî not reproducible on Dev ‚Äî due to version mismatches and behavior differences between environments.

Impact:
	‚Ä¢	Hundreds of users temporarily experienced failed call registrations or delays in connecting.
	‚Ä¢	Service degradation classified as minor (P3) after reassessment, as reloading the app restored functionality.

Mitigation & Fix:
	‚Ä¢	Temporary workaround: users instructed to reload the app to recover registration.
	‚Ä¢	Permanent fix implemented in the CloudTalk-io/phone repository (PR: fix(TRIVIAL, INC-898): infinite reconnection attempts) and released as version 7.84.2.
	‚Ä¢	Corresponding SIP service release 3.2.0 deployed to stabilize connection handling.

Status:
	‚Ä¢	Incident severity briefly raised to P2 but downgraded to P3 once impact and workaround were confirmed.
	‚Ä¢	Fix merged and deployed; registrations returned to normal.
	‚Ä¢	Incident closed and archived after post-fix validation.",P3,Phone,"Desktop, Phone",App Devs,"Insufficient testing, Missing proactive alerting, Regression, Reported by customers before we learned",,2025-06-10T21:13:32.134Z
923,inc-923-call-transcribe-is-not-processing-non-english-calls-19-06-2025,"*Generated by* @Du≈°an Argal√°≈° *via [incident.io](https://app.incident.io/cloudtalkio/incidents/923) on* June 23, 2025 9:16 AM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P3

### üë™ Team

- **Incident owner:** @Du≈°an Argal√°≈°
- **Reporter:** @Du≈°an Argal√°≈°
- **Active participants:** @Du≈°an Argal√°≈°
- **Observers:** incident

### üìù Custom Fields

- **Affected services:** [Call Transcribe](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRD3BQXPK79CN61732NJFB)
- **Affected teams:** [AI Team](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1GS8XEFEM2EXF9CN2)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/923)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C092L0M19PT)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** June 18, 2025 3:52 PM (GMT+2)
- **Resolved at:** June 19, 2025 10:30 AM (GMT+2)
- **Impact started at:** June 17, 2025 3:33 PM (GMT+2)

- **Identified at:** June 17, 2025 3:50 PM (GMT+2)
- **Fixed at:** June 19, 2025 8:55 AM (GMT+2)
- **Documented at:** June 19, 2025 3:55 PM (GMT+2)
- **Closed at:** June 19, 2025 4:00 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 1 days, 17 hours

# üñäÔ∏è Summary

The Call Transcribe service stopped processing **non-English calls** following the deployment of version `v0.2.10`, due to an **incompatible dependency conflict** between `whisperx` and a pinned `torch` version. This caused approximately **6,000 calls out of 55,000 total** in the affected period to remain **untranscribed**.

# Leadup

The issue began after deploying version `v0.2.10` of the `call-transcribe` service. As part of our dependency management strategy, the service allows automatic upgrades of sub-dependencies, such as `whisperx`. However, `torch`, a core dependency, was **explicitly pinned to a maximum version of 2.5**.

- In the new `whisperx` release, **support for `torch<=2.5` was dropped**.
- When the updated `whisperx` was pulled in automatically, it expected a newer version of `torch`, resulting in **model loading failures for non-English languages**.
- These failures were not caught during pre-release testing because **only English-language cases were manually verified**.

This incompatibility silently broke multilingual processing while leaving English call transcriptions unaffected, masking the issue during the release process.

# Fault

The root fault was a **dependency conflict**:

- `whisperx` was upgraded to a version incompatible with the explicitly locked version `torch<=2.5`.
- The conflict caused runtime errors when loading non-English transcription models.

# Detection

The issue was **first detected through Datadog error logs** within hours of the release. A `call-transcribe` alert began **triggering intermittently**, but its signal was weak and classified as noise due to **ongoing alert tuning**.

- Initial errors were visible in Datadog logs tagged with `error processing message`.
- The alert was overlooked as the service continued processing English calls without degradation.
- Only after a **support ticket surfaced**, and **error rates spiked to 10%**, was the issue prioritized.
- A deeper investigation confirmed the source of the error was model-loading logic for multilingual transcription.

# 5 why's analysis

**Why¬†#1: Why did Call Transcribe¬†stop processing non-English calls?**

**Answer:**¬†Because there was a dependency¬†conflict between¬†whisperx¬†and¬†torch¬†- the new version of¬†whisperx¬†was incompatible with the¬†pinned¬†torch<=2.5¬†version, causing model¬†loading failures for non-English languages.

**Why #2: Why did this¬†dependency conflict occur?**

**Answer:**¬†Because¬†whisperx¬†was allowed to auto-upgrade while¬†torch¬†was¬†explicitly locked to version 2.5 maximum. The newer¬†whisperx¬†dropped support for¬†torch<=2.5, creating an incompatible dependency state.

**Why #3: Why wasn't this incompatibility caught before¬†the release?**

**Answer:**¬†Because¬†there was no automated testing coverage¬†for non-English transcription scenarios, and manual QA only verified English-language cases. The bug¬†was invisible during pre-release testing.

**Why #4: Why¬†was there no QA coverage for multilingual transcription?**

**Answer:**¬†Because¬†the testing strategy was incomplete - it¬†focused on the¬†primary use¬†case (English) without considering the full¬†functional scope of the service. There was no systematic¬†approach to ensure all supported languages were tested.

**Why¬†#5: Why wasn't the issue¬†detected and resolved quickly after deployment?**

**Answer:**¬†Because the alerting system was undergoing calibration and the initial alerts¬†were dismissed as noise rather than investigated. There was insufficient confidence¬†in the alert thresholds and no¬†clear escalation process for investigating new¬†alert patterns.

# Root cause

The incident was caused by a **dependency incompatibility**:

1. **Explicit dependency pinning** to `torch<=2.5` in `call-transcribe`.
2. **Automatic upgrade of `whisperx`**, which internally deprecated support for `torch<=2.5`.
3. **Lack of QA coverage** for non-English transcriptions allowed the issue to reach production undetected.

## **Systemic Issues**

The analysis reveals this¬†wasn't just a technical failure, but a

**process and coverage gap**

across¬†multiple areas:

- **Testing¬†practices**¬†didn't match the service's full capability scope
- **Dependency management**¬†lacked safeguards for¬†compatibility validation
- **Monitoring and alerting**¬†systems¬†weren't mature enough for reliable¬†incident detection
- **QA processes**¬†were insufficient for a multilingual AI service

Together, these factors led to silent service degradation for multilingual transcription tasks.

# Mitigation and resolution

What steps did you take to resolve this incident?

Once the issue was clearly identified, the team took the following steps:

- Upgraded the `torch` dependency to **version 2.6**, resolving the compatibility issue with the latest `whisperx`.
- Released a **patched version** of the `call-transcribe` service with updated dependencies.
- **Reprocessed all impacted multilingual calls**, recovering missed data ([AT-349](https://cloudtalk.atlassian.net/browse/AT-349)).
- Tuned and refined existing **Datadog alerts** to reduce noise and improve signal for actual production-impacting issues ([AT-338](https://cloudtalk.atlassian.net/browse/AT-338)).

### **Why wasn‚Äôt the issue caught and mitigated earlier?**

The issue was not detected before release because **non-English transcription was not tested during manual QA**. The bug only affected those specific scenarios, and since English transcriptions continued to work, no immediate red flags were raised. This gap in testing coverage **allowed the problem to slip into production unnoticed**.

While the alerting system did trigger on the first day, it was **undergoing calibration** at the time, and the alert was **mistakenly deprioritized** as noise.

# Lessons learnt

### What went well

- Fast log access and traceability through **Datadog** accelerated triage.
- Existing alerting, while noisy, was technically correct and provided an early signal.

### ‚ùå What could have gone better

- **No automated QA coverage** for multilingual transcription led to the failure being missed during review.
- Alert was initially ignored due to recent alert rule changes and lack of confidence in thresholds.
- **Dependency upgrades were not explicitly reviewed**, relying on PDM to manage them safely.

### üîß What‚Äôs next

- Build **automated tests for multilingual transcription cases** and require them in CI.
- Define a **strict dependency upgrade policy**, including lockfile reviews and test runs per environment.
- Improve monitoring to detect **functional errors (e.g., zero processed calls per language)**.
- Plan and track QA improvements for AI services ‚Üí [Notion Plan](https://www.notion.so/AI-Team-1f92bccf284f80f19acec8a48caab983?pvs=21)

| Time | Event |
| --- | --- |
| **2025-06-19** |  |
| 15:52:59 | **Incident reported by Du≈°o Argal√°≈°**

Du≈°o Argal√°≈° reported the incident
Severity: P3
Status: Documenting |",P3,,Call Transcribe,AI Team,"Infrastructure, Release problems",https://www.notion.so/INC-923-Call-Transcribe-is-not-processing-non-english-calls-21b2bccf284f815bacffccb33c224691,2025-06-18T13:52:00.000Z
948,inc-948-asteriskactivecalls200-asterisk-euc1-5-and-23-05-07-2025,"RCA Summary - INC-948 (Asterisk Active Calls >200 on EUC1-5 and EUC1-23, 05-07-2025)

Root Cause:
A misconfiguration in Company 304277‚Äôs call routing caused the company‚Äôs own phone number (+18883062352) to be forwarded to itself, creating an infinite call loop. This loop continuously generated new call sessions on Asterisk servers asterisk-prod-euc1-5 and asterisk-prod-euc1-23, exhausting their capacity and leading to high active call counts (>200) and degraded performance.

Impact:
	‚Ä¢	Both affected Asterisk hosts became overloaded and unable to handle additional connections.
	‚Ä¢	Minor service degradation for other customers using the same Asterisk nodes.
	‚Ä¢	The issue was localized to specific tenants and did not affect system-wide call processing.

Mitigation & Fix:
	‚Ä¢	Affected company‚Äôs configuration was corrected manually, replacing the self-referencing forward rule with an announcement message to prevent recurrence.
	‚Ä¢	Both Asterisk instances were restarted to clear the stuck call loops.
	‚Ä¢	Future prevention proposed: Dashboard validation rule to block setting the same number as a forwarding destination.

Status:
	‚Ä¢	Incident resolved and verified by Jirka Srba.
	‚Ä¢	Ownership handed to Tomasz Brach for coordination with the Dashboard team.
	‚Ä¢	Incident closed after stability confirmation and no recurrence observed.",P3,Asterisk,Asterisk,Voice,"Infrastructure, Performance",,2025-07-05T16:13:47.813Z
994,inc-994-voiceagentoldmessages60-21-07-2025,"RCA Summary - INC-994 (Voice Agent Old Messages >60min, 21-07-2025)

Root Cause:
Voice agents from account 100410 were overloading Deepgram‚Äôs concurrent call capacity, causing processing delays in the voice-agents-prod-euc1.fifo SQS (Simple Queue Service) queue. This led to messages older than one hour accumulating in the queue, meaning that worker instances couldn‚Äôt process real customer calls in time.

Impact:
	‚Ä¢	System hit the Deepgram concurrency limit, blocking or delaying calls from other customers.
	‚Ä¢	Queue backlog reached over 100,000 pending messages, significantly slowing down message throughput.
	‚Ä¢	Affected overall responsiveness of Voice Agent call handling.

Mitigation & Fix:
	‚Ä¢	Removed calls from account 100410 from the queue to restore normal processing for active customers.
	‚Ä¢	Increased Deepgram concurrency limits to raise system throughput capacity.
	‚Ä¢	Future prevention plan: implement smart queueing and per-company concurrency limits in Voice Agents to isolate heavy users and protect global resources.

Status:
	‚Ä¢	Incident resolved by Filip Pro≈°ovsk√Ω.
	‚Ä¢	No further customer impact after cleanup and concurrency adjustment.
	‚Ä¢	Follow-up: design and prioritize queue and concurrency isolation mechanisms.",P3,Voice Agent,Voice Agent,AI Team,"Infrastructure, Performance",,2025-07-21T09:05:04.653Z
1012,inc-1012-kamailio-stuck-in-restart-loop-due-to-lb-health-check-30-07-2025,"RCA Summary - INC-1012 (Kamailio Stuck in Restart Loop due to LB Health Check, 30-07-2025)

Root Cause:
Following a restart of the kamailio-trunks instance in EUC1 (performed to resolve incident INC-1008), AWS load balancer health checks marked the service as unhealthy, triggering an automatic instance replacement cycle. The new instance subsequently got stuck in a restart loop, likely because it didn‚Äôt pass the health check within the allowed time threshold. The exact internal cause of the failed health check remains undetermined due to the instance being terminated automatically by AWS before deeper inspection was possible.

Impact:
	‚Ä¢	~15 minutes of dropped inbound and outbound calls in the EUC1 region.
	‚Ä¢	Approximately <10 affected call channels during the outage window.
	‚Ä¢	Partial service unavailability in trunk routing for customers connected to the impacted region.

Mitigation & Fix:
	‚Ä¢	The faulty instance was terminated automatically by the AWS load balancer, restoring service once a new healthy instance was launched.
	‚Ä¢	Manual validation confirmed no persistent fault in the Kamailio configuration or deployment process.
	‚Ä¢	Follow-up created to review and improve load balancer health check configuration and ensure high availability (HA) by maintaining at least two active trunk instances per region.

Status:
	‚Ä¢	Incident resolved by Filip Pro≈°ovsk√Ω and closed after service restoration.
	‚Ä¢	Follow-up assigned to Elena Darriba to improve Kamailio deployment resilience and health check behavior.
	‚Ä¢	No recurrence observed after monitoring period.",P3,Kamailio,Kamailio,,Infrastructure,,2025-07-30T04:35:49.000Z
1016,inc-1016-transcriptionsvisiblemessages100-31-07-2025,"RCA Summary - INC-1016 (Transcriptions Visible Messages >100, 31-07-2025)

Root Cause:
Delays in call transcriptions were caused by a combination of AWS vCPU quota limits and incorrect node allocation by Karpenter, the cluster autoscaler. Karpenter mistakenly scheduled transcribe pods on g6f instances with fractional GPUs, which are insufficient for the AI transcription workload that requires a full GPU. At the same time, the AWS GPU instance vCPU quota had been reached, preventing proper node scaling and causing a backlog.

Impact:
	‚Ä¢	Over 100 call transcriptions delayed by up to 2 hours.
	‚Ä¢	Temporary slowdown in AI-driven call processing and post-call transcription availability.
	‚Ä¢	No data loss; processing resumed once capacity was restored.

Mitigation & Fix:
	‚Ä¢	Updated Karpenter configuration to require nodes with at least one full GPU for transcription pods.
	‚Ä¢	Increased AWS vCPU quota from 64 to 196 cores, and raised both spot and on-demand GPU instance quotas to 512 to handle future spikes.
	‚Ä¢	Cleared existing backlog after quota expansion and node rescheduling.

Status:
	‚Ä¢	Incident resolved and verified by Filip Pro≈°ovsk√Ω.
	‚Ä¢	Follow-up suggested to continuously monitor GPU scaling efficiency and quotas.
	‚Ä¢	System fully operational with improved capacity and allocation safeguards.
",P3,,Call Transcribe,,"Architecture weakness, Infrastructure",,2025-07-31T11:33:49.541Z
1026,inc-1026-missing-call-recordings-on-warm-transfers-06-08-2025,"*Generated by* @Michal Popoviƒç *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1026) on* August 20, 2025 9:41 AM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P3

### üë™ Team

- **Incident owner:** @Ivan Hristov
- **Reporter:** @Ivan Hristov
- **Incident commander:** @Tomasz Niedziela-Brach
- **Active participants:** @Jiri Srba, @Ivan Hristov, @Lucia Mlyn√°rov√°, @Arnaldo Tema, @Roman Hartig, @Tomasz Niedziela-Brach, @Michal Popoviƒç, @Zoltan Viragh
- **Observers:** @Mark Humeniuk, @Debby Wu, @Martina Redajova

### üìù Custom Fields

*No custom fields have been set for this incident.*

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1026)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C0997TZRD60)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** August 6, 2025 10:20 AM (GMT+2)
- **Resolved at:** August 7, 2025 7:50 AM (GMT+2)
- **Impact started at:** July 29, 2025 10:20 AM (GMT+2)
- **Identified at:** August 6, 2025 10:20 AM (GMT+2)
- **Fixed at:** August 7, 2025 6:00 AM (GMT+2)
- **Closed at:** August 7, 2025 4:00 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 8 days, 19 hours

# üñäÔ∏è Summary

**SUMMARY**
**Issue:**
Since **July 29th**, **outbound calls** that are **warm transferred to internal extensions** are missing **call recordings** on the **main leg**. Total 870 Missing recordings. This issue only affected outbound calls warm transfered to internal extension.

- **Main leg (to outbound number):** Recording is missing
- **Internal call between agents:** Recording is present
- The issue appears to have been introduced unintentionally by a non-recording-related change: [**VOIP-1888**](https://cloudtalk.atlassian.net/browse/VOIP-1888).
- The dialplan invokes `record_process.sh` with **fewer arguments than expected** missing uuid ****variable not populated in specific dialplan function because of failed condition.

**Actions Taken**

- **Reverted VOIP-1888 changes**

**Next Steps:**

- **Engineering:** Open a **bug ticket** to address the root issue introduced by **VOIP-1888**

# Leadup

Implementation of **VOIP-1888** has been related to display internal calls in analytics. Together with changes engineer have implemented small fixes to conditions he believed were wrong. Condition evaluated length of variable and based on that executed specific code.

Condition evaluations which was identified as wrong was actually correct.

**removed** exten => s,1,GotoIf($[${LEN(${warm_transfer_uuid})} > 0]?:end)

---

**added** exten => s,1,GotoIf($[${LEN(${warm_transfer_uuid})} > ""0""]?:end)

---

LEN function is not producing text, but integer. This change has been implemented because we had in the past issues with similar conditions and it was believed that dialplan is always producing text as zero instead of integer.

# Fault

A missing execution step in the dialplan prevented the population of variables used to represent¬†`uuid`¬†and¬†`unique_id`¬†for the¬†**call_processing**¬†script. As a result, the correct recording was overwritten by a new recording that encountered the same behavior.

Specifically, the first call affected by the issue generated a recording saved as¬†`_.wav`. When a subsequent outbound call with a warm transfer to an internal extension hit the same bug, the system attempted to save its recording with the same¬†`_.wav`¬†filename, thereby overwriting the original file instead of creating a distinct one with a proper¬†`uuid_unique_id`¬†suffix.

# Detection

The issue was initially reported by customers who were unable to play recordings for specific calls. During the investigation, we discovered that the¬†`.sh`¬†script responsible for processing recordings was throwing an error, which ultimately led us to identify the root cause.

# 5 why's analysis

| Problem | **Why 1** | **Why 2** | **Why 3** | **Why 4** | **Why 5** | **Root Cause** | **Recurrence Prevention** | Tasks |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Call recordings with outbound warm transfer to internal extension have been lost. | Recordings were overwritten by another call recording with the same transfer scenario, since both were saved under the same generic filename. | Dialplan has not populated  unique call identifiers (e.g.,¬†`uuid`,¬†`unique_id`). | Code has been skipped by faulty condition. | Condition has been changed to evaluate text instead of integer. | Engineer has wrongly identified condition as incorrect during the code changes and implementation testing of the function containing these conditions. | Wrong evaluation of the condition. | Update the asterisk knowledge base to contains correct behaviour and return variable types for dialplan conditions. | [https://www.notion.so/cloudtalkio/Asterisk-dialplan-conditions-2552bccf284f80eb8a54e28ab9ef1373](https://www.notion.so/2552bccf284f80eb8a54e28ab9ef1373?pvs=21) |
|  |  |  |  | Testing has not revealed the issue. | Test cases related to specific scenario have not been executed because change has been not related to  implementation. | Changes not related to implementation pushed to the pull request. | Do not include unrelated changes to pull request for the implementation.If you find issue in the code you working with, create separate pull request and execute relevant test cases. | n/a |
|  |  |  |  |  | Warm transfer test cases has not been executed even when the changed code is executed during the transfer. | Always execute all test cases relate to the code. | Automated e2e test cases for all scenarios. | **VOIP-2172** together with E2E test initiative started by QA team. |
|  | Recordings were overwritten by another call recording with the same transfer scenario, since both were saved under the same generic filename. |  |  |  |  | Recording processing stored calls by generic filename without any specific identifications. | Improve shell script processing recordings to contains at least second identificator if others are missing. | https://cloudtalk.atlassian.net/browse/VOIP-2174 |

# Root cause

The issue occurred because a¬†**faulty dialplan condition**¬†prevented the population of unique call identifiers (`uuid`,¬†`unique_id`) during outbound warm transfers to internal extensions. This caused recordings to be saved under non-unique, generic filenames, leading to overwrites.

The faulty condition was introduced when the evaluation logic was incorrectly changed from integers to text. This mistake went unnoticed because:

- The change was bundled into an unrelated pull request, bypassing focused review.
- Test coverage was insufficient ‚Äî warm transfer scenarios were not executed, and no automated end-to-end tests existed for this specific flow.

# Mitigation and resolution

To mitigate the issue, we performed a rollback of the¬†**VOIP-1888**¬†changes in production. This approach was chosen because identifying the exact breaking line of code would have required additional time, and rolling back ensured service stability while the investigation continued.

# Lessons learnt

- **Type Conversion in Asterisk 11** Avoid relying on **Asterisk 11** for internal type conversions (e.g., `int` ‚Üí `string`). It behaves inconsistently and can introduce subtle, hard-to-diagnose errors.
- **Need for Comprehensive End-to-End Testing** As our Dialplan grows in complexity, management becomes increasingly challenging. To maintain reliability, we must implement robust end-to-end tests covering diverse scenarios before deploying changes to production.

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-07-29** |  |
| 10:20:00 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 1w later |  |
| **2025-08-06** |  |
| 10:20:00 | **Identified at**

Custom timestamp ""Identified at"" occurred |
| 10:20:24 | **Incident reported by Ivan Hristov**

Ivan Hristov reported the incident
Severity: P3
Status: Investigating |
| 10:20:29 | **Incident escalated**

Your workflow manually escalated the incident to the escalation path [Platform escalation path](https://app.incident.io/cloudtalkio/on-call/escalation-paths/01HYDHY88AJ9FPGQEVNCYAX78Y) |
| **2025-08-07** |  |
| 06:00:00 | **Fixed at**

Custom timestamp ""Fixed at"" occurred |
| 07:50:54 | **Incident resolved and entered the post-incident flow**

Ivan Hristov shared an update
Status: ~~Investigating~~ ‚Üí Documenting |
| 8h later |  |
| 16:00:29 | **Incident closed**

incident.io (incident lifecycle) shared an update
Status: ~~Documenting~~ ‚Üí Closed |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/1026?tab=follow-ups)

- **Improve Asterisk .sh script processing recordings**
    - **Owner:** @Ivan Hristov
    - **JIRA ticket:** [VOIP-2174](https://cloudtalk.atlassian.net/browse/VOIP-2174)
- **Automated testing of recordings and recording content for all call scenarios**
    - Owner: @Michal Popoviƒç
    - **JIRA ticket**: [VOIP-2172](https://cloudtalk.atlassian.net/browse/VOIP-2172)
- **Resolve bugs introduced by feat VOIP-1888**
    - **Owner:** @Ivan Hristov
    - **JIRA ticket:** [VOIP-2160](https://cloudtalk.atlassian.net/browse/VOIP-2160)
- **Recover missing warm transferred calls in analytics**
    - **Owner:** @Roman Hartig
    - **JIRA ticket:** [DSH-6026](https://cloudtalk.atlassian.net/browse/DSH-6026)
- **[Voice] Missing Recordings**
    - **Owner:** @Ivan Hristov
    - **JIRA ticket:** [VOIP-2142](https://cloudtalk.atlassian.net/browse/VOIP-2142)",P3,,,,"Reported by customers before we learned, Regression, Missing proactive alerting, Insufficient testing, Data loss",https://www.notion.so/INC-1026-Missing-Call-Recordings-on-Warm-transfers-2552bccf284f811c8740d34dfaf6b14c,2025-08-06T08:20:24.129Z
1027,inc-1027-mysqlinnodbrowlockwaittimecritical-campaigns-06-08-2025,"RCA Summary - INC-1027 (MySQL InnoDB Row Lock Wait Time Critical - Campaigns, 06-08-2025)

Root Cause:
The Campaigns API experienced database contention due to table-level locking during disposition saving operations. Specifically, the Saving Dispositions process locked rows (and in certain cases, large portions of the table) for extended periods, leading to elevated InnoDB row lock wait times on the primary production database (ct-prod-eu-db-1). This was the second consecutive occurrence of the same issue, the previous one being incident INC-1022.

Impact:
	‚Ä¢	Elevated InnoDB row lock wait times >5 seconds sustained for over 5 minutes.
	‚Ä¢	Temporary API slowdowns and increased latency for campaign-related requests.
	‚Ä¢	No data loss, but degraded performance and potential user-facing delays.

Mitigation & Fix:
	‚Ä¢	Incident initially paused to allow further analysis and confirmation.
	‚Ä¢	Root cause confirmed as locking caused by disposition save logic.
	‚Ä¢	Follow-up created: ‚ÄúSaving Dispositions locks the table‚Äù to refactor this logic and reduce locking granularity (assigned to Erik Dvorƒç√°k).
	‚Ä¢	Monitoring thresholds for MySQL row lock alerts reviewed to better capture recurrence patterns.

Status:
	‚Ä¢	Incident resolved and documented by Jirka Srba.
	‚Ä¢	Root cause under active remediation via follow-up work.
	‚Ä¢	Long-term fix planned to optimize transaction isolation and write concurrency for Campaigns database operations.",P3,Campaigns,Campaigns,App Devs,Performance,,2025-08-06T21:59:19.983Z
1028,inc-1028-kubepodcrashlooping-tray-07-08-2025,"What's going on?
The integrations-tray-0 pod in the 'integrations' namespace on the prod-eu-central-1 Kubernetes cluster is repeatedly failing to start and is in a CrashLoopBackOff state. This is impacting production services monitored by kube-prometheus-stack.
What caused it? (medium confidence)
Recent code changes introduced either a missing required environment variable (CT_CALL_SERVICE_URL) or altered secret key names for Zoho Phonebridge without updating the external secrets. These misconfigurations are preventing the pod from starting successfully.
What can I do next?
‚Ä¢ Retrieve and review container logs for integrations-tray-0 using kubectl logs (with --previous if restarts occurred) to identify the specific startup failure [1] [2]
‚Ä¢ If logs confirm a missing or invalid environment variable, update the deployment or secret to set the required value, then redeploy integrations-tray-0 [3]
‚Ä¢ If logs show secret access errors, correct Zoho Phonebridge secret naming in the external secret store and Kubernetes, then restart the integrations-tray pod [4]

The assumption is that it is crashing due to high event load and the process is dying. I usually dequeue the events to get it unstuck and process them later. We already have task planned to properly access what is happening with tray itegrations.",P3,Integrations,Integrations,Integrations Dev,"Architecture weakness, Performance",,2025-08-07T06:46:21.084Z
1029,inc-1029-asteriskactivecalls200-asterisk-prod-use1-0-07-08-2025,"RCA Summary - INC-1029 (Asterisk Active Calls >200 - asterisk-prod-use1-0, 07-08-2025)

Root Cause:
A sudden traffic spike from calls directed to number +13014506945 triggered a burst of SIP (Session Initiation Protocol) transactions in kamailio-trunks, overwhelming Asterisk instances asterisk-use1-0 and asterisk-use1-5. The high transaction load caused both servers to crash and become unresponsive, leading to temporary call processing failures. Evidence suggests the spike originated from spam or abusive call patterns targeting the customer behind that number, not from legitimate usage.

Impact:
	‚Ä¢	Two Asterisk nodes (use1-0 and use1-5) were temporarily unavailable.
	‚Ä¢	Short-term call disruptions and dropped sessions in the us-east-1 region.
	‚Ä¢	No persistent customer impact‚Äîno complaints reported after recovery.
	‚Ä¢	Minor issue found where greeting files were missing on asterisk-use1-0, causing additional call handling warnings until remounted.

Mitigation & Fix:
	‚Ä¢	Remounted the missing greetings directory to restore Asterisk audio resources.
	‚Ä¢	Silenced cascading alerts to stabilize monitoring during recovery.
	‚Ä¢	Recovered affected nodes; service fully restored.
	‚Ä¢	Identified spam traffic source; confirmed the incident was external and isolated.
	‚Ä¢	Ongoing investigation to introduce traffic throttling and abuse detection on trunks to prevent similar overload scenarios.

Status:
	‚Ä¢	Incident resolved and verified by Tomasz Brach and Ivan Hristov.
	‚Ä¢	Root cause confirmed as malicious call burst overwhelming trunk load.
	‚Ä¢	Follow-up actions in progress to harden protection and add proactive throttling for suspicious traffic.",P3,,,,Performance,,2025-08-07T13:41:47.840Z
1038,inc-1038-transcriptionsvisiblemessages100-08-08-2025,"*Generated by* @Jakub Bober *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1038) on* August 11, 2025 10:25 AM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P3

### üë™ Team

- **Incident owner:** @Jakub Bober
- **Incident commander:** @Jaroslav Tomeƒçek
- **Active participants:** @Jiri Srba, @Peter Bakos, @Jaroslav Tomeƒçek, @Lenka ≈†efƒç√°kov√°, @Jakub Bober
- **Observers:** @Du≈°an Argal√°≈°, @Jan Cienciala, @Lukas Masar

### üìù Custom Fields

- **Caused by:** [Call Transcribe](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRD3BQXPK79CN61732NJFB)
- **Affected services:** [Call Transcribe](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRD3BQXPK79CN61732NJFB)
- **Affected teams:** [AI Team](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1GS8XEFEM2EXF9CN2)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1038)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C0995D14VFH)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** August 8, 2025 11:48 AM (GMT+2)
- **Accepted at:** August 8, 2025 11:58 AM (GMT+2)
- **Resolved at:** August 8, 2025 2:09 PM (GMT+2)
- **Impact started at:** August 8, 2025 11:48 AM (GMT+2)
- **Identified at:** August 8, 2025 12:39 PM (GMT+2)
- **Fixed at:** August 8, 2025 1:20 PM (GMT+2)
- **Reviewed at:** August 8, 2025 1:45 PM (GMT+2)
- **Closed at:** August 8, 2025 1:46 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 1 hours, 31 minutes

# üñäÔ∏è Summary

There are more than 100 transcriptions waiting to be processed in ""calls-transcribe-prod-euc1"" SQS queue

**Summary:** needs to be investigated by AI-Platform team, workers are not catching up

**Alert instances:** 1

- **alertname**: TranscriptionsVisibleMessages100
- **application**: call-transcribe
- **cluster**: prod-eu-central-1
- **container**: euc1
- **endpoint**: 9106
- **environment**: prod
- **exported_job**: aws_sqs
- **instance**: 10.150.37.106:9106
- **job**: prometheus-cloudwatch-exporter-euc1
- **namespace**: monitoring
- **pod**: prometheus-cloudwatch-exporter-euc1-64c586b7c7-hfvb5
- **prometheus**: monitoring/kube-prometheus-stack-prometheus
- **queue_name**: calls-transcribe-prod-euc1
- **region**: eu-central-1
- **service**: prometheus-cloudwatch-exporter-euc1
- **severity**: critical
- **slack_channel**: #dev-ai-team-reporting
- **team**: ai

# Leadup

In AI transcription service, we are downloading WhisperX model on  service startup.
This model is downloaded directly from GitHub via version tagged reference.  

# Fault

During service startup, the call-transcriber service downloads the WhisperX model directly from GitHub using a version-tagged reference. A temporary GitHub outage affected repositories accessed via tags, making the model download fail. 

Files referenced directly from main branches were unaffected.

# Detection

An alert was triggered when a large number of transcription requests accumulated in the SQS queue. Investigation showed that the call-transcriber service had stopped processing messages.

# 5 why's analysis

**1. Why was the call-transcriber service not processing SQS messages?**

- The service was crashing, preventing it from handling transcription messages.

**2. Why was the service crashing?**

- It failed during startup due to an error downloading the WhisperX model.

**3. Why did the WhisperX model download fail?**

- GitHub experienced a partial outage where files accessed via tagged references could not be downloaded.

**4. Why was the model downloaded at service startup instead of using a cached or pre-bundled version?**

- The service was designed to always download the tagged version at startup, with no fallback or cache in place.

**5. Why was there no fallback or caching mechanism for model availability?**

- The initial design did not account for third-party dependency outages, and no redundancy or resilience plan was implemented for critical model files.

# Root cause

The call-transcriber **service depends on downloading the WhisperX model from GitHub at startup, using a version-tagged reference.

The design lacked a fallback or caching mechanism for the model, making it a single point of failure.

When GitHub experienced a partial outage that disrupted downloads of tagged references, the service failed to start, causing message backlogs in the transcription queue.

# Mitigation and resolution

@Jakub Bober  reviewed service logs and identified that the pods were crashing due to a 404 HTTP error when attempting to download the WhisperX model.

He reproduced the issue manually and confirmed it was caused by a GitHub outage affecting downloads from tagged references. Testing alternative versions revealed that only the model referenced from the main branch was accessible.

@Jiri Srba updated the model link in the environment variables to point to the working main branch reference and restarted the deployment, restoring service functionality.

# Lessons learnt

- Relying on external services at runtime without redundancy or caching creates fragility
- Models should not be downloaded during runtime but should instead by available at startup
    - https://cloudtalk.atlassian.net/browse/AT-446

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-08-08** |  |
| 11:48:49 | **Incident reported in triage by Prometheus Alertmanager alert**

Prometheus Alertmanager alert reported the incident
Severity: None
Status: Triage |
| 11:48:49 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 11:58:32 | **Incident accepted**

Jirka Srba shared an update
Severity: ~~None~~ ‚Üí P3
Status: ~~Triage~~ ‚Üí Investigating |
| 12:17:34 | **Incident resolved and entered the post-incident flow**

Jirka Srba shared an update
Status: ~~Investigating~~ ‚Üí Documenting |
| 12:20:21 | **Incident re-opened**

Jirka Srba shared an update
Status: ~~Documenting~~ ‚Üí Investigating |
| 12:39:00 | **Identified at**

Custom timestamp ""Identified at"" occurred |
| 13:20:00 | **Fixed at**

Custom timestamp ""Fixed at"" occurred |
| 13:42:34 | **Incident resolved and entered the post-incident flow**

Peter Bakos shared an update
Status: ~~Investigating~~ ‚Üí Documenting
The incident was caused by the call-transcribe service being unable to load the WhisperX model from the expected GitHub release URL, resulting in pod failures and a backlog of transcription jobs in the SQS queue.

The call-transcribe pipeline is running again and processing the queued jobs.

Post mortem action is to find a mroe robust way to load the model going forward. |
| 13:45:00 | **Reviewed at**

Custom timestamp ""Reviewed at"" occurred |
| 14:09:01 | **Incident re-opened**

Jirka Srba shared an update
Status: ~~Documenting~~ ‚Üí Monitoring |
| 14:09:22 | **Incident resolved and entered the post-incident flow**

Jirka Srba shared an update
Status: ~~Monitoring~~ ‚Üí Documenting |",P3,Call Transcribe,Call Transcribe,AI Team,"Missing proactive alerting, Architecture weakness",https://www.notion.so/INC-1038-TranscriptionsVisibleMessages100-24c2bccf284f8170a620f6441b022b7e,2025-08-08T09:48:49.551Z
1065,inc-1065-wrong-public-holiday-settings-slovakia-01-09-2025,"RCA Summary - INC-1065 (Wrong Public Holiday Settings - Slovakia, 01-09-2025)

Root Cause:
An incorrect configuration in the holiday database for Slovakia (country_id = 201) caused September 1st and November 17th to be marked as active public holidays when they should not have been. This resulted in misapplied business-hour settings for Slovak customers, where calls were incorrectly routed or restricted based on invalid holiday logic.

Impact:
	‚Ä¢	Slovak customers experienced incorrect call routing and business-hour behavior from 00:00 to 09:35 AM on September 1st, 2025.
	‚Ä¢	No data loss or service degradation beyond routing misbehavior.
	‚Ä¢	Only customers relying on holiday-based routing were affected.

Mitigation & Fix:
	‚Ä¢	A database cleanup was performed to remove invalid holiday records for Slovakia (SELECT * FROM holidays WHERE country_id = 201 AND date LIKE '%-09-01' OR '%-11-17').
	‚Ä¢	Future occurrences of these dates were also removed to prevent recurrence.
	‚Ä¢	Validated that corrected records propagate properly to all routing services.

Status:
	‚Ä¢	Incident resolved and verified by Elena Darriba.
	‚Ä¢	Cleanup completed by Peter Stanko.
	‚Ä¢	Post-fix monitoring confirmed correct routing behavior.
	‚Ä¢	Follow-up completed to ensure holiday DB validation before deployment of new calendar data.",P3,,,,Other,,2025-09-01T07:26:24.495Z
1087,inc-1087-kubepodcrashlooping-integrations-tray-09-09-2025,"RCA Summary - INC-1087 (KubePodCrashLooping - Integrations Tray, 09-09-2025)

Root Cause:
The Integrations Tray service experienced repeated pod crash loops due to JavaScript heap memory exhaustion in a Node.js process. Logs showed repeated FATAL ERROR: Reached heap limit Allocation failed - JavaScript heap out of memory, indicating that garbage collection was unable to reclaim sufficient memory, leading to process termination and Kubernetes continuously restarting the affected pods.

Impact:
	‚Ä¢	Service degradation for integrations relying on Tray (delayed or failed executions).
	‚Ä¢	Increased Kubernetes resource churn due to repeated pod restarts.
	‚Ä¢	No data loss, but temporary disruption in automation flows for some customers.

Mitigation & Fix:
	‚Ä¢	Immediate action: Restarted affected pods to restore partial service.
	‚Ä¢	Root cause validated by reviewing heap usage patterns ‚Äî confirmed memory leak or excessive in-memory object allocation under high load.
	‚Ä¢	Follow-ups created and completed:
	1.	‚ÄúTray - crashing pods‚Äù ‚Äî optimize memory handling and monitor heap growth (completed by Vlad Cirstean).
	2.	‚ÄúIncrease Integration reliability and scalability‚Äù ‚Äî improve container resource allocation and retry logic (assigned to Marian Nociar).
	‚Ä¢	Allocated higher memory limits and updated deployment configuration to prevent immediate recurrence.

Status:
	‚Ä¢	Incident resolved and verified by Tomasz Brach.
	‚Ä¢	Follow-ups implemented to strengthen reliability and scalability.
	‚Ä¢	Post-fix monitoring confirmed stable pod behavior with no further crashes.",P3,Integrations,Integrations,Integrations Dev,,,2025-09-09T09:37:51.112Z
1098,inc-1098-ai-services-use-all-available-postresql-connections-17-09-2025,"*Generated by* @Jakub Bober *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1098) on* September 24, 2025 10:39 AM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P3

### üë™ Team

- **Incident owner:** @Jakub Bober
- **Incident commander:** @Dino Trojak
- **Active participants:** @Jakub Bober, @Dino Trojak, @Michal Popoviƒç, @Filip Prosovsky, @Jaroslav Tomeƒçek, @Jiri Srba

### üìù Custom Fields

- **Caused by:** [Calls Management](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRD3SWMRW98A8NW1JYAEBJ)
- **Affected services:** [Calls Management](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRD3SWMRW98A8NW1JYAEBJ), [CT Call Analyzer](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDBX73X5301V4QPYH4T57), [Call Transcribe](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRD3BQXPK79CN61732NJFB) and [Push Notifications](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDXZY5G06TBEVGB0EZVWF)
- **Affected teams:** [AI Team](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1GS8XEFEM2EXF9CN2)
- **Root Cause Label:** Performance and Infrastructure

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1098)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C09FWBB8UJG)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** September 17, 2025 7:49 PM (GMT+2)
- **Accepted at:** September 17, 2025 11:14 PM (GMT+2)
- **Resolved at:** September 18, 2025 8:17 PM (GMT+2)
- **Impact started at:** September 16, 2025 7:10 PM (GMT+2)
- **Identified at:** September 18, 2025 3:21 PM (GMT+2)
- **Fixed at:** September 18, 2025 3:51 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 1 days, 20 hours

# üñäÔ∏è Summary

**Problem**: More than 500 messages were waiting to be processed in the ct-call-analyzer-prod-euc1 SQS queue, causing a backlog.

**Impact**: Message processing delays affected the CT Call Analyzer, Call Transcribe, and Calls Management services. Over 500 messages were queued before resolution.

**Root cause**: PostgreSQL connection slot exhaustion prevented call analyzer workers from processing messages efficiently.

**Steps to resolve**: We increased the number of calls-management workers and raised the PostgreSQL connection limit. This allowed message processing to recover and services to return to normal operation.

# Leadup

Over the past several months, load on AI-related services steadily increased as new customers were onboarded. This led to a gradual rise in database utilization, particularly from query-heavy services such as `ct-call-analyzer`.

# Fault

The PostgreSQL RDS instance became overloaded, with both CPU utilization and database connections reaching their limits. As a result, dependent services (e.g., calls-management) experienced errors when attempting to query the database.

# Detection

The issue was first detected through error logs from the calls-management service, which reported failed database connections. Further investigation confirmed that the root cause was resource exhaustion on the PostgreSQL instance.

# 5 why's analysis

### **1. Why did PostgreSQL connections start failing?**

Because the RDS instance was overloaded and could not handle more queries.

### **2. Why was the RDS instance overloaded?**

Because CPU utilization was steadily increasing due to growing query load from services (notably `ct-call-analyzer`).

### **3. Why did the growing query load overwhelm the database?**

Because the database instance was not scaled up in time to match the steady increase in demand.

### **4. Why was the database not scaled in time?**

Because monitoring and capacity planning did not trigger alerts or proactive scaling decisions as the load grew gradually (rather than via a sudden spike).

### **5. Why was monitoring/capacity planning insufficient?**

Because thresholds, scaling policies, or review processes were not aligned with real growth trends, leading to missed opportunities for preventive scaling.

# Root cause

The `ct-call-analyzer` service generated the highest sustained CPU load on the PostgreSQL RDS instance and generated higher number of database connections. This was not the result of a sudden spike but a steady increase over time. The gradual growth was not acted upon, and the database was not scaled proactively, leading to resource exhaustion.

# Mitigation and resolution

The PostgreSQL RDS instance was scaled to a higher tier, providing additional CPU and connection capacity. This resolved the resource exhaustion and restored normal operation for dependent services.

# Lessons learnt

- A steady rise in demand can be just as harmful as a sudden traffic spike if not addressed in time
- Scaling decisions should be driven by observed growth trends, not only by reaching hard limits
- Database capacity (CPU, connections, and query throughput) requires continuous tracking and alerting
- Query performance should be reviewed regularly

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-09-16** |  |
| 19:10:00 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 19:10:00 | **First error logs in services reporting connection problems** |
| **2025-09-17** |  |
| 19:49:19 | **Incident reported in triage by Prometheus Alertmanager alert**

Prometheus Alertmanager alert reported the incident
Severity: None
Status: Triage |
| 3h later |  |
| 23:14:47 | **Incident accepted**

Michal Popoviƒç shared an update
Severity: ~~None~~ ‚Üí P4
Status: ~~Triage~~ ‚Üí Investigating |
| **2025-09-18** |  |
| 11:27:37 | **Incident resolved and entered the post-incident flow**

Jakub Bober shared an update
Status: ~~Investigating~~ ‚Üí Documenting |
| 3h later |  |
| 15:21:00 | **Identified at**

Custom timestamp ""Identified at"" occurred |
| 15:27:00 | **Error logs starting to appear again** |
| 15:51:00 | **Fixed at**

Custom timestamp ""Fixed at"" occurred |
| 15:51:00 | **PostgreSQL RDS resized** |
| 16:00:09 | **Incident re-opened**

Michal Popoviƒç shared an update
Status: ~~Documenting~~ ‚Üí Investigating |
| 16:13:56 | **Severity upgraded from P4 ‚Üí P3**

Michal Popoviƒç shared an update
Severity: ~~P4~~ ‚Üí P3 |
| 16:30:00 | **Calls-management consumer lag resolved** |
| 3h later |  |
| 20:17:01 | **Incident paused**

Jakub Bober shared an update
Status: ~~Investigating~~ ‚Üí Paused |
| 20:17:34 | **Incident resumed**

Jakub Bober shared an update
Status: ~~Paused~~ ‚Üí Documenting |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/1098?tab=post-incident)

- **Increase RDS instance postgres-cloudtalk-prod-euc1 size to db.m7g.xlarge**
    - **Owner:** @Jiri Srba
    - **JIRA ticket:** [INFRA-2819](https://cloudtalk.atlassian.net/browse/INFRA-2819)",P3,Calls Management,"Calls Management, Call Transcribe, CT Call Analyzer, Push Notifications",AI Team,"Performance, Infrastructure",https://www.notion.so/INC-1098-AI-services-use-all-available-postresql-connections-2782bccf284f810d9fd5f6c5a071f701,2025-09-17T17:49:19.545Z
1112,inc-1112-voice-agents-failed-deployment-22-09-2025,"RCA Summary - INC-1112 (Voice Agents Failed Deployment, 22-09-2025)

Root Cause:
A misconfigured Redis environment variable in the deployment configuration caused the Voice Agents service to fail during startup. The incorrect Redis connection details prevented the service from initializing properly, resulting in a failed deployment and unavailability of voice agent functionality until the configuration was corrected.

Impact:
	‚Ä¢	Voice Agents service unavailable during the failed deployment window.
	‚Ä¢	Temporary disruption of AI-powered call processing features depending on Voice Agents.
	‚Ä¢	No data loss or downstream impact after recovery.

Mitigation & Fix:
	‚Ä¢	Deployment immediately halted and Redis configuration corrected.
	‚Ä¢	Redeployed the service successfully once the correct environment variable was set.
	‚Ä¢	Verified all dependent services connected properly post-fix.
	‚Ä¢	AI SRE monitored for stability post-recovery.

Status:
	‚Ä¢	Incident resolved by Filip Pro≈°ovsk√Ω.
	‚Ä¢	No further anomalies detected after redeployment.
	‚Ä¢	Follow-ups were not required; incident closed after full service restoration.",P3,Ansible,Voice Agent,AI Team,,,2025-09-22T13:22:25.155Z
1128,inc-1128-the-integrations-section-of-the-new-dashboard-will-not-load-29-09,"*Generated by* @Roman Hartig *via [incident.io](https://app.incident.io/cloudtalkio/incidents/1128) on* October 1, 2025 3:48 PM (GMT+2)*. All timestamps are local to Europe/Bratislava*

### ‚ÑπÔ∏è Key Information

- **Incident Type:** Application
- **Severity:** P3

### üë™ Team

- **Incident owner:** @Elena Darriba
- **Reporter:** @Zoltan Viragh
- **Active participants:** @Elena Darriba, @Zoltan Viragh, @Roman Hartig, @Filip Prosovsky, @Arnaldo Tema
- **Observers:** @Serhii Shevchenko, @Debby Wu, @Jakub Bober

### üìù Custom Fields

- **Caused by:** [Dashboard Backend](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDF811D4F519DG66R632Q)
- **Affected services:** [Dashboard Frontend](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDG4Q3EZQF5XHJNKM8HSD) and [Dashboard Backend](https://app.incident.io/cloudtalkio/catalog/01JM9CCTHV1HTSWDG4FKGPZ4NW/01JMKRDF811D4F519DG66R632Q)
- **Affected teams:** [Dashboard Devs](https://app.incident.io/cloudtalkio/catalog/01JM9C4EP17PKA2W4DYPQ7103J/01JM9C4EP1DKFB0JJSKCNGRN5K)

### üîó Useful Links

- [Incident homepage](https://app.incident.io/cloudtalkio/incidents/1128)
- [Slack channel](https://slack.com/app_redirect?team=TBMUGUCDP&channel=C09HKNCK5R9)

### ‚è±Ô∏è Key Timestamps

- **Reported at:** September 29, 2025 7:41 PM (GMT+2)
- **Accepted at:** September 29, 2025 7:45 PM (GMT+2)
- **Resolved at:** September 29, 2025 8:28 PM (GMT+2)
- **Impact started at:** September 29, 2025 4:00 PM (GMT+2)
- **Identified at:** September 29, 2025 8:14 PM (GMT+2)
- **Fixed at:** September 29, 2025 8:18 PM (GMT+2)
- **Closed at:** September 30, 2025 4:24 PM (GMT+2)

### ‚è≥ Durations

- **Incident duration:** 4 hours, 17 minutes

# üñäÔ∏è Summary

**Problem**: The Integrations section of the new dashboard failed to load, displaying a network error message to users.

**Impact**: The Integrations section of the new dashboard would not load for all users, showing a network error. The issue affected all customers using this dashboard section.

**Root cause**: Frontend changes were released that depended on backend updates which had not been successfully deployed. The backend release failed, so the frontend expected new data that was not available, resulting in errors.

**Steps to resolve**: We re-ran the backend release pipeline to create and deploy the missing version 170, restoring compatibility between frontend and backend.

# Fault

`dashboard-backend`  github action generating docker image with service failed

# Detection

Customer raised a support ticket.

# 5 why's analysis

Why do we haven‚Äôt got notification earlier ‚áí there are no alerts for PODs on prod in pending state (see action items)

Why we have to do manual releases as we have already Port driven releases with success/failure notifications ‚áí those are pending stories in DSH backlog (see action items) 

# Root cause

Frontend changes were released that depended on backend updates which had not been successfully deployed. The backend release failed, so the frontend expected new data that was not available, resulting in errors.

# Mitigation and resolution

We re-ran the backend release pipeline to create and deploy the missing version 170, restoring compatibility between frontend and backend.

# üìÜ Incident Timeline

| Time | Event |
| --- | --- |
| **2025-09-29** |  |
| 16:00:38 | **Impact started at**

Custom timestamp ""Impact started at"" occurred |
| 3h later |  |
| 19:41:38 | **Incident reported by Zolt√°n Ill√©s**

Zolt√°n Ill√©s reported the incident
Severity: P3
Status: Investigating |
| 19:41:43 | **Escalated to Platform escalation path**

Your workflow manually escalated the incident to the escalation path [Platform escalation path](https://app.incident.io/cloudtalkio/on-call/escalation-paths/01HYDHY88AJ9FPGQEVNCYAX78Y) |
| 19:53:35 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

Seems like failed deployment and bump of image:
https://github.com/CloudTalk-io/dashboard-backend/actions/runs/18092090781
https://github.com/CloudTalk-io/argocd-kube/commit/3e5f61d04ac6ab85a200fea368592d49daf8f900 |
| 19:58:00 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

However, this does not seem like the root cause, this is something Kubernetes can handle and will not delete wrong replicas or add the ones with the wrong image until they are ready, healthy to serve traffic |
| 19:58:19 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message*<!subteam^S08SC236HPB>  This is from the previous pod (restarted one)
```{""level"":""error"",""time"":1759147983462,""pid"":1,""hostname"":""dashboard-backend-api-5cf5c758fd-wtmkz"",""req"":{""id"":""req-1mx6"",""method"":""GET"",""url"":""/health"",""query"":{},""headers"":{""host"":""10.150.191.123:3000"",""user-agent"":""kube-probe/1.33+"",""accept"":""*/*"",""connection"":""close""},""remoteAddress"":""10.150.170.240"",""remotePort"":47898},""dd"":{""trace_id"":""5794663285581314548"",""span_id"":""5794663285581314548"",""service"":""dashboard-backend"",""version"":""v0.169.0"",""env"":""prod""},""moduleName"":""ExceptionFilter"",""exceptionWithMetadata"":{""requestMetadata"":{""reqId"":""req-1mx6"",""url"":""/health""},""requestTags"":{""nodeVersion"":""v20.18.1"",""requestType"":""http"",""httpMethod"":""GET""},""requestUser"":{},""err"":{""message"":""connect ECONNREFUSED 0.0.0.0:3000"",""name"":""Error"",""stack"":""Error: connect ECONNREFUSED 0.0.0.0:3000\n    at AxiosError.from (/usr/src/app/node_modules/axios/dist/node/axios.cjs:857:14)\n    at RedirectableRequest.handleRequestError (/usr/src/app/node_modules/axios/dist/node/axios.cjs:3169:25)\n    at RedirectableRequest.emit (node:events:518:28)\n    at eventHandlers.&lt;computed&gt; (/usr/src/app/node_modules/follow-redirects/index.js:49:24)\n    at ClientRequest.emit (node:events:518:28)\n    at req.emit (/usr/src/app/node_modules/dd-trace/packages/datadog-instrumentations/src/http/client.js:97:25)\n    at emitErrorEvent (node:_http_client:101:11)\n    at Socket.socketErrorListener (node:_http_client:504:5)\n    at Socket.emit (node:events:530:35)\n    at Socket.emit (/usr/src/app/node_modules/dd-trace/packages/datadog-instrumentations/src/net.js:61:25)\n    at Axios.request (/usr/src/app/node_modules/axios/dist/node/axios.cjs:4258:41)\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at async Axios.request (/usr/src/app/node_modules/axios/dist/node/axios.cjs:4253:14)\n    at async Axios.request (/usr/...* |
| 20:05:24 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

Maybe some changes on the frontend today? https://github.com/CloudTalk-io/dashboard-frontend-monorepo/commits/main/ |
| 20:06:01 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

@Harry Can it be related that changes added to the frontend are fine, but as 170 tag was not built and released, it fails now? |
| 20:10:15 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

Re-running dashboard release to 170 |
| 20:14:00 | **Identified at**

Custom timestamp ""Identified at"" occurred |
| 20:16:11 | **Message from Roman Hartig**

Roman Hartig's message was pinned by Filip Pro≈°ovsk√Ω

```
GraphQLError: Cannot query field ""logTickets"" on type ""ZendeskConfig"". Did you mean ""importTickets"", ""logMissed"", or ""logMissedAs""?
```

this is the main issue - dd (https://app.datadoghq.eu/apm/traces?query=env%3Aprod%20service%3Adashboard-backend%20status%3Aerror%20%40error.message%3A%22Cannot%20query%20field%20%5C%22logTickets%5C%22%20on%20type%20%5C%22ZendeskConfig%5C%22.%20Did%20you%20mean%20%5C%22importTickets%5C%22%2C%20%5C%22logMissed%5C%22%2C%20or%20%5C%22logMissedAs%5C%22%3F%22&agg_m=count&agg_m_source=base&agg_t=count&cols=core_service%2Ccore_resource_name%2Clog_duration%2Clog_http.method%2Clog_http.status_code&fromUser=false&graphType=waterfall&historicalData=true&messageDisplay=inline&query_translation_version=v0&refresh_mode=sliding&shouldShowLegend=true&sort=desc&spanID=2886556523556202347&spanType=all&storage=hot&timeHint=1759169676112&trace=AwAAAZmWrwNQO91s-gAAABhBWm1Xcnd0aUFBQk5ydEFYY0w1SkJWdy0AAAAkZjE5OTk2YWYtMWU5NC00MGI5LWE1NjItMGYyZjliMzUzMGVmAAAAZA&traceID=4493717891091403425&traceQuery=&view=spans&start=1759083338715&end=1759169738715&paused=false) -> looking more into it |
| 20:16:32 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

@Harry then it is related to changes I sent above if that is the main error |
| 20:18:00 | **Fixed at**

Custom timestamp ""Fixed at"" occurred |
| 20:26:53 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

Summary:
- Changes released on frontend (v1.161.0 (https://github.com/CloudTalk-io/dashboard-frontend-monorepo/releases/tag/dashboard-v1.161.0)) without properly released backend caused the issue
1. Failed release of dashboard-backend
    a. This issue was handled by infrastructure and Kubernetes did not allow not existing image to be deployed, keeping the app (version 169) up
    b. However, as frontend was released, it was expected from backend to receive some data (introduced in version 170)
        i. As the app version was older, this caused error on the frontend side
Recovery: Rerun the release pipeline to create 170 app version
Root cause:
- If app would be on unified release process, failed backend deployment would be received as slack notif and team could handle the errors sooner.
- Did not verified functionality after release
- Version bumped before checked it really exists and is functional |
| 20:28:57 | **Incident resolved and entered the post-incident flow**

Filip Pro≈°ovsk√Ω shared an update
Status: ~~Investigating~~ ‚Üí Documenting |
| 20:31:38 | **Message from Filip Pro≈°ovsk√Ω**

Filip Pro≈°ovsk√Ω pinned their own message

I also see failed deployments on frontend, so actual release of the changes and impact was approx from 4pm today (cc @Harry):
https://github.com/CloudTalk-io/dashboard-frontend-monorepo/actions/workflows/prod-build.yml |
| **2025-09-30** |  |
| 16:24:51 | **Opted out of post-incident flow**

Elena Darriba shared an update
Status: ~~Documenting~~ ‚Üí Closed |

# üëü Follow-ups

[View follow ups in incident.io](https://app.incident.io/cloudtalkio/incidents/1128?tab=post-incident)

- **[DSH] Integrations Wont Load**
    - **Owner:** Unassigned
    - **JIRA ticket:** [DSH-6245](https://cloudtalk.atlassian.net/browse/DSH-6245)
- https://cloudtalk.atlassian.net/browse/DSH-4963
- https://cloudtalk.atlassian.net/browse/DSH-5018
- https://cloudtalk.atlassian.net/browse/DSH-6277",P3,Dashboard Backend,"Dashboard Frontend, Dashboard Backend",Dashboard Devs,"Insufficient testing, Release problems",https://www.notion.so/INC-1128-The-Integrations-Section-Of-The-New-Dashboard-Will-Not-Load-27f2bccf284f81969235f42c5cb53d62,2025-09-29T17:41:38.120Z
1135,inc-1135-vm-drop-doesnt-work-01-10-2025,"RCA Summary - INC-1135 (VM Drop Doesn‚Äôt Work, 01-10-2025)

Root Cause:
The voicemail (VM) drop feature failed due to missing message delivery in Redis Pub/Sub, which relies on a fire-and-forget mechanism. When no subscriber is connected at the time a message is published, Redis drops it silently, resulting in no event delivery to the service responsible for executing voicemail drops. This transient disconnection between publisher and subscriber caused the temporary outage.

Impact:
	‚Ä¢	Voicemail drop, upload, and related features (transfer, 3-way call, call monitor, click-to-call) were temporarily non-functional.
	‚Ä¢	No lasting data loss; once subscribers reconnected, normal behavior resumed.
	‚Ä¢	The incident did not affect ongoing or historical calls beyond the temporary feature disruption.

Mitigation & Fix:
	‚Ä¢	Services were restarted to restore Pub/Sub connections, immediately recovering voicemail drop functionality.
	‚Ä¢	End-to-end testing confirmed restoration of all dependent call features.
	‚Ä¢	Discussion initiated to explore more reliable event propagation mechanisms (e.g., persistent queues like Redis Streams or Kafka) to avoid reliance on transient Pub/Sub delivery.

Status:
	‚Ä¢	Incident resolved by Dino Trojak.
	‚Ä¢	Debrief held on October 2, 2025, with the Voice and Observability teams.
	‚Ä¢	Observability confirmed current limitations in detecting dropped Pub/Sub messages.
	‚Ä¢	Incident closed after full feature verification and service stability confirmation.",P3,Realtime WS,"Daemon, Realtime WS","App Devs, Voice","Insufficient testing, Infrastructure, Missing proactive alerting, Human error",,2025-10-01T11:01:38.817Z
1148,inc-1148-viptelvmtele-outbound-calls-failing-06-10-2025,"RCA Summary - INC-1148 (Viptel/VMtele Outbound Calls Failing, 06-10-2025)

Root Cause:
Outbound calls routed through the VMtele SIP trunk were failing because SIP 200 OK packets exceeded the 1500-byte MTU (Maximum Transmission Unit) size limit. The excessive packet size was caused by large proprietary SIP headers included by VMtele (specifically internal X- headers). This caused fragmentation issues that led to dropped or incomplete SIP responses, breaking call establishment.

Impact:
	‚Ä¢	Outbound calls through VMtele trunk failed for affected customers.
	‚Ä¢	The issue primarily impacted users in regions relying on VMtele routing (EU-based carriers).
	‚Ä¢	No inbound call impact; failure limited to outbound session establishment.

Mitigation & Fix:
	‚Ä¢	Coordinated with VMtele‚Äôs technical team, who removed the unnecessary internal X header from their 200 OK responses, reducing message size below the 1500-byte threshold.
	‚Ä¢	Outbound calls successfully restored after change verification.
	‚Ä¢	Planned improvement: migrate VMtele trunk communication from UDP to TCP to eliminate future MTU-related issues and ensure reliable SIP packet delivery. Migration scheduled for the weekend.

Status:
	‚Ä¢	Incident resolved by Michal Popoviƒç.
	‚Ä¢	VMtele trunk re-enabled and monitored ‚Äî verified working correctly.
	‚Ä¢	Follow-up planned: transition to TCP transport for long-term stability.",P3,,,Voice,"Other, Missing proactive alerting, Insufficient testing",,2025-10-06T07:26:42.073Z
1149,inc-1149-kubepodcrashlooping-integrations-tray-06-10-2025,"RCA Summary - INC-1149 (KubePodCrashLooping - Integrations Tray, 06-10-2025)

Root Cause:
The integrations-tray pod in the prod-eu-central-1 cluster entered a CrashLoopBackOff state following a deployment of Docker image v2.45.1. The crash loop began immediately after rollout, indicating the new image contained breaking changes or misconfiguration that prevented the application from starting successfully. Missing pod logs limited direct visibility into the exact failure mode, but patterns suggest initialization failure or memory/resource exhaustion as the likely cause.

Impact:
	‚Ä¢	Integrations service unavailable starting from 09:12:50 UTC.
	‚Ä¢	All Tray-based integration workflows were temporarily disrupted, causing partial service degradation for customers depending on real-time data synchronization.
	‚Ä¢	No data loss occurred; the disruption was limited to runtime unavailability.

Mitigation & Fix:
	‚Ä¢	Restarted the integrations-tray-0 pod after clearing pending backlogs to avoid repeated startup failures.
	‚Ä¢	Validated StatefulSet health to ensure automatic recreation and stabilization of pods.
	‚Ä¢	Post-recovery inspection confirmed stable operation with no further crash loops.
	‚Ä¢	Follow-up planned to:
	1.	Improve deployment validation and smoke tests before image rollouts.
	2.	Enhance log retention for failed pod startups to ensure faster root cause identification.

Status:
	‚Ä¢	Incident managed and resolved by Jirka Srba.
	‚Ä¢	Severity P3; issue mitigated with full service restoration after rollback/restart.
	‚Ä¢	Preventive measures under review to reduce recurrence risk during future deployments.",P3,,,,,,2025-10-06T09:13:21.105Z
1150,inc-1150-apps-watchdog-alert-error-rate-increased-on-the-get-appcontactspag,"RCA Summary - INC-1150 (Apps Watchdog Alert - Error Rate Increased on GET /app/contacts/paginate, 06-10-2025)

Root Cause:
The GET /app/contacts/paginate/all endpoint in API v3 was repeatedly failing due to memory exhaustion when handling customers with very large contact datasets (>90K records). The issue stemmed from a legacy pagination query that performs inefficient full-table operations, consuming excessive memory under heavy load. As a result, the application pods frequently hit their memory limits and restarted, leading to increased error rates.

Impact:
	‚Ä¢	Users from 12 large accounts (listed below) were unable to access their contact lists for extended periods.
	‚Ä¢	Contacts API experienced 491 failures in 15 days before the alert triggered.
	‚Ä¢	Affected companies: 179911, 128575, 305505, 279211, 139839, 310171, 239863, 264457, 247665, 227479, 298427, 234543.
	‚Ä¢	No data loss, but degraded user experience when accessing contact pages.

Mitigation & Fix:
	‚Ä¢	Immediate workaround: increased pod memory limit from 128MB ‚Üí 256MB, and later to 512MB, which stabilized API availability.
	‚Ä¢	Follow-up options under consideration:
	1.	Optimize the legacy query to reduce memory footprint (challenging due to query complexity).
	2.	Adopt the new Contacts Service with proper pagination support.
	3.	Long-term: refactor API v3 to delegate large-scale data handling to a dedicated service.
	‚Ä¢	PRs implemented:
	‚Ä¢	fix(APPS-TRIVIAL): increased mem limit (#187, #188) by Michal Ustan√≠k.
	‚Ä¢	feat(TRIVIAL): increase api-v3 resources (#8038) by Jirka Srba.

Status:
	‚Ä¢	Incident severity set to P3.
	‚Ä¢	Memory increase deployed and verified ‚Äî API stabilized.
	‚Ä¢	Ongoing plan: migrate to Contacts Service pagination for scalable resolution.
	‚Ä¢	No recurrence since resource limits were raised.",P3,,,,"Performance, Infrastructure",,2025-10-06T09:18:11.234Z
1156,inc-1156-kubepodcrashlooping-integrations-tray-08-10-2025,"RCA Summary - INC-1156 (KubePodCrashLooping - Integrations Tray, 08-10-2025)

Root Cause:
The integrations-tray pods in the production EU cluster entered a CrashLoopBackOff state due to Out of Memory (OOM) errors. The service was consuming excessive memory during event processing, particularly when handling large batches of queued integration events. As memory limits were reached, Kubernetes repeatedly terminated and restarted the pods, leading to instability.

Impact:
	‚Ä¢	Integrations functionality degraded across multiple customers during the crash period.
	‚Ä¢	All four pods in the integrations-tray StatefulSet were affected simultaneously.
	‚Ä¢	Processing of queued events was delayed until the system stabilized.
	‚Ä¢	No permanent data loss occurred; events remained in the queue and were retried later.

Mitigation & Fix:
	‚Ä¢	Cleared backlog of problematic events with statuses 1, 0, 10 to reduce load and allow pods to recover.
	‚Ä¢	All four pods were restarted successfully and resumed normal operation.
	‚Ä¢	Created a dedicated spike (INT-3436) to investigate and resolve underlying OOM issues, assigned to Martin ƒåerm√°k. The spike aims to identify:
	‚Ä¢	Memory hotspots in Tray‚Äôs event processing logic.
	‚Ä¢	Potential batching or retry inefficiencies.
	‚Ä¢	Optimal resource limits and scaling configurations.

Status:
	‚Ä¢	Incident severity: P3.
	‚Ä¢	Temporary recovery achieved; monitoring continues.
	‚Ä¢	Long-term remediation in progress under INT-3436 ‚Äì ‚ÄúInvestigate OOM issues for Tray‚Äù.
	‚Ä¢	No recurrence observed since pods resumed normal performance.",P3,,,,,,2025-10-08T11:59:06.129Z
1157,inc-1157-voicemail-detection-runningoutofmemory5-09-10-2025,"RCA Summary - INC-1157 (Voicemail Detection Running Out of Memory, 09-10-2025)

Root Cause:
Multiple voicemail detection EC2 instances in the EUC1 production environment reported critically low free memory (<5%) due to a memory leak in the voicemail detection service. Over time, memory consumption continuously increased until the affected instances became unstable. The auto-scaling group (ASG) terminated one unhealthy instance (i-07ec3d853abd515d6) automatically, while others (i-09c7495fdc96aa0b2, i-022bbb9b78763921a) also approached exhaustion.

Impact:
	‚Ä¢	Temporary degradation of voicemail detection accuracy and performance.
	‚Ä¢	Occasional service interruptions during instance terminations and replacements.
	‚Ä¢	No data loss, but partial downtime for voicemail detection tasks.

Mitigation & Fix:
	‚Ä¢	Refreshed instances in the ASG to recover from low-memory state and restore normal performance.
	‚Ä¢	Temporary fix: increased instance type to t3.medium to provide higher memory capacity.
	‚Ä¢	Root cause (memory leak) acknowledged but deferred, as voicemail detection is scheduled for complete redesign and replacement.

Status:
	‚Ä¢	Incident resolved by Jirka Srba.
	‚Ä¢	Temporary stability achieved with increased memory allocation.
	‚Ä¢	Long-term solution planned: replacement of the current voicemail detection service to eliminate recurring memory leak issues.",P3,Voicemail Detection,Voicemail Detection,AI Team,,,2025-10-08T22:26:17.275Z
1159,inc-1159-call-details-is-not-loading-impacting-all-customers-dsh-6325-09-10,"What's going on?
Customers reported that call details were not loading. While initially thought to impact all customers, investigation revealed this only affected companies with zero configured voice agents, making the call interface unusable for those companies.
What caused it?
A recent deployment of the voice agent update is strongly correlated with the incident. Service was restored to affected companies immediately after rolling back the update. However, due to the absence of telemetry and detailed logs, the specific failure mechanism remains unidentified, and alternative causes cannot be fully ruled out.
What can I do next?
1. Re-deploy the rolled-back voice agent update to a sandbox environment mimicking affected configurations and monitor for call details loading failures, capturing any relevant logs or system responses [1] [2]


the fix was to do this, essentially:

from
 if (voiceAgents?.length > 0) {
to
 if (voiceAgents !== undefined) {

no tests were covering whatever functionality this broke",P3,Statistics Frontend,Statistics Frontend,Dashboard Devs,"Missing proactive alerting, Reported by customers before we learned, Insufficient testing, Release problems",https://www.notion.so/INC-1159-Call-details-is-not-loading-impacting-all-customers-DSH-6325-2872bccf284f810d9bf7fb6e7972c43d,2025-10-09T07:15:08.074Z
1160,inc-1160-workflow-automation-events-kubepodcrashlooping-09-10-2025,"RCA Summary - INC-1160 (Workflow Automation Events - KubePodCrashLooping, 09-10-2025)

Root Cause:
The Workflow Automation (WFA) Events service experienced pod crash loops caused by incorrect or missing database indexes following a recent deployment. The absence of proper indexing led to inefficient query execution and excessive resource consumption, resulting in repeated pod restarts in the Kubernetes cluster.

Impact:
	‚Ä¢	Workflow executions were delayed for approximately one hour.
	‚Ä¢	No workflow data was lost ‚Äî all queued events were eventually reprocessed once the service recovered.
	‚Ä¢	Customers observed temporary delays in automated workflow triggers but not total system failure.

Mitigation & Fix:
	‚Ä¢	Identified and recreated missing indexes on affected database tables.
	‚Ä¢	Deployed a hotfix to address the indexing issue and restore service stability.
	‚Ä¢	Validated recovery by confirming that all pods returned to healthy state and all delayed events were successfully processed.
	‚Ä¢	Follow-up: review deployment and schema migration process to ensure indexes are validated automatically before release.

Status:
	‚Ä¢	Severity classified as P3 (minor outage).
	‚Ä¢	Incident resolved and verified by Jirka Srba and Martin ƒåerm√°k.
	‚Ä¢	Post-fix monitoring confirmed normal workflow event processing and no data loss.",P3,,,,,,2025-10-09T08:45:21.150Z